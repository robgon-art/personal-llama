{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robgon-art/personal-llama/blob/main/3_Evaluate_LLaMa_7B_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkBmY3vQvRSw"
      },
      "outputs": [],
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install git+https://github.com/robgon-art/llama-cpp-python.git --force-reinstall --upgrade --no-cache-dir --verbose\n",
        "!pip install llama_index huggingface_hub sentence-transformers\n",
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.25.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown 15vLnOyyJBtkjhizR-FqMiILpSD-tYgT3\n",
        "# !unzip robgon_articles_md.zip\n",
        "!gdown 14j_8FcDCJEt5-LPCwXDuU8Yrjpr7nTAp"
      ],
      "metadata": {
        "id": "qx81W006yuu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cf00dc6-0e08-423c-d5d5-ddce1ba62a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=14j_8FcDCJEt5-LPCwXDuU8Yrjpr7nTAp\n",
            "To: /content/robgon_qa.csv\n",
            "\r  0% 0.00/192k [00:00<?, ?B/s]\r100% 192k/192k [00:00<00:00, 2.79MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oI-kXwg5bHF-"
      },
      "outputs": [],
      "source": [
        "model_size = \"7B\"\n",
        "\n",
        "if model_size == \"7B\":\n",
        "  model_name_or_path = \"TheBloke/Llama-2-7B-chat-GGUF\"\n",
        "  model_basename = \"llama-2-7b-chat.Q4_K_M.gguf\"\n",
        "elif model_size == \"13B\":\n",
        "  model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGUF\"\n",
        "  model_basename = \"llama-2-13b-chat.Q4_K_M.gguf\"\n",
        "else:\n",
        "  print(\"Invalid model size\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBEJr-G-2ht4"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap words in text output in Colab\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "rs72UX3j5fKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms import LlamaCPP\n",
        "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "0Z-zNGVa018P",
        "outputId": "7d01d5f1-83f3-42a3-f526-5655b6359d43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LlamaCPP(\n",
        "    # You can pass in the URL to a GGUF model to download it automatically\n",
        "    model_url=None,\n",
        "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
        "    model_path=model_path,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=512, # 256\n",
        "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
        "    context_window=3900,\n",
        "    # kwargs to pass to __call__()\n",
        "    generate_kwargs={},\n",
        "    # kwargs to pass to __init__()\n",
        "    # set to at least 1 to use GPU\n",
        "    model_kwargs={\"n_gpu_layers\": 43},\n",
        "    # transform inputs into Llama2 format\n",
        "    messages_to_prompt=messages_to_prompt,\n",
        "    completion_to_prompt=completion_to_prompt,\n",
        "    verbose=True, # True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "yjupsjrI0w1h",
        "outputId": "d8ecb74e-12da-4997-95c3-dcfaff07014a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# try to answer without any references\n",
        "response = llm.complete(\"What is Muybridge Derby?\")\n",
        "print(response.text.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "4XULr8jV2CBc",
        "outputId": "98d2b159-6a08-44fe-d54d-d983b0e2fdf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thank you for asking! I'm here to help you with your question. However, I must inform you that \"Muybridge Derby\" is not a real or recognized term in any context. It's possible that it may be a misspelling or a made-up term, and I cannot provide information on something that does not exist.\n",
            "If you could provide more context or clarify the term you are referring to, I would be happy to help you to the best of my abilities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load the model\n",
        "encoder = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941,
          "referenced_widgets": [
            "18aa79e035394ed6a02bf1495af70f40",
            "aa50de51c9a748ed9c0ad06ceaf9d1f0",
            "23667d3c4b3e45a088f438187c9cf475",
            "9ade56e15d0a4bdea3c67d76f58138b7",
            "1ea3c8e7bfe74fdda587a2222ec636de",
            "bf71056733544616b9698cdea60d59c2",
            "1253dd0a6a0e4183b84cc94e2ef57460",
            "26b603b2f8924c2597b80cac035bb1b6",
            "5a8569d8179a4a3186e3bb64e883f41a",
            "f970bcb570b645108162fa5a7e529719",
            "18ad08a749714e96bd96fc985e5ed979",
            "2646f1a26e884ec9a84d77654ba4466e",
            "228727e7af5442a286b6134bc97a324b",
            "b60a4abadb434a0280aa070a4f76bdf7",
            "f0a1797a87a7439b8c25b299f94bdefd",
            "5788beb2f23e418b8e52c17dd1ca948b",
            "314a4e5e6536433f853ab6016904aa59",
            "3e20db0deb9847db9bd9fb7df511f9a8",
            "9dd3e7569d4a422a9ce1c099fb5cc20c",
            "32578e9dcf874f39b64ae540b10c8aea",
            "23a8910a6a904448b7632e540fe2ea0a",
            "f2cce73487624739be1fb3e94154d4fb",
            "8394faa9610c4c1d86c655e20cf5bcce",
            "3328faa4a63c42a19507514b88526a04",
            "d416b8e6f3144a85b2e6d14c1fb322ae",
            "9789807537654eceb1b635e377ed88a6",
            "564afd75da6c466690c77a2b261f703d",
            "971382a0a7d74caab23b51f7f06b3672",
            "cc98f15114c545d78a1593ca2fd107d7",
            "6bae940999df459cb653959281f60de6",
            "bd26864a885249eea5ca142e7b2429ca",
            "8444896b86f14a10b50633846067408d",
            "ddae6ac9e6784466b9fdda444ba48619",
            "121e6ecec11f4f4db1ab33ec86f6c80c",
            "8673a299a95d461fa85e6ea73f4bd507",
            "dcd52422685f49b6983462d25dd99314",
            "a17c07ddf9894cc0996fcbd92ded9e30",
            "c337a799cc1d41778abec5ac3689974e",
            "ef86dd18fefb485aa9dd8b26a95f0ba9",
            "2f561d8eed9d4cfe85037ed8accccd86",
            "0298ecc334bd41b684aac9e6d193a536",
            "5b9fd552619a4b86819d21e0a06a5329",
            "7b104eef802e4576940c5cfc6f8f5175",
            "fcc4809117c24b01bcfaa4b2c0b85977",
            "d788f4adbafa43df9ba5d82d5852fc95",
            "e4467e5ea588474291d643bea578a339",
            "a4b753e832244ff5824fb3e0a131429d",
            "f231fdaa91c6457cab81b30aad255c91",
            "ab92845745794431918e2cce7e803445",
            "6a0e4266dd4545feae15e7d185456350",
            "b446f69333bf43169cc1d93ce624d01f",
            "c1cc418270184f618f8e239d508db47a",
            "37ea1661b6614f5f9d5d34d580141ba6",
            "40ff3b85ba0548bb9dde7449524b8882",
            "8950c1b83c46435492ec14dd9742f18f",
            "9339f8e9997745bfb0771f07d3f45928",
            "7dd36c8f6e334beab25a16cfa20cd43e",
            "70d2018559f64d9f9f1143421c5a95d2",
            "064450998e0a45c1818ca7f5aa2cffd9",
            "4ac2dfba62064a029dea956dc3a23823",
            "6e21c951f1a348a8a999105ab4544336",
            "59a426bb30504570afc800a1d5d05b4a",
            "be7c727dbced434b8eb5cf0677896863",
            "bc27b4d01a554d04912b6a3a552e947b",
            "bd335d01efce4a2580914ee4f346dd79",
            "36eb469f17a44423aaefb25d7d85604d",
            "76badc2cd3ea441ebaf248d1a8c323d1",
            "16ffe0dc078a48e38dcbf76e7cb50af8",
            "4b8b2019bd784fe090e56131fd14a5e0",
            "e7aedfebfa044b4692ae35c5164d9752",
            "95d8024639674025835bed02297f89c9",
            "1ff4e0d2879b48d7a4409710ce07a4c4",
            "e5b3bc78d51b4f6da8d0cbfc2ce8316d",
            "77b9623eb98649eaaf7663331da9d4d2",
            "c9220e96668c46b6b6fcd163b60a611b",
            "07c66ca7efca4f02a01e527054c49378",
            "4e1f757218f845858e7a906bc8f8e244",
            "06f4e981d3ec490994cddeb023485a2f",
            "79315e00a6bd4109baf82800aa62eafa",
            "09e4f6103bd645f5af4619371fa99f54",
            "250b4236805d4b1184f603c9dd801cb6",
            "2927aca5cb8c4842bef1656cbd5b30a0",
            "f352088dc043497086cc6f08db487147",
            "99ba9c12d63f4c20bad039803995a44d",
            "1e1dd3519b214c5a80a41dbca5448ee2",
            "ccdf40ad7716471c85501bc6213b034a",
            "199e9c72f6304eddb89e2a41dcac081f",
            "8b03aee580cc4a4bba9ee4a2bac359f7",
            "a204233baedc4bad8da14ce97f818ee9",
            "4497493313d24126a74caadab8660692",
            "3bfbad53318e4f39b267e3ab4a3a526a",
            "a4f51e96c5284e0fa24742a95fec6783",
            "e87f391cabac459faf868edd5bfef2fa",
            "442da98e48594db0802ee0bd0153e2a5",
            "360bce965d254495a6bcf569ff25d11a",
            "4f7f1018766f4d6a88e36868cce99745",
            "4a9055068c68439baeb00d106e3c8ed9",
            "a816aecd895e4f10ade2376c249afdb7",
            "b807cc70552e4ec99eb05c410f1f052e",
            "5dd56d32324d43938ce4d8a86125d187",
            "3ee18070fcca4808b04240265dfc0cb4",
            "eaafa2ef8ba34b7f8262a463af514fcb",
            "8cf91382e1b843138e2c7c20bff5af4b",
            "ab33fca1f3ea4352b103fb72a9fde8ad",
            "445955f383c241cc9157e7d558e6b543",
            "cc95b85e83df429bbe8f9090aaaf7418",
            "f1885f8522e2423d90652fd460fe7171",
            "d1964eb0bb2c4813874a7cbf6cfd494a",
            "c9a5c2d6fca94a598be3e6ca95946037",
            "4172edf447af4b778310ac9e340af584",
            "4860a4f8752341ad968065f327fdc63b",
            "93e1de09068044c686dffed4d88d7bd2",
            "3393747e8a7147d9b3221e0ebb8156e6",
            "b3255f9dc9bd4c50b81bb785bce1227a",
            "2b4ce1faeba94ca181e49e6fecadd61c",
            "adc3e57d08194502955c1ac6ed32b13d",
            "66a5924f9c4245eb8d7c238ca255697f",
            "59b7d7eebeb04bf8a9dec61f98c3cad4",
            "7d8e8224b33a48139afb9af479cc620c",
            "7667c88014fd486789248577eafd5ea9",
            "f4b81bf5c3d34c4083467bfd4e248f04",
            "b2139ed216284f35a2c7700eedaf65dc",
            "bd15572cffc24692885576b37a95d083",
            "482659744ec74f719bcf4e4d257a57ca",
            "7f8a54443da7440db261baf2f3dd8821",
            "8d79b70b41b1461998b20de45139bb2c",
            "3b625706302d44dba9b21f782957224f",
            "09ae58f4420a4622ac2be94150920b29",
            "9bb0a1d4bfed4b6d91c90930903a9942",
            "9af40f06f3244faa822e294cc731ef67",
            "45dd93dff20e4cf4a6e03847b2d793d6",
            "68ffe6cb41674c3a8412c43e13099e68",
            "74f0b1ff34614168880a80b410be7383",
            "a24c031c9a474f518b558a69b09c586e",
            "55b1bc0f4abf40e3890f80bbcfa340dd",
            "c061c36222b143a48cfcc43a98e77415",
            "b4ac0021d24e44349c395629ec8a3bb5",
            "f2ddaf1515674b828718046eb6c90bb4",
            "0ac27448f4b444a8a36e61c4cfd6ae2f",
            "62ee171c43ce47e08ad9d6284fc08bd5",
            "c44e5854a3ca478ebe69263e9f6982b6",
            "c5fce33bacd24bbf958bfd5af405bc55",
            "ed7b3f67f53a452bbf6fd142bba0c752",
            "8333d38965f04c3db3f856d0b537a798",
            "3b5cd96afe7848278609ae7cde7d41b8",
            "1030cbbb4b1d48c88ecd13ae3f068953",
            "920cd1098d5845728b8fa51c16b1e7c6",
            "474c16e53f7a40d38317f5895eec89dd",
            "9a5cbe105b77461085f349fccced43da",
            "c14f991b2c3e4d44a0ff9bb65de9488d",
            "9343e38c0186419f9c9920967dd2ebfc",
            "81e6920a1e1c4ff8b87444bc39abab9a",
            "8d62bd788d5b40bf9db938094e0cd7e8",
            "ea279f516fb544f49766aa5710340541",
            "9bacb81cddcf4224967b1e24c10773ca",
            "08814d44e15b4620b29fa00f9922e70b",
            "4c3d70d83b63475d84c33dd7899d3c59",
            "9911e8ae0baa4ded849f0c1d7a827382",
            "577bab97d13b4511af4b5a3487f28e76",
            "b345d1c45e2642429823758efc6f9d0f",
            "79ccd0d69a4e4a5bb1c4885a07612b25",
            "7d1b3ac6d86c4dd5855c9ca29f1d83da",
            "0a6b6221f5074efdb9a9301c7d9b8e72",
            "1c2f99cd9033498bba40111478e4d61e",
            "3e9f7c8ecf8246b6a1d184d3dfb2f19e"
          ]
        },
        "id": "lIHRjg-iy_TA",
        "outputId": "4b94dcf0-d62b-46a9-836b-f2260607f2df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "18aa79e035394ed6a02bf1495af70f40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)a8e1d/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2646f1a26e884ec9a84d77654ba4466e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8394faa9610c4c1d86c655e20cf5bcce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)b20bca8e1d/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "121e6ecec11f4f4db1ab33ec86f6c80c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)0bca8e1d/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d788f4adbafa43df9ba5d82d5852fc95"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9339f8e9997745bfb0771f07d3f45928"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)e1d/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76badc2cd3ea441ebaf248d1a8c323d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06f4e981d3ec490994cddeb023485a2f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a204233baedc4bad8da14ce97f818ee9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5dd56d32324d43938ce4d8a86125d187"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)a8e1d/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4860a4f8752341ad968065f327fdc63b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2139ed216284f35a2c7700eedaf65dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)8e1d/train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74f0b1ff34614168880a80b410be7383"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)b20bca8e1d/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8333d38965f04c3db3f856d0b537a798"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)bca8e1d/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9bacb81cddcf4224967b1e24c10773ca"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "sentences = [\"This is an example sentence\", \"This is another example sentence\"]\n",
        "embeddings = encoder.encode(sentences)\n",
        "print(embeddings)\n",
        "\n",
        "# Normalize the embeddings\n",
        "text_features_1 = embeddings[0] / np.linalg.norm(embeddings[0], axis=-1, keepdims=True)\n",
        "text_features_2 = embeddings[1] / np.linalg.norm(embeddings[1], axis=-1, keepdims=True)\n",
        "\n",
        "# Calculate the cosine similarity\n",
        "similarity = np.dot(text_features_1, text_features_2.T)\n",
        "print(similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "qeW__Ll5rcCr",
        "outputId": "8897084e-baab-4d57-c882-02aaf7637104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.02250259 -0.07829171 -0.02303071 ... -0.0082793   0.02652686\n",
            "  -0.00201896]\n",
            " [ 0.05012981 -0.03327598 -0.01251665 ... -0.01040575  0.02814883\n",
            "  -0.01429978]]\n",
            "0.90200233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "4yd_C77sxU2L",
        "outputId": "7fe94ce7-0a0e-4a10-e615-fe1273d0f7ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv(\"/content/robgon_qa.csv\")\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788
        },
        "id": "r-5kHzurrvWr",
        "outputId": "7987a8d9-2717-417a-dad8-c5dda91cb9f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                         doc_metadata  \\\n",
              "0   {'title': 'Writing Songs with GPT-4: Part 2, C...   \n",
              "1   {'title': '#Hands-on Tutorials', 'subtitle': '...   \n",
              "2   {'title': 'Using AI to Create New Comic Strips...   \n",
              "3   {'title': 'Using AI to Create New Comic Strips...   \n",
              "4   {'title': 'BIG.art: Using Machine Learning to ...   \n",
              "..                                                ...   \n",
              "95  {'title': 'Frost Songs: Using AI to Generate M...   \n",
              "96  {'title': 'AI-Memer: Using Machine Learning to...   \n",
              "97  {'title': 'AI-Memer: Using Machine Learning to...   \n",
              "98  {'title': 'Benford’s Law — A Simple Explanatio...   \n",
              "99  {'title': 'Muybridge Derby: Bringing Animal Lo...   \n",
              "\n",
              "                                          doc_content  \\\n",
              "0   Source Code\\nThe source code for this project ...   \n",
              "1   Using CLIP to Filter the Images for Training\\n...   \n",
              "2   Final Thoughts\\nComparing the two systems, I f...   \n",
              "3   Mark Madness\\nFor the Mark Madness comic, I us...   \n",
              "4   Source Code\\nThe  source code  for this projec...   \n",
              "..                                                ...   \n",
              "95  Background\\nFor the last five months, I have b...   \n",
              "96  Next Steps\\nAlthough the results are pretty go...   \n",
              "97  GPT-3 Da Vinci\\nOpenAI’s GPT-3 Da Vinci is cur...   \n",
              "98  Summary\\nIn this article, I gave an overview o...   \n",
              "99  Muybridge Derby\\nFor this project, I wanted to...   \n",
              "\n",
              "                                             question  \\\n",
              "0   What is the main objective of Robert A. Gonsal...   \n",
              "1   How did Robert use CLIP to filter the images f...   \n",
              "2   What are the limitations Robert A. Gonsalves f...   \n",
              "3   How did Robert A. Gonsalves modify the images ...   \n",
              "4   What machine learning tools does Robert A. Gon...   \n",
              "..                                                ...   \n",
              "95  What was Robert A. Gonsalves' critique of curr...   \n",
              "96  What is the next model that the developers beh...   \n",
              "97  How does Robert A. Gonsalves use GPT-3 Da Vinc...   \n",
              "98  What are the three real datasets that Robert A...   \n",
              "99  What techniques did Robert A. Gonsalves use to...   \n",
              "\n",
              "                                               answer  \n",
              "0   The main objective of his project 'GPT-4 Chord...  \n",
              "1   Robert used CLIP to filter the images by compa...  \n",
              "2   Robert found that while DALL-E could generate ...  \n",
              "3   Robert cleaned up the images in Photoshop, add...  \n",
              "4   He uses GLIDE and BSRGAN to create these high-...  \n",
              "..                                                ...  \n",
              "95  Robert A. Gonsalves noted that while AI models...  \n",
              "96  The developers at EleutherAI are planning to b...  \n",
              "97  Robert A. Gonsalves uses GPT-3 Da Vinci by cre...  \n",
              "98  Robert A. Gonsalves used the datasets of city/...  \n",
              "99  He used a combination of Midjourney to create ...  \n",
              "\n",
              "[100 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c538e4c0-b230-451b-bfaf-c41745ddb6e3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc_metadata</th>\n",
              "      <th>doc_content</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{'title': 'Writing Songs with GPT-4: Part 2, C...</td>\n",
              "      <td>Source Code\\nThe source code for this project ...</td>\n",
              "      <td>What is the main objective of Robert A. Gonsal...</td>\n",
              "      <td>The main objective of his project 'GPT-4 Chord...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{'title': '#Hands-on Tutorials', 'subtitle': '...</td>\n",
              "      <td>Using CLIP to Filter the Images for Training\\n...</td>\n",
              "      <td>How did Robert use CLIP to filter the images f...</td>\n",
              "      <td>Robert used CLIP to filter the images by compa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>{'title': 'Using AI to Create New Comic Strips...</td>\n",
              "      <td>Final Thoughts\\nComparing the two systems, I f...</td>\n",
              "      <td>What are the limitations Robert A. Gonsalves f...</td>\n",
              "      <td>Robert found that while DALL-E could generate ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>{'title': 'Using AI to Create New Comic Strips...</td>\n",
              "      <td>Mark Madness\\nFor the Mark Madness comic, I us...</td>\n",
              "      <td>How did Robert A. Gonsalves modify the images ...</td>\n",
              "      <td>Robert cleaned up the images in Photoshop, add...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>{'title': 'BIG.art: Using Machine Learning to ...</td>\n",
              "      <td>Source Code\\nThe  source code  for this projec...</td>\n",
              "      <td>What machine learning tools does Robert A. Gon...</td>\n",
              "      <td>He uses GLIDE and BSRGAN to create these high-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>{'title': 'Frost Songs: Using AI to Generate M...</td>\n",
              "      <td>Background\\nFor the last five months, I have b...</td>\n",
              "      <td>What was Robert A. Gonsalves' critique of curr...</td>\n",
              "      <td>Robert A. Gonsalves noted that while AI models...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>{'title': 'AI-Memer: Using Machine Learning to...</td>\n",
              "      <td>Next Steps\\nAlthough the results are pretty go...</td>\n",
              "      <td>What is the next model that the developers beh...</td>\n",
              "      <td>The developers at EleutherAI are planning to b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>{'title': 'AI-Memer: Using Machine Learning to...</td>\n",
              "      <td>GPT-3 Da Vinci\\nOpenAI’s GPT-3 Da Vinci is cur...</td>\n",
              "      <td>How does Robert A. Gonsalves use GPT-3 Da Vinc...</td>\n",
              "      <td>Robert A. Gonsalves uses GPT-3 Da Vinci by cre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>{'title': 'Benford’s Law — A Simple Explanatio...</td>\n",
              "      <td>Summary\\nIn this article, I gave an overview o...</td>\n",
              "      <td>What are the three real datasets that Robert A...</td>\n",
              "      <td>Robert A. Gonsalves used the datasets of city/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>{'title': 'Muybridge Derby: Bringing Animal Lo...</td>\n",
              "      <td>Muybridge Derby\\nFor this project, I wanted to...</td>\n",
              "      <td>What techniques did Robert A. Gonsalves use to...</td>\n",
              "      <td>He used a combination of Midjourney to create ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c538e4c0-b230-451b-bfaf-c41745ddb6e3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c538e4c0-b230-451b-bfaf-c41745ddb6e3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c538e4c0-b230-451b-bfaf-c41745ddb6e3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c58ab1fb-49f3-4d04-a706-fbab9c418779\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c58ab1fb-49f3-4d04-a706-fbab9c418779')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c58ab1fb-49f3-4d04-a706-fbab9c418779 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_b401a32d-e137-4825-a14e-8ff695085a02\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b401a32d-e137-4825-a14e-8ff695085a02 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the first five rows\n",
        "for index, row in df.iloc[:5].iterrows():\n",
        "  print(f\"Row {index+1}:\")\n",
        "  print(f\"doc_metadata: {row['doc_metadata']}\")\n",
        "  print(f\"doc_content: {row['doc_content']}\")\n",
        "  print(f\"question: {row['question']}\")\n",
        "  print(f\"answer: {row['answer']}\\n\")\n",
        "  print(\"************************************\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g3fZYL1xsT6A",
        "outputId": "64794521-08a6-4216-cbde-cc30992348a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 1:\n",
            "doc_metadata: {'title': 'Writing Songs with GPT-4: Part 2, Chords', 'subtitle': 'How to use the latest large language model from OpenAI to help compose chords for original songs', 'author': 'Robert A. Gonsalves', 'date': 'GPT-4 Chords - https://medium.com/towards-data-science/writing-songs-with-gpt-4-part-2-chords-173cfda0e5a1', 'nickname': 'GPT-4 Chords', 'url': 'https://medium.com/towards-data-science/writing-songs-with-gpt-4-part-2-chords-173cfda0e5a1'}\n",
            "doc_content: Source Code\n",
            "The source code for this project is available on  GitHub .\n",
            "question: What is the main objective of Robert A. Gonsalves' project 'GPT-4 Chords'?\n",
            "answer: The main objective of his project 'GPT-4 Chords' is to demonstrate how to use the latest large language model from OpenAI, GPT-4, to help compose chords for original songs.\n",
            "\n",
            "************************************\n",
            "Row 2:\n",
            "doc_metadata: {'title': '#Hands-on Tutorials', 'subtitle': '# MAGNet: Modern Art Generator using Deep Neural Networks', 'author': '## How I used CLIP, SWAGAN, and a custom genetic algorithm to create modern paintings from text descriptions', 'date': 'MAGnet - https://medium.com/towards-data-science/magnet-modern-art-generator-using-deep-neural-networks-57537457bb7', 'nickname': 'MAGnet', 'url': 'https://medium.com/towards-data-science/magnet-modern-art-generator-using-deep-neural-networks-57537457bb7'}\n",
            "doc_content: Using CLIP to Filter the Images for Training\n",
            "After cropping the images, I ended up having over 12,000 paintings to work with. That’s enough to train a GAN, but not all of the paintings are good. Just because a painter is tagged on WikiArt as being “modern” doesn’t mean that all of their works are good examples of modern painting. I used CLIP to filter the images as I did in my GANscapes project to winnow down the dataset.\n",
            "OpenAI designed their two models for CLIP, an image and text encoder, to perform semantic searches. They trained the two models on a dataset of images with corresponding phrases. The goal of the models is to have the encoded images match the encoded phrases.\n",
            "Once trained, the image encoder system converts images to embeddings, lists of 512 floating-point numbers that capture the images’ general features. The text encoder converts a text phrase to similar embedding that can be compared to image embeddings for a semantic search.\n",
            "For MAGnet, I compare the embedding from the phrase “modern painting” to the embeddings from the paintings to find the top 10,000 images that best match the phrase. The source code is in the Colab  here .\n",
            "Here are some examples that didn’t make the cut as modern paintings. The first two probably scored lower because they don't have a lot of contrast. The third seems more like a drawing than a painting.\n",
            "question: How did Robert use CLIP to filter the images for the MAGnet project?\n",
            "answer: Robert used CLIP to filter the images by comparing the embedding from the phrase \"modern painting\" to the embeddings from the paintings. This allowed him to find the top 10,000 images that best matched the phrase \"modern painting\". This was done to ensure that the images used for training were good examples of modern painting.\n",
            "\n",
            "************************************\n",
            "Row 3:\n",
            "doc_metadata: {'title': 'Using AI to Create New Comic Strips without Writing Any Code', 'subtitle': 'A tutorial on how to use GPT-3 and DALL-E to generate original content for the funny pages', 'author': 'Robert A. Gonsalves', 'date': 'AI Comics - https://medium.com/towards-data-science/using-ai-to-create-new-comic-strips-without-writing-any-code-cc669bb317a7', 'nickname': 'AI Comics', 'url': 'https://medium.com/towards-data-science/using-ai-to-create-new-comic-strips-without-writing-any-code-cc669bb317a7'}\n",
            "doc_content: Final Thoughts\n",
            "Comparing the two systems, I found that GPT-3 does a much better job generating the text than DALL-E does for the artwork. Although the initial images for the comics look OK as concept renderings, they need some cleanup to be used in production. But the main problem is DALL-E doesn’t generate characters consistently for the comic strip. The primary limitation stems from the 1024x1024 image size in DALL-E. Note that OpenAI  recently added  a new “outpainting” feature to more easily add to generated images. However, the new feature only works in a piecemeal fashion. It only ever considers a 1024x1024 frame for image generation. The system would need an “upload reference frames” feature to generate visual elements consistently, like comic strip characters.\n",
            "question: What are the limitations Robert A. Gonsalves found in using DALL-E for generating artwork for comic strips?\n",
            "answer: Robert found that while DALL-E could generate concept renderings for the comic strips, the images often required cleanup before they could be used in production. He also noted that DALL-E struggled with generating characters consistently across the comic strip. This was primarily due to the system's limitation of only considering a 1024x1024 frame for image generation. Even with the new \"outpainting\" feature, which allows for easier addition to generated images, the system still operates in a piecemeal fashion. He suggested that the system would benefit from an \"upload reference frames\" feature to generate visual elements, like comic strip characters, more consistently.\n",
            "\n",
            "************************************\n",
            "Row 4:\n",
            "doc_metadata: {'title': 'Using AI to Create New Comic Strips without Writing Any Code', 'subtitle': 'A tutorial on how to use GPT-3 and DALL-E to generate original content for the funny pages', 'author': 'Robert A. Gonsalves', 'date': 'AI Comics - https://medium.com/towards-data-science/using-ai-to-create-new-comic-strips-without-writing-any-code-cc669bb317a7', 'nickname': 'AI Comics', 'url': 'https://medium.com/towards-data-science/using-ai-to-create-new-comic-strips-without-writing-any-code-cc669bb317a7'}\n",
            "doc_content: Mark Madness\n",
            "For the Mark Madness comic, I used this prompt, “A comic set in a basketball court with two characters wearing a red uniform, Mark, a talented college basketball player, and Enrique, an egotistical teammate.” Here are the four images DALL-E created.\n",
            "I liked the second one, but I needed to clean it up a bit in Photoshop. And I added the title and the dialog generated by GPT-3, replacing the gibberish text in the word balloons. Here are the before and after images.\n",
            "This is kinda like one of those visual puzzles where you need to find the ten differences. Can you spot them all? 🙂\n",
            "I like the overall look of the comic, with some lovely details like the red vertical bar that matches the team color. The eyes of the characters, however, do not match at all. I used a little trick in Photoshop to replicate Mark’s right eye and Enrique’s left eye. I used the Comic Sans font for the dialog, of course.\n",
            "question: How did Robert A. Gonsalves modify the images created by DALL-E for the Mark Madness comic?\n",
            "answer: Robert cleaned up the images in Photoshop, added the title, and replaced the gibberish text in the word balloons with dialog generated by GPT-3. He also used a trick in Photoshop to replicate the eyes of the characters Mark and Enrique, and used the Comic Sans font for the dialog.\n",
            "\n",
            "************************************\n",
            "Row 5:\n",
            "doc_metadata: {'title': 'BIG.art: Using Machine Learning to Create High-Res Fine Art', 'subtitle': 'How to use GLIDE and BSRGAN to create ultra-high-resolution digital paintings with fine details', 'author': 'Robert A. Gonsalves', 'date': 'BigArt - https://medium.com/towards-data-science/big-art-using-machine-learning-to-create-high-res-fine-art-7dd695f99788', 'nickname': 'BigArt', 'url': 'https://medium.com/towards-data-science/big-art-using-machine-learning-to-create-high-res-fine-art-7dd695f99788'}\n",
            "doc_content: Source Code\n",
            "The  source code  for this project is available on GitHub. I am releasing the sources under the CC BY-SA license. You can create your own images using this  Google Colab .\n",
            "If you use this code to create new images, please give attribution like this: This image was created with  BIG.art   by  Robert A. Gonsalves .\n",
            "question: What machine learning tools does Robert A. Gonsalves use to create ultra-high-resolution digital paintings?\n",
            "answer: He uses GLIDE and BSRGAN to create these high-resolution digital paintings.\n",
            "\n",
            "************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"Answer the question briefly.\"\n",
        "\n",
        "# \"\"\"Answer the question from the user guided by the reference matrial.\n",
        "# Don't use phrase 'reference material'. Instead use the tile of the article or the nickname of the project.\n",
        "# In the answer, refer to the author as 'he/him/his' if his name was used in the question.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "t15InmsFtOBa",
        "outputId": "fb701e32-4f5c-4a7f-c293-5314a5e8bfd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the rows\n",
        "scores = []\n",
        "for index, row in df.iloc[:].iterrows():\n",
        "  reference_answer = row['answer'].replace(\"Robert A. Gonsalves\", \"he\").replace(\"Robert\", \"he\")\n",
        "  prompt =row['doc_metadata'] + \"\\n\" + row['doc_content'] + \"\\n\\n\" + prefix + \"\\n\\n\" + row['question']\n",
        "  print(\"Index:           \", index)\n",
        "  print(\"Reference:       \", row['doc_content'])\n",
        "  print(\"Question:        \", row['question'])\n",
        "  print(\"GPT-4 Answer:    \", reference_answer)\n",
        "  response = llm.complete(prompt)\n",
        "  answer = response.text.strip().replace(\"Robert A. Gonsalves\", \"he\").replace(\"Robert\", \"he\")\n",
        "  print(\"LLaMa 7B Answer: \", answer)\n",
        "\n",
        "  sentences = [reference_answer, answer]\n",
        "  embeddings = encoder.encode(sentences)\n",
        "\n",
        "  # Normalize the embeddings\n",
        "  text_features_1 = embeddings[0] / np.linalg.norm(embeddings[0], axis=-1, keepdims=True)\n",
        "  text_features_2 = embeddings[1] / np.linalg.norm(embeddings[1], axis=-1, keepdims=True)\n",
        "\n",
        "  # Calculate the cosine similarity\n",
        "  similarity = np.dot(text_features_1, text_features_2.T)\n",
        "  scores.append(similarity)\n",
        "  print(\"LLaMa 7B Score:  \", similarity)\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lvR_-0NCtxK1",
        "outputId": "5662595d-37bb-4e02-b3fe-ab79ca699003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index:            0\n",
            "Reference:        Source Code\n",
            "The source code for this project is available on  GitHub .\n",
            "Question:         What is the main objective of Robert A. Gonsalves' project 'GPT-4 Chords'?\n",
            "GPT-4 Answer:     The main objective of his project 'GPT-4 Chords' is to demonstrate how to use the latest large language model from OpenAI, GPT-4, to help compose chords for original songs.\n",
            "LLaMa 7B Answer:  The main objective of he' project \"GPT-4 Chords\" is to use the latest large language model from OpenAI, GPT-4, to help compose chords for original songs.\n",
            "LLaMa 7B Score:   0.98218435\n",
            "\n",
            "Index:            1\n",
            "Reference:        Using CLIP to Filter the Images for Training\n",
            "After cropping the images, I ended up having over 12,000 paintings to work with. That’s enough to train a GAN, but not all of the paintings are good. Just because a painter is tagged on WikiArt as being “modern” doesn’t mean that all of their works are good examples of modern painting. I used CLIP to filter the images as I did in my GANscapes project to winnow down the dataset.\n",
            "OpenAI designed their two models for CLIP, an image and text encoder, to perform semantic searches. They trained the two models on a dataset of images with corresponding phrases. The goal of the models is to have the encoded images match the encoded phrases.\n",
            "Once trained, the image encoder system converts images to embeddings, lists of 512 floating-point numbers that capture the images’ general features. The text encoder converts a text phrase to similar embedding that can be compared to image embeddings for a semantic search.\n",
            "For MAGnet, I compare the embedding from the phrase “modern painting” to the embeddings from the paintings to find the top 10,000 images that best match the phrase. The source code is in the Colab  here .\n",
            "Here are some examples that didn’t make the cut as modern paintings. The first two probably scored lower because they don't have a lot of contrast. The third seems more like a drawing than a painting.\n",
            "Question:         How did Robert use CLIP to filter the images for the MAGnet project?\n",
            "GPT-4 Answer:     he used CLIP to filter the images by comparing the embedding from the phrase \"modern painting\" to the embeddings from the paintings. This allowed him to find the top 10,000 images that best matched the phrase \"modern painting\". This was done to ensure that the images used for training were good examples of modern painting.\n",
            "LLaMa 7B Answer:  he used CLIP to filter the images for the MAGnet project by comparing the embedding from the phrase \"modern painting\" to the embeddings from the paintings and finding the top 10,000 images that best match the phrase.\n",
            "LLaMa 7B Score:   0.8643008\n",
            "\n",
            "Index:            2\n",
            "Reference:        Final Thoughts\n",
            "Comparing the two systems, I found that GPT-3 does a much better job generating the text than DALL-E does for the artwork. Although the initial images for the comics look OK as concept renderings, they need some cleanup to be used in production. But the main problem is DALL-E doesn’t generate characters consistently for the comic strip. The primary limitation stems from the 1024x1024 image size in DALL-E. Note that OpenAI  recently added  a new “outpainting” feature to more easily add to generated images. However, the new feature only works in a piecemeal fashion. It only ever considers a 1024x1024 frame for image generation. The system would need an “upload reference frames” feature to generate visual elements consistently, like comic strip characters.\n",
            "Question:         What are the limitations Robert A. Gonsalves found in using DALL-E for generating artwork for comic strips?\n",
            "GPT-4 Answer:     he found that while DALL-E could generate concept renderings for the comic strips, the images often required cleanup before they could be used in production. He also noted that DALL-E struggled with generating characters consistently across the comic strip. This was primarily due to the system's limitation of only considering a 1024x1024 frame for image generation. Even with the new \"outpainting\" feature, which allows for easier addition to generated images, the system still operates in a piecemeal fashion. He suggested that the system would benefit from an \"upload reference frames\" feature to generate visual elements, like comic strip characters, more consistently.\n",
            "LLaMa 7B Answer:  According to he' tutorial, the main limitation of using DALL-E for generating artwork for comic strips is that it struggles to consistently generate characters within a 1024x1024 image size. Additionally, the system lacks an \"upload reference frames\" feature, which would be necessary to generate visual elements consistently, like comic strip characters.\n",
            "LLaMa 7B Score:   0.7768611\n",
            "\n",
            "Index:            3\n",
            "Reference:        Mark Madness\n",
            "For the Mark Madness comic, I used this prompt, “A comic set in a basketball court with two characters wearing a red uniform, Mark, a talented college basketball player, and Enrique, an egotistical teammate.” Here are the four images DALL-E created.\n",
            "I liked the second one, but I needed to clean it up a bit in Photoshop. And I added the title and the dialog generated by GPT-3, replacing the gibberish text in the word balloons. Here are the before and after images.\n",
            "This is kinda like one of those visual puzzles where you need to find the ten differences. Can you spot them all? 🙂\n",
            "I like the overall look of the comic, with some lovely details like the red vertical bar that matches the team color. The eyes of the characters, however, do not match at all. I used a little trick in Photoshop to replicate Mark’s right eye and Enrique’s left eye. I used the Comic Sans font for the dialog, of course.\n",
            "Question:         How did Robert A. Gonsalves modify the images created by DALL-E for the Mark Madness comic?\n",
            "GPT-4 Answer:     he cleaned up the images in Photoshop, added the title, and replaced the gibberish text in the word balloons with dialog generated by GPT-3. He also used a trick in Photoshop to replicate the eyes of the characters Mark and Enrique, and used the Comic Sans font for the dialog.\n",
            "LLaMa 7B Answer:  he modified the images created by DALL-E for the Mark Madness comic by cleaning up the second image in Photoshop and adding the title and dialog generated by GPT-3, replacing the gibberish text in the word balloons. He also used a little trick in Photoshop to replicate the eyes of the characters.\n",
            "LLaMa 7B Score:   0.82649976\n",
            "\n",
            "Index:            4\n",
            "Reference:        Source Code\n",
            "The  source code  for this project is available on GitHub. I am releasing the sources under the CC BY-SA license. You can create your own images using this  Google Colab .\n",
            "If you use this code to create new images, please give attribution like this: This image was created with  BIG.art   by  Robert A. Gonsalves .\n",
            "Question:         What machine learning tools does Robert A. Gonsalves use to create ultra-high-resolution digital paintings?\n",
            "GPT-4 Answer:     He uses GLIDE and BSRGAN to create these high-resolution digital paintings.\n",
            "LLaMa 7B Answer:  According to the article, he uses two machine learning tools to create ultra-high-resolution digital paintings: GLIDE and BSRGAN.\n",
            "LLaMa 7B Score:   0.8471364\n",
            "\n",
            "Index:            5\n",
            "Reference:        Interacting with GPT-4\n",
            "Using the example song from my first two articles, I asked GPT-4 to compose a melody one part at a time. Here’s how the first interaction went.\n",
            "> RobG: Write a melody for the song Dogs and Cats in guitar tab format. Play one note for each syllable.\n",
            "> [G] Dogs and cats, they [C] roam the streets [Em] Furry friends with [D] hearts that beat [G] Claws and paws, they [C] rule the land [Em] Tails a-waggin’, [D] they understand\n",
            "> GPT-4: I’m an AI text-based model, so I can’t create or play a sound. But I can give you an example of how you might put these chords to a simple melody using guitar tab format, each note for each syllable.\n",
            "> The basic chords provided are G, C, Em, and D. I’ll use the G major scale as the melody foundation.\n",
            "> Remember, in tab format, the lines represent the strings of a guitar (from low E on bottom to high E on top), and the numbers represent which fret to play on that string. Here’s a simple example:\n",
            "\n",
            "\n",
            "```\n",
            "e|----------------0------------------|-----------------3------------------| B|---------1---3-----3-1---------1--|-------0---1-3-------3-1------------| G|-------0---0-----------2-0--------|-----0---0---------------2-0--------| D|-----2----------------------2------|---2-------------------------------| A|----------------------------------|-----------------------------------| E|----------------------------------|-----------------------------------|      G          C                   Em             D e|----------------0------------------|-----------------3------------------| B|---------1---3-----3-1---------1--|-------0---1-3-------3-1------------| G|-------0---0-----------2-0--------|-----0---0---------------2-0--------| D|-----2----------------------2------|---2-------------------------------| A|----------------------------------|-----------------------------------| E|----------------------------------|-----------------------------------|      G          C                   Em             D\n",
            "```\n",
            "> This is just a very simple example and the melody might not fully capture the spirit of your lyrics, but I hope it helps as a starting point. You may want to adjust the rhythm and melody to suit your song better.\n",
            "\n",
            "\n",
            "This is interesting. It’s a two-part musical phrase that is repeated twice. But on closer inspection, you can see that there are too many syllables in the phrases; it has twelve+eleven when it should be seven+seven. Also, the vertical lines don’t line up. I repeated the process for the second verse and cleaned up the phrases a bit in a text editor to look like this.\n",
            "```\n",
            "e|-------------0-----|-------------------| B|-------1---3---3---|-------0---1-3-----| G|-----0---0---------|-----0---0-----0---| D|---2---------------|---2---------------| A|-------------------|-------------------| E|-------------------|-------------------|      G       C             Em      D e|-------------------|-------------------| B|-------0-3-1-0-----|-------0---3-1-----| G|-----1---------2---|-----0---0-----2---| D|---2---------------|---2---------------| A|-------------------|-------------------| E|-------------------|-------------------|      Am      C           G       D\n",
            "```\n",
            "I wrote some Python code  here  to convert guitar tabs to MIDI files, so I could play them back to hear how they sounded. The code is based on a similar project by Brian Caffey [2]. Here’s the complete score.\n",
            "And here’s what the song sounds like. I used  Band-in-a-Box  by PG Music to arrange the song with piano, guitar, bass, and drums. Feel free to sing along. 😀\n",
            "It’s simple but catchy, kinda like a nursery song. The first note is a bit odd in that it is an E, which would make the G chord effectively a G6. And this occurs with every instance of the G chord, too. I think the best thing about the tune is the variation the third time through with the Am and G. Up next, I’ll show you how I created melodies for a ska song.\n",
            "Question:         How did Robert A. Gonsalves use GPT-4 to create melodies for a song?\n",
            "GPT-4 Answer:     he interacted with GPT-4 by asking it to compose a melody for a song, one part at a time. He provided the lyrics and asked the AI to create a melody in guitar tab format, with one note for each syllable. GPT-4, being a text-based model, provided an example of how the chords could be put to a simple melody using guitar tab format. he then adjusted the melody to suit the rhythm of the song. He also wrote Python code to convert the guitar tabs to MIDI files, so he could hear how the melody sounded.\n",
            "LLaMa 7B Answer:  he used GPT-4 to create melodies for a song by providing the language model with lyrics and asking it to compose a melody one part at a time, using guitar tab format. GPT-4 generated a simple melody based on the provided lyrics, which Gonsalves then cleaned up and adjusted to suit the song better.\n",
            "LLaMa 7B Score:   0.7904641\n",
            "\n",
            "Index:            6\n",
            "Reference:        Writing Songs with GPT-4: Part 3, Melodies\n",
            "towardsdatascience.com\n",
            "Question:         What is the main focus of Robert A. Gonsalves' article \"Writing Songs with GPT-4: Part 2, Chords\"?\n",
            "GPT-4 Answer:     In his article, he discusses how to use the latest large language model from OpenAI, GPT-4, to help compose chords for original songs.\n",
            "LLaMa 7B Answer:  The main focus of he' article \"Writing Songs with GPT-4: Part 2, Chords\" is to explore how the latest large language model from OpenAI, GPT-4, can be used to help compose chords for original songs.\n",
            "LLaMa 7B Score:   0.9433632\n",
            "\n",
            "Index:            7\n",
            "Reference:        StyleCLIP\n",
            "As you may be able to guess from the name, StyleCLIP is a system that uses the CLIP encoders to steer StyleGAN 2 to generate images from a text phrase. In their paper, “StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery,” the authors…\n",
            "> … introduce an optimization scheme that utilizes a CLIPbased loss to modify an input latent vector in response to a user-provided text prompt. — Or Patashnik, et al.\n",
            "\n",
            "\n",
            "StyleCLIP works by iteratively changing the input into Style GAN 2 to steer the generated image to match the text prompt as measured by CLIP.\n",
            "The system starts with the average image defined by the StyleGAN 2 model that’s sent into the CLIP image encoder. The text prompt is sent into the CLIP text encoder, and the two embeddings are compared. The StyleCLIP optimizer then steers the generator to create images that get successively closer to the features described in the prompt. StyleCLIP does this by minimizing the difference between the image embedding and the text embedding with each iteration. The learning rate is set to 10% at this stage, so the change in each iteration can be substantial. After about 40 passes, the image will start to look like what the prompt describes.\n",
            "As mentioned in the overview, the StyleGAN 2 model was trained using faces of real people posted on Flickr. The output will generally look like a real person and not a Halloween baddie, like a witch or a monster. But VQGAN can get us there.\n",
            "Question:         How does Robert A. Gonsalves describe the process of generating images using StyleCLIP in his article \"SpookyGAN - Rendering Scary Faces with Machine Learning\"?\n",
            "GPT-4 Answer:     In his article, he explains that StyleCLIP works by iteratively changing the input into StyleGAN 2 to steer the generated image to match a text prompt as measured by CLIP. The process begins with the average image defined by the StyleGAN 2 model, which is sent into the CLIP image encoder. The text prompt is sent into the CLIP text encoder, and the two embeddings are compared. The StyleCLIP optimizer then steers the generator to create images that get successively closer to the features described in the prompt. This is done by minimizing the difference between the image embedding and the text embedding with each iteration. After about 40 passes, the image will start to look like what the prompt describes.\n",
            "LLaMa 7B Answer:  In his article \"SpookyGAN - Rendering Scary Faces with Machine Learning,\" he describes the process of generating images using StyleCLIP as follows:\n",
            "StyleCLIP works by iteratively changing the input into Style GAN 2 to steer the generated image to match the text prompt as measured by CLIP. The system starts with the average image defined by the StyleGAN 2 model that’s sent into the CLIP image encoder, and then the text prompt is sent into the CLIP text encoder, and the two embeddings are compared. The StyleCLIP optimizer then steers the generator to create images that get successively closer to the features described in the prompt.\n",
            "Gonsalves, R. A. (2020, October 15). SpookyGAN - Rendering Scary Faces with Machine Learning. Towards Data Science. Retrieved from <https://medium.com/towards-data-science/spookygan-rendering-scary-faces-with-machine-learning-c390000206c6>\n",
            "LLaMa 7B Score:   0.8920058\n",
            "\n",
            "Index:            8\n",
            "Reference:        Future Work\n",
            "Future work could include exploring how the two types of analyses, adherence to lognormal distributions and compliance with Benford’s Law, may be related when the datasets do not closely match the ideals. These tools together may help in determining any underlying reasons for any discrepancies in the data.\n",
            "Question:         What future research does Robert A. Gonsalves suggest in his article \"Benford’s Law — A Simple Explanation\"?\n",
            "GPT-4 Answer:     In his article, he suggests that future research could involve investigating how adherence to lognormal distributions and compliance with Benford’s Law might be related when the datasets do not closely match the ideals. He believes that these tools, used together, could assist in identifying any underlying reasons for discrepancies in the data.\n",
            "LLaMa 7B Answer:  In his article \"Benford's Law — A Simple Explanation,\" he suggests that future research could explore the relationship between adherence to lognormal distributions and compliance with Benford's Law when datasets do not closely match the ideals. He proposes that these tools together may help in determining any underlying reasons for discrepancies in the data.\n",
            "LLaMa 7B Score:   0.9777674\n",
            "\n",
            "Index:            9\n",
            "Reference:        Hands-on Tutorials\n",
            "Apr 1, 2021\n",
            "Ganscapes - https://medium.com/towards-data-science/ganscapes-using-ai-to-create-new-impressionist-paintings-d6af1cf94c56\n",
            "This is my third article on experimenting with Generative Adversarial Networks (GANs) to create fine art. The first two articles focused on  creating abstract art  by  using image augmentation , but this one focuses on creating Impressionist landscape paintings.\n",
            "I posted all of the  source code  for GANscapes on GitHub and posted the original paintings on Kaggle. You can create new landscape paintings using the  Google Colab here .\n",
            "Question:         What was the main focus of the Ganscapes project by Robert A. Gonsalves?\n",
            "GPT-4 Answer:     The main focus of the Ganscapes project was to experiment with Generative Adversarial Networks (GANs) to create Impressionist landscape paintings. This was a shift from his previous projects which focused on creating abstract art using image augmentation.\n",
            "LLaMa 7B Answer:  The main focus of the Ganscapes project by he is to use Generative Adversarial Networks (GANs) to create new Impressionist landscape paintings based on a dataset of 5,000 public domain Impressionist paintings.\n",
            "LLaMa 7B Score:   0.88376665\n",
            "\n",
            "Index:            10\n",
            "Reference:        Generative Adversarial Networks\n",
            "In 2014, Ian Goodfellow and seven coauthors at the Université de Montréal presented a  paper  on Generative Adversarial Networks (GANs)[2].  They came up with a way to train two ANNs that effectively compete with each other to create content like photos, songs, prose, and yes, paintings.  The first ANN is called the Generator and the second is called the Discriminator. The Generator is trying to create realistic output, in this case, a color painting. The Discriminator is trying to discern real paintings from the training set as opposed to fake paintings from the generator. Here’s what a GAN architecture looks like.\n",
            "A series of random noise is fed into the Generator, which then uses its trained weights to generate the resultant output, in this case, a color image. The Discriminator is trained by alternating between processing real paintings, with an expected output of 1 and fake paintings, with an expected output of -1. After each painting is sent to the Discriminator, it sends back detailed feedback about why the painting is not real, and the Generator adjusts its weights with this new knowledge to try and do better the next time.  The two networks in the GAN are effectively trained together in an adversarial fashion . The Generator gets better at trying to pass off a fake image as real, and the Discriminator gets better at determining which input is real, and which is fake. Eventually, the Generator gets pretty good at generating realistic-looking images. You can read more about GANs, and the math they use, in Shweta Goyal’s post  here .\n",
            "Question:         How does the Generator in the MachineRay project improve its ability to create realistic images?\n",
            "GPT-4 Answer:     The Generator in the MachineRay project improves its ability to create realistic images through feedback from the Discriminator. After each painting is sent to the Discriminator, it sends back detailed feedback about why the painting is not real. The Generator then adjusts its weights with this new knowledge to try and do better the next time. This adversarial training process continues until the Generator gets pretty good at generating realistic-looking images.\n",
            "LLaMa 7B Answer:  In the MachineRay project, the Generator improves its ability to create realistic images by competing with the Discriminator in an adversarial manner. The Generator uses random noise as input and adjusts its weights based on the feedback it receives from the Discriminator, which is trained to distinguish between real and fake paintings. Through this process, the Generator learns to generate more realistic images that can fool the Discriminator, leading to improved performance over time.\n",
            "LLaMa 7B Score:   0.8730217\n",
            "\n",
            "Index:            11\n",
            "Reference:        Contrastive Similarity Test\n",
            "For my  article last month , I researched various metrics for gauging the esthetics of generated art. I discovered a technique using the CLIP model by OpenAI [2] that can be used to estimate esthetic quality. When I compared an image embedding to the embeddings of positive and negative text prompts, like “bad art” and “good art,” I found that using the difference in similarity produced reasonable results. I also calculated a prompt similarity metric to see how well the image matched the prompt using similar logic. The graph below shows the two metrics for the sixteen images above.\n",
            "You can see how the renderings using the newer Midjourney rank higher on both the esthetic quality metric (vertical axis) and prompt similarity (horizontal axis) than the older versions, and V4 stands a cut above the rest. Two notable outliers are v2–1 (unusually good prompt similarity) and v2–3 (unusually bad on both metrics). Scrolling up to look at the images again, these metrics seem to hold up reasonably well. The math and Python for this test are on my GitHub repository  here .\n",
            "Question:         What metrics did Robert use to gauge the esthetics of generated art in his exploration of Midjourney V4?\n",
            "GPT-4 Answer:     In his exploration of Midjourney V4, he used a technique involving the CLIP model by OpenAI to estimate esthetic quality. He compared an image embedding to the embeddings of positive and negative text prompts, such as “bad art” and “good art.” The difference in similarity was used to produce results. He also calculated a prompt similarity metric to see how well the image matched the prompt.\n",
            "LLaMa 7B Answer:  In his article, he Gonsalves used two metrics to gauge the esthetics of generated art:\n",
            "1. Esthetic quality metric: This measures how aesthetically pleasing an image is, with higher values indicating better esthetic quality.\n",
            "2. Prompt similarity metric: This assesses how well an image matches its corresponding text prompt, with higher values indicating better alignment between the two.\n",
            "LLaMa 7B Score:   0.6661899\n",
            "\n",
            "Index:            12\n",
            "Reference:        Building an AI 8-Ball with RoBERTa\n",
            "Robert A. Gonsalves\n",
            "Nov 1, 2020\n",
            "AI 8-Ball - https://medium.com/towards-data-science/building-an-ai-8-ball-with-roberta-2bfbf6f5519b\n",
            "In this post, I’ll show you how I built an AI 8-Ball that can answer yes/no questions using the power of the Internet and current Machine Learning techniques. I’ll start with a high-level description of the system and then get into the details of the components along with excerpts of the Python code. I’ll show how you can get both a percentage no-to-yes answer and a percentage confidence level as outputs. Finally, I’ll give instructions on how to run the AI 8-Ball to answer your own questions.\n",
            "Question:         What does the AI 8-Ball built by Robert A. Gonsalves do?\n",
            "GPT-4 Answer:     The AI 8-Ball built by him is designed to answer yes/no questions using the power of the Internet and current Machine Learning techniques. It provides both a percentage no-to-yes answer and a percentage confidence level as outputs.\n",
            "LLaMa 7B Answer:  The AI 8-Ball built by he is a machine learning model that can answer yes/no questions based on the information it has been trained on. It uses a combination of natural language processing and probability theory to generate answers, and provides both a percentage no-to-yes answer and a percentage confidence level as outputs.\n",
            "LLaMa 7B Score:   0.938753\n",
            "\n",
            "Index:            13\n",
            "Reference:        Background\n",
            "Natural Language Processing (NLP) is a field of Linguistics and Computer Science that studies how machines, with their computer languages like Java and Python, can communicate with humans, with their natural languages like English and Swahili. One of the first proponents of teaching computers to understand human language was Alan Turing. He wrote about it in 1950 [1].\n",
            "> We may hope that machines will eventually compete with men in all purely intellectual fields. But which are the best ones to start with? Even this is a difficult decision. Many people think that a very abstract activity, like the playing of chess, would be best. It can also be maintained that it is best to provide the machine with the best sense organs that money can buy, and then   teach it to understand and speak English. —  Alan Turing , Mathematician and Computer Scientist\n",
            "Question:         What is the main purpose of the PlotJam project as described by Robert A. Gonsalves?\n",
            "GPT-4 Answer:     The main purpose of the PlotJam project, as described by he, is to use AI, specifically GPT-2, to generate plot summaries for books that don't exist yet. This can be particularly useful for writers who are experiencing writer's block and need some inspiration to get started.\n",
            "LLaMa 7B Answer:  The main purpose of the PlotJam project, as described by he, is to use Natural Language Processing (NLP) and artificial intelligence (AI) to create plot summaries of books that don't exist yet.\n",
            "LLaMa 7B Score:   0.9001566\n",
            "\n",
            "Index:            14\n",
            "Reference:        Acknowledgments\n",
            "I would like to thank Jennifer Lim for her help with this project. I also thank Oliver Strimpel, Penny Lennox, and Fez Aswat for reviewing the article and providing helpful feedback.\n",
            "Question:         What was the purpose of the VoxMorphia project by Robert A. Gonsalves?\n",
            "GPT-4 Answer:     The purpose of the VoxMorphia project by he was to use AI, specifically GPT-3, to reimagine classic songs in the style of different artists like Bob Marley, Radiohead, and Megan Thee Stallion.\n",
            "LLaMa 7B Answer:  The purpose of the VoxMorphia project by he is to demonstrate how artificial intelligence (AI) can be used for style transfer of song lyrics, specifically reimagining classic songs in the style of Bob Marley, Radiohead, and Megan Thee Stallion using Generative Pre-trained Transformer 3 (GPT-3).\n",
            "LLaMa 7B Score:   0.80312824\n",
            "\n",
            "Index:            15\n",
            "Reference:        From Landscapes to Abstract Paintings\n",
            "I remember learning in my Art History 101 class that many abstract painters started by painting figurative subjects, like people and landscapes. For example here are sequences of paintings by Mondrian, Klee, and Kandinsky that show their progressions from landscape to abstract art.\n",
            "Question:         What inspired Robert A. Gonsalves to create abstract art using StyleGAN2 ADA?\n",
            "GPT-4 Answer:     he was inspired by the progression of many abstract painters who started by painting figurative subjects, like people and landscapes, and then moved to abstract art. He mentioned the sequences of paintings by Mondrian, Klee, and Kandinsky as examples of this progression. This led him to use StyleGAN2 ADA to generate improved abstract paintings with AI.\n",
            "LLaMa 7B Answer:  According to the article \"Creating Abstract Art with StyleGAN2 ADA\" by he, he was inspired to create abstract art using StyleGAN2 ADA because of its ability to generate improved abstract paintings through Adaptive Discriminator Augmentation and Learning Transfer.\n",
            "LLaMa 7B Score:   0.8605746\n",
            "\n",
            "Index:            16\n",
            "Reference:        Background\n",
            "The origin of the Limerick form is unknown, but it is often attributed to having come from the town of Limerick, Ireland [2][3]. One of the first collections of limericks is “Anecdotes and Adventures of Fifteen Gentlemen” [4] by English illustrator and poet Edward Lear, published in 1821. Here is an example from the book.\n",
            "> A Tailor who sailed from Quebec, In a storm ventur’d once upon deck, But the waves of the sea, Were as strong as could be, And he tumbled in up to his neck. - Edward Lear\n",
            "Question:         What did Robert A. Gonsalves do to improve the speed of his AI training for rhyming?\n",
            "GPT-4 Answer:     To improve the speed of his AI training for rhyming, he upgraded to Pro from Colab, as the latter was slow.\n",
            "LLaMa 7B Answer:  According to the text, he upgraded from Colab to Pro to improve the speed of his AI training for rhyming.\n",
            "LLaMa 7B Score:   0.9535356\n",
            "\n",
            "Index:            17\n",
            "Reference:        Prior Work\n",
            "Apparently, I am not the only one looking to get a transformer to generate text with metered prose. For example, in his paper “Haiku Generation, A Transformer Based Approach, With Lots Of Control,” Giacomo Miceli notes that the typical Haiku meter pattern is not strictly followed [1].\n",
            "> Modern and especially English language haikus do not follow the 5–7–5 pattern very strictly, but usually adhere to a short-long-short form of around 10/12 words in total. — Giacomo Miceli\n",
            "\n",
            "\n",
            "Miceli’s Haikoo system creates some excellent prose, too, but only 1 of the 12 examples in the paper follows the [5, 7, 5] pattern. Note that I borrowed the first line to see where Deep Haiku would take it. Here they are.\n",
            "Miceli references another paper that directly addresses the meter of generated prose. It’s not for writing Haikus; it’s for writing limericks. In their paper, “There Once Was a Really Bad Poet, It Was Automated but You Didn’t Know It,” Jianyou Wang et al. discuss their early experiments using OpenAIs GPT-2 model.\n",
            "> A naïve implementation of GPT-2 simply cannot produce original and valid limericks. GPT-2 tends to generate long sentences that exceed the syllable limit for limericks. To meet a syllable constraint, we would need to truncate the generated sentences, which creates lines that do not end correctly. — Jianyou Wang, et al.\n",
            "\n",
            "\n",
            "They go on to describe a custom transformer for generating limericks, LimGen, that chooses words with constraints on part-of-speech, syllable counts, and placement of stressed syllables.\n",
            "For Deep Haiku, I built a system that generates Haikus from a user-specified prompt with adherence to a [5, 7, 5] meter by fine-tuning a general-purpose transformer.\n",
            "Question:         What was the main challenge faced by Jianyou Wang and his team when using GPT-2 for generating limericks?\n",
            "GPT-4 Answer:     The main challenge they faced was that GPT-2 tended to generate long sentences that exceeded the syllable limit for limericks. To meet the syllable constraint, they had to truncate the generated sentences, which resulted in lines that did not end correctly.\n",
            "LLaMa 7B Answer:  The main challenge faced by Jianyou Wang and his team when using GPT-2 for generating limericks was that the model tended to generate long sentences that exceeded the syllable limit for limericks, making it necessary to truncate the generated sentences, which resulted in lines that did not end correctly.\n",
            "LLaMa 7B Score:   0.9206872\n",
            "\n",
            "Index:            18\n",
            "Reference:        Appendix A — Gallery of MachineRay Results\n",
            "Question:         What was the purpose of Robert A. Gonsalves' project, MachineRay?\n",
            "GPT-4 Answer:     The purpose of MachineRay was to use artificial intelligence, specifically a GAN (Generative Adversarial Network), to create abstract art. he trained the AI using public domain paintings.\n",
            "LLaMa 7B Answer:  According to the given text, the purpose of he' project, MachineRay, was to train a Generative Adversarial Network (GAN) using public domain paintings in order to create abstract art.\n",
            "LLaMa 7B Score:   0.8514701\n",
            "\n",
            "Index:            19\n",
            "Reference:        Gathering Movie Plots\n",
            "On Kaggle, which hosts numerous datasets, I found a large list of movie plot summaries by Rounak Banik called  The Movie Dataset . It contains titles, release years, genres, summaries, etc., for over 40K movies. He released the dataset under the CC0 (Public Domain) license. Here is a sample of 5 entries.\n",
            "I used a module called KeyBERT [9] to extract the themes from the summaries. You can see my Python code  here .\n",
            "I found another  collection of datasets  on Kaggle from Shivam Bansal. He collected summaries of about 20K streaming shows on Netflix, Amazon, Hulu, and Disney+. Here is a sample of the data.\n",
            "I used KeyBERT again to capture themes from the summaries of the streaming shows.\n",
            "To teach GPT-J how to create titles and summaries from genres and themes, I assemble an entry like this for each movie and TV show.\n",
            "```\n",
            "GENRE: action science fiction THEME: saving the world TITLE: The Matrix SUMMARY: Set in the 22nd century, The Matrix tells the story of a computer hacker who joins a group of underground insurgents fighting the vast and powerful computers who now rule the earth. GENRE: comedy sitcom THEME: workplace comedy TITLE: 30 Rock SUMMARY: The life of the head writer at a late-night television variety show. From the creator and stars of SNL comes this workplace comedy. A brash network executive bullies head writer Liz Lemon into hiring an unstable movie star.\n",
            "```\n",
            "Question:         How does Robert use the KeyBERT module in the GreenLit project?\n",
            "GPT-4 Answer:     In the GreenLit project, he uses the KeyBERT module to extract themes from movie and TV show summaries. He applies this to datasets he found on Kaggle, which include a large list of movie plot summaries and a collection of summaries of streaming shows. The extracted themes, along with genres, titles, and summaries, are then used to train the GPT-J model to create new titles and summaries.\n",
            "LLaMa 7B Answer:  In the GreenLit project, he uses the KeyBERT module to extract themes from movie and TV show summaries. He applies this module to two different datasets: The Movie Dataset, which contains over 40,000 movie summaries, and a collection of streaming show summaries from Kaggle. By using KeyBERT, he is able to identify common themes in these summaries, which he can then use to train a GPT-J model to create new titles and summaries based on genres and themes.\n",
            "LLaMa 7B Score:   0.9600232\n",
            "\n",
            "Index:            20\n",
            "Reference:        Creating the Pre-Chorus\n",
            "A pre-chorus is a melody that leads into the chorus of a song. Note that the ChatGPT user interface included the context of previous interactions in the session, so I could be a bit more brief in my prompt to generate the melody for the pre-chorus.\n",
            "> Write a melody for the pre-chorus.\n",
            "> [Am] No more hiding, [C] no more doubts, [G] It’s time to let our [D] courage out.\n",
            "> GPT-4 Sure, I can provide a simple melody for the pre-chorus using the provided chord progression. Here’s the tab:\n",
            "\n",
            "\n",
            "```\n",
            "e|------0-0-0-0----|-0---------------|-----------------|-0---------------| B|----1----------|----3-1-1-1-1---|----1-3-3-1-3-3-|-------3-3-1-3-3-| G|--0--------------|---------------|----------------|----------------| D|-----------------|----------------|----------------|----------------| A|-----------------|----------------|----------------|----------------| E|-----------------|----------------|----------------|----------------|\n",
            "```\n",
            "> This melody uses the A minor, C Major, G Major, and D Major scales respectively. As always, feel free to modify this as you see fit for your performance.\n",
            "\n",
            "\n",
            "I’m sure it meant the word “chords” instead of “scales” in its response since the melody it generated is in G Major. Just to be sure, I asked for clarification.\n",
            "> RobG: Did you mean to use the word “scales”?\n",
            "> GPT-4:  My apologies for any confusion. I meant to say that the melody notes are based on the respective chords, not scales.\n",
            "\n",
            "\n",
            "OK, that cleared things up. As for the melody, I got the sense that the many notes it wrote could be interpreted as held notes. In other words, a string of four eighth notes could be played as a one half note. This reflects a limitation of the guitar tab format in that the note lengths are not expressed. Seen through this lens, the melody is good. Here is the melody for the pre-chorus, as written originally and after I converted some of the repeated notes to single ones with longer durations.\n",
            "OK, the second take sounds better with the held notes. Here are the lyrics and chords again for reference.\n",
            "[Am] No more hiding, [C] no more doubts, [G] It’s time to let our [D] courage out.\n",
            "Question:         How did Robert interpret the multiple notes generated by GPT-4 for the pre-chorus melody?\n",
            "GPT-4 Answer:     he interpreted the multiple notes as held notes, meaning a string of four eighth notes could be played as one half note. This interpretation was due to the limitation of the guitar tab format, which does not express note lengths. After converting some of the repeated notes to single ones with longer durations, he found the melody to be more pleasing.\n",
            "LLaMa 7B Answer:  Based on the conversation between he and GPT-4, it seems that he interpreted the multiple notes generated by GPT-4 as held notes, rather than individual notes. He suggested converting some of the repeated notes to single ones with longer durations, which suggests that he saw the melody as a series of sustained sounds rather than a succession of short notes.\n",
            "LLaMa 7B Score:   0.64485526\n",
            "\n",
            "Index:            21\n",
            "Reference:        Diversifying Character Names\n",
            "After some initial experiments, I noticed that the inclusion of the Friends scripts in the training dataset caused the model to use the names of the six central characters often. For example, the system would create period pieces set in the 18th century with characters named Joey, Phoebe, and Chandler.\n",
            "To diversify the character names, I swapped out the character names for each of the 236 Friends episodes. I used a list of  first names  collected by the US Social Security Office.\n",
            "For example, the script shown above used these character names for the training data:\n",
            "Ross → Lucas Chandler → Antonio Joey → Eddie Rachel → Charlotte Phoebe → Stella Monica → Luciana\n",
            "I also changed any references to “Central Perk” to “Coffee Shop” to help remove the “Friends-ness” of the scripts. Here’s the modified script:\n",
            "```\n",
            "SCRIPT : [Scene, Coffee Shop] LUCIANA - There's nothing to tell! He's just some guy I work with! EDDIE - C'mon, you're going out with the guy! There's gotta be something wrong with him! ANTONIO - All right Eddie, be nice. So does he have a hump? A hump and a hairpiece? STELLA - Wait, does he eat chalk? (They all stare, bemused.) STELLA - Just, 'cause, I don't want her to go through what I went through with Carl- oh! LUCIANA - Okay, everybody relax. This is not even a date. It's just two people going out to dinner and- not having sex. ANTONIO - Sounds like a date to me.\n",
            "```\n",
            "It’s interesting to see how simply changing the character names makes it seem like a different show.\n",
            "Question:         How did Robert diversify the character names in the scripts generated by GreenLit?\n",
            "GPT-4 Answer:     he diversified the character names by swapping out the names from the Friends scripts used in the training dataset. He used a list of first names collected by the US Social Security Office to replace the original character names. For instance, Ross was changed to Lucas, Chandler to Antonio, Joey to Eddie, Rachel to Charlotte, Phoebe to Stella, and Monica to Luciana. He also replaced any references to “Central Perk” with “Coffee Shop” to further remove the association with Friends.\n",
            "LLaMa 7B Answer:  he diversified the character names in the scripts generated by GreenLit by swapping out the character names from the Friends series with names collected from a list of first names provided by the US Social Security Office. This helped to create more diverse character names in the generated scripts, removing any association with the \"Friends-ness\" of the original characters.\n",
            "LLaMa 7B Score:   0.7140132\n",
            "\n",
            "Index:            22\n",
            "Reference:        Adding Punctuation and Capitalization to Text with FastPunct\n",
            "I noticed that the Haikus in the first dataset were all lowercase and without punctuation. I know that writing poems this way is a style choice, but I decided to add uppercase letters and punctuation because it helps with the text quality analysis in the next step. I used the  FastPunct  module for this.\n",
            "Here’s some sample code in Python that adds punctuation and casing to one of the Haikus in the dataset.\n",
            "```\n",
            "from fastpunct import FastPunct fastpunct = FastPunct() print(fastpunct.punct([\"\"\" was it all a dream                           i most certainly hope not                           that was happiness \"\"\"])) # Output: #  Was it all a dream? #  I most certainly hope not. #  That was happiness.\n",
            "```\n",
            "Notice how capitalizing the first word of each sentence also recognizes the first line as a question and punctuates it accordingly. Here is a couple of Haikus from the dataset to illustrate what FastPunct does.\n",
            "Notice how FastPunct did a little more work with these samples. The last example added a comma after “hide” and put an apostrophe before the contracted “cause.”\n",
            "Some of the Haikus in the second dataset have casing and punctuation, but many don’t. So I stripped the punctuation and case and put it back in with FastPunct for consistency. Here’s what some examples look like from this dataset.\n",
            "Combining the datasets gave me over 150K Haikus. I then filtered them down using the methods described in the following steps.\n",
            "Question:         How did Robert A. Gonsalves improve the quality of the Haikus in the dataset for the Deep Haiku project?\n",
            "GPT-4 Answer:     For the Deep Haiku project, he noticed that the Haikus in the first dataset were all lowercase and without punctuation. To improve the text quality, he decided to add uppercase letters and punctuation using the FastPunct module. He also applied the same process to the second dataset for consistency, even though some of the Haikus already had casing and punctuation. This resulted in a combined dataset of over 150K Haikus.\n",
            "LLaMa 7B Answer:  Hello! I'm here to help you with your question. To answer your question, he improved the quality of the Haikus in the dataset for the Deep Haiku project by adding punctuation and capitalization to the text. He used a module called FastPunct to add punctuation and casing to the Haikus in the dataset. This helped to improve the overall quality of the text and make it easier to analyze in the next step.\n",
            "LLaMa 7B Score:   0.8130896\n",
            "\n",
            "Index:            23\n",
            "Reference:        Hands-on Tutorials\n",
            "Sep 1, 2021\n",
            "AITunes - https://medium.com/towards-data-science/ai-tunes-creating-new-songs-with-artificial-intelligence-4fb383218146\n",
            "More\n",
            "I have been experimenting with various AI-based music generation systems for over a year now, and I hate to say it, but most of the music generated by AI sounds like junk. It’s either too complicated and rambling or too simple and repetitive. It almost never has a pleasing melody with a global structure to the composition.\n",
            "We have witnessed recent strides in AI-based content creation for other forms of media, like images and text. So it is confounding that the quality of AI-generated music seems so far behind.\n",
            "When I heard that OpenAI released an API for fine-tuning their GPT-3 text processing models [1], my first thought was, aha!, maybe I can use it to create music that doesn’t suck. It took a little work, but I managed to wrangle some training data from music in the public domain and used it to teach GPT-3 how to compose music. I will explain what I did fully in this article, but first, here is a sample that I cherry-picked from five generated songs.\n",
            "OK, it’s not great, but at least to my ears, it sounds interesting and, dare I say, good?\n",
            "Up next is an overview of the system, which I dubbed AI-Tunes.\n",
            "Question:         What motivated Robert to fine-tune OpenAI's GPT-3 for music generation?\n",
            "GPT-4 Answer:     he was motivated by the lack of quality in AI-generated music. He found most of the AI-generated music to be either too complicated, rambling, too simple, or repetitive. Given the advancements in AI-based content creation for other forms of media, he was inspired to use OpenAI's GPT-3 to create music that was more pleasing and had a global structure.\n",
            "LLaMa 7B Answer:  he was motivated to fine-tune OpenAI's GPT-3 for music generation because he was disappointed with the quality of AI-generated music, which he found to be either too complicated or too simple and repetitive. He wanted to see if he could use the API to create music that sounds pleasing and has a global structure to the composition.\n",
            "LLaMa 7B Score:   0.9364232\n",
            "\n",
            "Index:            24\n",
            "Reference:        get the features from the text query using CLIP text_query = 'an abstract painting with orange triangles' query_features = clip.encode_text(text_query)  get an initial population of images, i.e. 200 latents = random(num_initial_samples)  run the genetic algorithm for some number of steps, i.e. 5 times for g in range(num_generations):    generate the images   images = swagan.generate_images(latents)    get the image features using CLIP and rank them to get the top 4   image_features = clip.encode_image(images)   ranked_indices = get_top_N_(query_features, image_features, 4)    start with a blank slate   new_latents = empty_set    copy the best four latents (A, B, C, D)   for i in range(4):     offspring = latents[ranked_indices[i]]     new_latents.append(offspring)    loop through all 16 combinations   for j in range(4):     for i in range(4):        combine i with j       offspring = (1-recombination_amount) * new_latents[i] +            recombination_amount * new_latents[j]               add a little mutation       offspring += random() * mutation_amount                add it to the batch       new_latents.append(offspring)       use the new latents   latents = new_latents\n",
            "```\n",
            "OK, let’s see how this works with a real example.\n",
            "The parameters are set like this:  text_query = ‘an abstract painting with orange triangles’  num_initial_samples = 200  recombination_amount =0.333  mutation_amount = 0.25  num_generations = 5\n",
            "After generating 200 images, here are the top four that match the query.\n",
            "The images definitely show a lot of orange colors, and some triangles can be seen. Here’s what the first generation produces. The letter “x” indicates crossover, and the symbol ’ indicates mutation.\n",
            "The original four images are at the top, and the combinations with mutations are below. This system will look at all 20 images (the original 4 plus the 16 combinations) and use CLIP to determine the top 4 for the next generation.\n",
            "I ran the GA for five generations. If you are curious, you can see all of the intermediate results in the Colab  here . Below is the final result.\n",
            "Not bad! You can see that the final result is mostly based on the initial top two images, but the system managed to find some additional orange triangles during the run of the genetic algorithm.\n",
            "Up next, I’ll share a technique that I developed that improves the results when using GAs with GANs.\n",
            "Question:         How does the MAGnet project use a genetic algorithm to create modern paintings from text descriptions?\n",
            "GPT-4 Answer:     The MAGnet project uses a genetic algorithm in conjunction with CLIP and SWAGAN to generate modern paintings from text descriptions. The process begins by encoding the text query using CLIP to get the features. An initial population of images is generated, and the genetic algorithm is run for a certain number of steps. The images are generated using SWAGAN and their features are encoded using CLIP. The images are then ranked and the top four are selected. These top four images undergo a process of recombination and mutation to create new images. This process is repeated for a set number of generations. The final result is a unique image that is based on the initial text description.\n",
            "LLaMa 7B Answer:  The MAGnet project uses a genetic algorithm (GA) to generate modern paintings from text descriptions. The GA works by creating an initial population of images, then iteratively evolving the population through crossover and mutation operations. The algorithm uses CLIP (Contrastive Language-Image Pre-training) to encode both the text query and the images, and then ranks the images based on how well they match the query. The top four images are selected for the next generation, and the process is repeated until a satisfactory result is achieved.\n",
            "LLaMa 7B Score:   0.9545512\n",
            "\n",
            "Index:            25\n",
            "Reference:        Discussion and Next Steps\n",
            "As I have shown, ChatGPT knows a lot about music theory and can generate chord progressions for various styles of music and provide color commentary for its choices. It can also take feedback to create variations of the composed songs and provide additional comments. It occasionally makes mistakes, like giving contradictory and seemingly incorrect recommendations for chords. But the results sounded just fine in the two cases where that happened.\n",
            "One thing I didn’t try was showing ChatGPT the chords to an original song as a work in progress and asking it to suggest some improvements or add a new part. Based on my experience with it, the results would be pretty good.\n",
            "The system has a long way to go for composing melodies, however. My initial try at creating a melody for a blues song did not sound good at all.\n",
            "Question:         What are some of the limitations Robert A. Gonsalves identified in using ChatGPT for music composition?\n",
            "GPT-4 Answer:     he identified that while ChatGPT can generate chord progressions and provide commentary on its choices, it occasionally makes mistakes such as giving contradictory and seemingly incorrect recommendations for chords. He also noted that the system has a long way to go in terms of composing melodies, as his initial attempt at creating a melody for a blues song did not yield satisfactory results.\n",
            "LLaMa 7B Answer:  Based on the article \"Using ChatGPT as a Creative Writing Partner — Part 2: Music\" by he, some limitations of using ChatGPT for music composition include:\n",
            "1. Generating melodies: While ChatGPT can create chord progressions and provide color commentary, it struggles with composing melodies, as demonstrated in the author's attempt to create a melody for a blues song.\n",
            "2. Making mistakes: The AI model occasionally provides contradictory or seemingly incorrect recommendations for chords, which can result in suboptimal compositions.\n",
            "3. Limited creativity: While ChatGPT can generate new ideas based on the input provided, it may not be able to come up with entirely original or innovative melodies or chord progressions.\n",
            "LLaMa 7B Score:   0.7572725\n",
            "\n",
            "Index:            26\n",
            "Reference:        Final Thoughts and Future Steps\n",
            "The results of the DALL-E Mini text-to-image model appear to be very good. And VQGAN is a versatile image rendering model with many unexpected uses, like expanding images to change the aspect ratio.\n",
            "I may try using VQGAN as a general inpainting tool in a future project. Although the system will quantize the masked area to 16x16 pixel blocks, it may be possible to blend back in both the original latent vectors and the original pixels outside the masked regions. And using CLIP as a guiding text encoder, it might be possible to perform a function like “draw a party hat on the pug.” 😀\n",
            "Question:         What future applications does Robert A. Gonsalves see for the VQGAN model?\n",
            "GPT-4 Answer:     he envisions using VQGAN as a general inpainting tool in future projects. He believes that despite the system quantizing the masked area to 16x16 pixel blocks, it might be possible to blend back in both the original latent vectors and the original pixels outside the masked regions. Furthermore, with the use of CLIP as a guiding text encoder, he suggests that it might be possible to perform functions like \"draw a party hat on the pug.\"\n",
            "LLaMa 7B Answer:  According to he, the author of the article \"E-DALL-E: Creating Digital Art with Varying Aspect Ratios,\" he sees potential future applications for the VQGAN model as a general inpainting tool. Specifically, he suggests using VQGAN to blend back in both the original latent vectors and the original pixels outside the masked regions when inpainting images expanded with DALL-E Mini. This could potentially enable the creation of more detailed and realistic images with varying aspect ratios.\n",
            "LLaMa 7B Score:   0.76798594\n",
            "\n",
            "Index:            27\n",
            "Reference:        Creating Variations and Remixes\n",
            "After creating four thumbnails with a text prompt, the system allows you to make additional variations using the V1-V4 buttons. For example, for the prompt, “a man and woman holding an umbrella on a city street in the rain,” the system generates four thumbnails. After choosing V2 for the upper right, it generates four variations. Note that I used version four for this and all of the following experiments.\n",
            "The variations all look pretty good. Since version 3 of Midjourney, you can modify the text prompt when creating variations. The feature is called Remix mode and is available in the settings. For example, I added the following text to the prompts using Remix mode: “1880s”, “1950s”, and “future.” Here are the results.\n",
            "Sure enough, the system renders the image with distinctive visual looks associated with the specified time periods.\n",
            "Question:         How does Robert A. Gonsalves use the Remix mode in Midjourney Art?\n",
            "GPT-4 Answer:     he uses the Remix mode in Midjourney Art to modify the text prompt when creating variations of an image. He demonstrates this by adding different time periods like \"1880s\", \"1950s\", and \"future\" to the prompts, resulting in the system rendering the image with distinctive visual looks associated with the specified time periods.\n",
            "LLaMa 7B Answer:  In his article \"Exploring Midjourney V4 for Creating Digital Art,\" he explains how he uses the Remix mode in Midjourney Art to create variations of an image based on a modified text prompt. He adds specific time periods to the original prompt using Remix mode, and the system generates images with distinctive visual looks associated with those time periods.\n",
            "LLaMa 7B Score:   0.8695078\n",
            "\n",
            "Index:            28\n",
            "Reference:        A Roll of the Dice\n",
            "To get a better understanding of how distributions follow BL, we’ll take a look at three simulations using dice rolls.\n",
            "Imagine that you are taking an online statistics class. It’s a big class. There are 10,000 students. The teacher asks each student to roll a six-sided die and enter their result into a spreadsheet. Here is a histogram of the results.\n",
            "With 10,000 rolls and six possible outcomes for each roll, the expected result would be about 1,666 rolls per outcome. The results above are close to this, ranging from 1,624 for the number 1, to 1,714 for the number 5. This is an approximation of an even distribution.\n",
            "Question:         What does Robert use as an example to explain how distributions follow Benford's Law?\n",
            "GPT-4 Answer:     he uses the example of an online statistics class with 10,000 students. Each student is asked to roll a six-sided die and enter their result into a spreadsheet. The expected result would be about 1,666 rolls per outcome, which is an approximation of an even distribution.\n",
            "LLaMa 7B Answer:  In the article \"Benford's Law - A Simple Explanation\" by he, he uses three simulations using dice rolls to illustrate how distributions can follow Benford's Law. The examples are:\n",
            "1. A class of 10,000 students rolling a six-sided die and entering their results into a spreadsheet, resulting in a histogram close to an even distribution.\n",
            "In this example, the expected number of rolls per outcome is approximately 1,666, and the actual results are close to this range, indicating a roughly even distribution.\n",
            "LLaMa 7B Score:   0.6056303\n",
            "\n",
            "Index:            29\n",
            "Reference:        Don’t repeat yourself. Don’t repeat yourself.\n",
            "Sometimes the GPT-2 text generator gets stuck on a particular phrase and repeats it multiple times in a plot summary. You can see some repetition in this example.\n",
            "```\n",
            "GENRE : Science Fiction  TITLE : Baseline Behavior  PLOT : In the preface, Guy Sager explains his term for the branch of psychology that emphasizes  the individual’s sense of themselves  vs. the collective and their social relations. He believes that  the individual’s sense of themselves  can be affected by the environment (via the “environment in which they find themselves”). In other words,  the individual’s sense of themselves  can be saved by the environment (via their …\n",
            "```\n",
            "Notice how the phrase “the individual’s sense of themselves” is repeated three times. I wrote a little function that looks for this artifact to filter out entries that have repetition. Here’s the Python code.\n",
            "If the function sees a phrase of a given length in the input one or more times, it gets flagged with repetition. I use the phrase length of five to filter out repetitive plot summaries.\n",
            "Question:         How does Robert A. Gonsalves address the issue of repetition in the plot summaries generated by PlotJam?\n",
            "GPT-4 Answer:     he addresses this issue by creating a Python function that identifies and flags any repeated phrases in the generated plot summaries. This function works by detecting if a phrase of a given length appears more than once in the input. He uses a phrase length of five to filter out repetitive plot summaries, thus enhancing the quality and uniqueness of the generated content.\n",
            "LLaMa 7B Answer:  he, the author of the article \"Got Writer's Block? It's PlotJam to the Rescue!\", addresses the issue of repetition in the plot summaries generated by PlotJam by using a custom function to identify and filter out repetitive phrases. The function checks for repeated phrases of a specific length (five characters) in the input text, and flags any entries with repetition. This helps to reduce the amount of repetition in the generated plot summaries and improve their overall quality.\n",
            "LLaMa 7B Score:   0.8209743\n",
            "\n",
            "Index:            30\n",
            "Reference:        DALL-E Mini\n",
            "Developers Boris Dayma and Pedro Cuenca led a team to create a free, open-source text-to-image generator called DALL-E Mini [2]. The system uses a pair of trained BERT Transformers [10] to convert text into latent vectors that can render images using VQGAN. As the name implies, the DALL-E Mini model is relatively small. It “only” has 400 million parameters and was trained on “only” 15 million images.\n",
            "Here is the diagram of the inferencing pipeline for DALL-E Mini.\n",
            "The text prompt is fed into BART and then decoded to produce samples of latent vectors for VQGAN, which renders the candidate images. Note that the system does not generate the images iteratively. It transforms into vectors used by VQGAN to render the images directly. A complete explanation of how DALL-E Mini works is  here .\n",
            "Here are some sample images for the prompts, “a painting of rolling farmland,” “an abstract painting with orange triangles,” and “a still life painting of a bowl of fruit.”\n",
            "Although they’re not as good as the images from DALL-E 2, the output from DALL-E Mini is decent. There is a good amount of variety in the images, and each composition is generally sound, albeit lacking in fine details.\n",
            "Next, I will show you how to expand the output to have different aspect ratios.\n",
            "Question:         What is the primary function of DALL-E Mini as described by Robert A. Gonsalves?\n",
            "GPT-4 Answer:     DALL-E Mini, as explained by he, is a free, open-source text-to-image generator. It uses a pair of trained BERT Transformers to convert text into latent vectors, which are then used by VQGAN to render images. The system does not generate the images iteratively but transforms the text directly into vectors for image rendering. He also mentions that the model is relatively small with 400 million parameters and was trained on 15 million images.\n",
            "LLaMa 7B Answer:  According to he, the primary function of DALL-E Mini is to convert text into latent vectors that can render images using VQGAN.\n",
            "LLaMa 7B Score:   0.67712474\n",
            "\n",
            "Index:            31\n",
            "Reference:        Using GPT-4 to Create Chords for Songs\n",
            "In my previous article, I asked GPT-4 to create lyrics for a rock song about cats and dogs. Here’s my first attempt to get the model to create chords for this song.\n",
            "> RobG: Write chords for the first verse of this rock song.\n",
            "> Dogs and cats, they roam the streets Furry friends with hearts that beat Claws and paws, they rule the land Tails a-waggin’, they understand\n",
            "> Side by side, they chase the wind Bound together, best of friends They’re rockin’ in the moonlit night Dogs and cats, they’re out of sight\n",
            "> GPT-4: Here are the chords for the first verse of the song:\n",
            "> G C Dogs and cats, they roam the streets Em D Furry friends with hearts that beat G C Claws and paws, they rule the land Em D Tails a-waggin’, they understand\n",
            "> Am C Side by side, they chase the wind G D Bound together, best of friends G C They’re rockin’ in the moonlit night Em D Dogs and cats, they’re out of sight\n",
            "\n",
            "\n",
            "The chords seem pretty good, albeit a tad basic. Like most Large Language Models, GPT-4 was trained on lots of text from books and the Internet and it seems to have picked up on enough discussions about chords to generate decent sequences. I like the AABA structure of the progression: G — C — Em — D, G — C — Em — D,  Am  —  C  —  G  —  D,  G — C — Em — D.\n",
            "However, there is an issue with text formatting. The chords don’t line up with the phrasing of the lyrics, i.e., the “D” chord should probably line up with “they’re” or “out” on the last line. I found an alternative approach that works fine: put the chords in line with the lyrics. I just had to ask.\n",
            "> RobG: Show the chords in line with the lyrics in square brackets separated by spaces.\n",
            "> GPT-4: [G] Dogs and cats, they [C] roam the streets [Em] Furry friends with [D] hearts that beat [G] Claws and paws, they [C] rule the land [Em] Tails a-waggin’, [D] they understand\n",
            "> [Am] Side by side, they [C] chase the wind [G] Bound together, [D] best of friends [G] They’re rockin’ in the [C] moonlit night [Em] Dogs and cats, they’re [D] out of sight\n",
            "\n",
            "\n",
            "Ah, much better. Having the D chord start before “out” for the last line makes sense because “they’re” is used as a pick-up note before the next chord starts.\n",
            "In addition to generating chord progression, GPT-4 also explains things related to music theory. So I occasionally asked it questions about the songs I was working on, like this.\n",
            "> RobG: What key is this in?\n",
            "> GPT-4:  The chords provided for the rock song are in the key of G major. The G major key consists of the following chords: G (I), Am (ii), Bm (iii), C (IV), D (V), Em (vi), and F#dim (vii°).\n",
            "\n",
            "\n",
            "That seems correct. And I appreciated the full explanation of the chords used in the key of G major. Notice how it used both the letter names of the chords and the Roman numeral notation, where major chords are capitalized, i.e., (I), and minor chords are in lowercase, i.e., (ii.) I liked the way it used the ‘°’ character for the diminished chord.\n",
            "I used a commercial app called Band-in-a-Box (BiaB) from  PG Music  to hear what the chords sounded like. I typed in the chord names, pasted in the corresponding lyrics, chose a “rock” style, and it rendered the song. Here it is. Note that there is no melody, so feel free to sing along with your tune.\n",
            "That sounded pretty good. The chord sequence was fine, and BiaB did an excellent job with the guitar, bass, and drum parts. Without the vocal melody, the best parts of the tune were the guitar fills at the end of each phrase.\n",
            "Up next, I’ll show how I used AI to extract both the lyrics and chords from recordings of songs.\n",
            "Question:         How did Robert A. Gonsalves use GPT-4 to create chords for a song?\n",
            "GPT-4 Answer:     he used GPT-4 to generate chords for a song by inputting the lyrics and asking the model to provide the chords. He then adjusted the formatting by asking the model to place the chords in line with the lyrics. He also used GPT-4 to determine the key of the song. After generating the chords, he used a commercial app called Band-in-a-Box to hear what the chords sounded like with the lyrics.\n",
            "LLaMa 7B Answer:  he used GPT-4 to generate chords for a rock song by providing the model with the lyrics and asking it to create chords that fit the rhythm and melody of the song. The model generated a chord progression in the key of G major, which Gonsalves found to be decent but a bit basic. He then asked the model to show the chords in line with the lyrics, and the model provided them in square brackets separated by spaces. Additionally, GPT-4 explained things related to music theory, such as the key of the song and the chord progression used.\n",
            "LLaMa 7B Score:   0.83841264\n",
            "\n",
            "Index:            32\n",
            "Reference:        Generating a Limerick\n",
            "With the trained system, I could use it interactively to create a limerick given a topic. First, I started a rhyme set given a topic, then I made the limerick, line by line.\n",
            "Question:         What did Robert A. Gonsalves do to improve the speed of his AI Limericks project?\n",
            "GPT-4 Answer:     To improve the speed of his AI Limericks project, he upgraded to Pro on Colab.\n",
            "LLaMa 7B Answer:  According to the text, he upgraded from Colab to Pro to improve the speed of his AI Limericks project.\n",
            "LLaMa 7B Score:   0.9393681\n",
            "\n",
            "Index:            33\n",
            "Reference:        Discussion\n",
            "Without any specific prompting to steer it in a different direction, GPT-4 seems to be working from a formula when composing songs. For example, it tends to use an  (A) verse, (B) chorus, (A) verse, (B) chorus, (C) bridge, (B) chorus, and (A) outro  form for the songs. And it always uses a four-chord pattern for the song parts.\n",
            "By default, it will stick with the common diatonic chords in the key, but, as you saw above, you can steer it to use chromatic chords. In general, the chord patterns it creates sound good. I didn’t use the “regenerate” command at all. And the patterns seem unique, with occasional sparks of innovation.\n",
            "In the discussion at the end of my last article, I hypothesized that most music fans wouldn’t care much if they found out that songwriters were using AI systems to help compose lyrics for songs. This would probably be true for composing chords, too. If, in the end, the songs are good, then the fans will like them, regardless of the tools that were used to create them.\n",
            "Question:         What is the typical song structure that GPT-4 seems to follow when composing songs, according to Robert A. Gonsalves?\n",
            "GPT-4 Answer:     According to him, GPT-4 typically follows a formulaic structure when composing songs. This structure often includes a verse (A), chorus (B), verse (A), chorus (B), bridge (C), chorus (B), and an outro (A). It also consistently uses a four-chord pattern for the song parts.\n",
            "LLaMa 7B Answer:  According to he, the typical song structure that GPT-4 tends to follow when composing songs is an (A) verse, (B) chorus, (A) verse, (B) chorus, (C) bridge, (B) chorus, and (A) outro form. Additionally, it tends to use a four-chord pattern for the song parts by default, but can be steered to use chromatic chords with some prompting.\n",
            "LLaMa 7B Score:   0.9589962\n",
            "\n",
            "Index:            34\n",
            "Reference:        Component Details\n",
            "In March 2022, OpenAI released a series of AI models for image creation called GLIDE, a so-called diffusion model, an alternative to Generative Adversarial Networks (GANs). Here is an explanation of how diffusion models work by two engineers at Google Research.\n",
            "> Diffusion models work by corrupting the training data by progressively adding Gaussian noise, slowly wiping out details in the data until it becomes pure noise, and then training a neural network to reverse this corruption process. Running this reversed corruption process synthesizes data from pure noise by gradually denoising it until a clean sample is produced. This synthesis procedure can be interpreted as an optimization algorithm that follows the gradient of the data density to produce likely samples. — Jonathan Ho and Chitwan Saharia [3]\n",
            "\n",
            "\n",
            "Diffusion models are basically noise reduction models that have been trained for so long that they generate new images given pure noise as input.\n",
            "OpenAI’s GLIDE is based on their earlier investigation into using diffusion models for image synthesis. Their 2021 paper, with a bold statement in the title,  Diffusion Models Beat GANs on Image Synthesis , showed that diffusion models conditioned on classes of images could achieve better image quality than state-of-the-art generative models [4]. In their latest paper,  GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models , the authors…\n",
            "> … observe that GLIDE with classifier-free guidance is capable of generalizing to a wide variety of prompts. The model often generates realistic shadows and reflections, as well as high-quality textures. It is also capable of producing illustrations in various styles, such as the style of a particular artist or painting, or in general styles like pixel art. — Alex Nichol et al. [1]\n",
            "\n",
            "\n",
            "For BIG.art, I use the GLIDE image generator to take in a text prompt and produce a series of seven 64x64 images. The system attempts to depict what is described in the prompt. I then feed the images and the prompt into the GLIDE upsampler that increases the resolution to 256x256. The system was trained to use the prompt to help add detail when resizing up.\n",
            "For example, GLIDE produces the following seven images from the prompt “a seascape with crashing waves.”\n",
            "OK, those looked pretty good. Here’s another set for “Boston city skyline.”\n",
            "These kinda-sorta look like Boston, but not exactly. In any case, I'll use the fourth one for the resizing discussion below.\n",
            "Note that OpenAI released trained GLIDE models that cannot create images of people. The authors state…\n",
            "> … releasing our model without safeguards would significantly reduce the skills required to create convincing disinformation or Deepfakes. … In order to mitigate potentially harmful impacts of releasing these models, we filtered training images … containing people to reduce the capabilities of the model in many people-centric problematic use cases. — Alex Nichol et al. [1]\n",
            "Question:         How does Robert use the GLIDE image generator in the BIG.art project?\n",
            "GPT-4 Answer:     In the BIG.art project, he uses the GLIDE image generator to take in a text prompt and produce a series of seven 64x64 images. The system attempts to depict what is described in the prompt. He then feeds the images and the prompt into the GLIDE upsampler that increases the resolution to 256x256. The system was trained to use the prompt to help add detail when resizing up.\n",
            "LLaMa 7B Answer:  In the BIG.art project, he uses the GLIDE image generator to take in a text prompt and produce a series of seven 64x64 images. The system attempts to depict what is described in the prompt, and then the images are fed into the GLIDE upsampler to increase the resolution to 256x256. This allows the system to add detail when resizing up, using the prompt to help guide the process.\n",
            "LLaMa 7B Score:   0.97240937\n",
            "\n",
            "Index:            35\n",
            "Reference:        VoxMorphia: Using AI for Style Transfer of Song Lyrics\n",
            "Robert A. Gonsalves\n",
            "May 2, 2021\n",
            "Voxmorphia - https://medium.com/towards-data-science/voxmorphia-using-ai-for-style-transfer-of-song-lyrics-fe22d52128dd\n",
            "For this experiment, which I call VoxMorphia, I chose to transform songs that are in the public domain, including classic tunes like “Yes Sir, That’s My Baby.” Using GPT-3 and some additional open-source projects, I adapted the words of classic songs to be in the lyrical style of Bob Marley, Thom Yorke from Radiohead, Megan Thee Stallion, and 43 other songwriters.\n",
            "I chose to highlight these three recording artists to show a range of styles: Bob Marley’s smooth melodic lines over vintage reggae rhythms, Thom Yorke’s angst-filled ramblings over complex alt-rock patterns, and Megan Thee Stallion’s clever and sexually explicit rapping over sweet hip hop beats.\n",
            "Question:         What was the purpose of Robert A. Gonsalves' project, VoxMorphia?\n",
            "GPT-4 Answer:     The purpose of VoxMorphia was to use AI, specifically GPT-3, to transform the lyrics of public domain songs into the lyrical styles of various artists, including Bob Marley, Thom Yorke from Radiohead, and Megan Thee Stallion. He aimed to showcase a range of styles through this experiment.\n",
            "LLaMa 7B Answer:  According to the provided text, the purpose of he' project, VoxMorphia, was to use artificial intelligence (AI) for style transfer of song lyrics by adapting the words of classic songs to be in the lyrical style of various recording artists, including Bob Marley, Thom Yorke from Radiohead, and Megan Thee Stallion.\n",
            "LLaMa 7B Score:   0.8661428\n",
            "\n",
            "Index:            36\n",
            "Reference:        Training StyleGAN2 ADA\n",
            "I trained the GAN on Google Colab Pro for about three weeks. Because the Colab times out every 24 hours, I kept the results on my Google Drive and pick up where it left off. Here is the command I used to kick off the training:\n",
            "```\n",
            "!python stylegan2-ada/train.py --aug=ada --mirror=1 --metrics=none --snap=1 --gpus=1 \\ --data=/content/drive/MyDrive/GANscapes/dataset_1024 \\ --outdir=/content/drive/MyDrive/GANscapes/models_1024\n",
            "```\n",
            "I noticed an issue with the ADA variable  p , which determines how much image augmentation is used during training. Because the  p  value always starts at zero, it takes the system a while each day to climb back up to around 0.2. I fixed this in  my fork of StyleGAN2 ADA  to allow  p  to be set when the augmentation is set to ADA (the implementation in NVidia’s repo will throw an error if  p  is set when using ADA.)\n",
            "Here’s the command I used on subsequent restarts.\n",
            "```\n",
            "!python stylegan2-ada/train.py --aug=ada --p 0.186 --mirror=1 \\ --metrics=none --snap=1 --gpus=1 \\ --data=/content/drive/MyDrive/GANscapes/dataset_1024 \\ --outdir=/content/drive/MyDrive/GANscapes/models_1024 \\ --resume=/content/drive/MyDrive/GANscapes/models_1024/00020-dataset_1024-mirror-auto1-ada-p0.183-resumecustom/network-snapshot-000396.pkl\n",
            "```\n",
            "I replaced 0.186 with the last used  p  value and the  resume  path to the previously saved model.\n",
            "Here are some samples from the trained GANscapes system.\n",
            "Nice! Some of the generated paintings look more abstract than others, but overall, the quality seems pretty good. The system tends to depict natural items well, like trees, water, clouds, etc. However, it seems to be struggling a bit with buildings, boats, and other human-made objects.\n",
            "Question:         How did Robert manage to continue the training of the GANscapes system on Google Colab Pro despite its 24-hour timeout limit?\n",
            "GPT-4 Answer:     he managed to continue the training of the GANscapes system on Google Colab Pro by saving the results on his Google Drive. This allowed him to pick up where he left off after each timeout. He also modified the StyleGAN2 ADA to allow the ADA variable 'p', which determines the level of image augmentation during training, to be set when the augmentation is set to ADA. This prevented the system from having to climb back up to around 0.2 each day, thus saving time and resources.\n",
            "LLaMa 7B Answer:  he was able to continue training the GANscapes system on Google Colab Pro despite its 24-hour timeout limit by keeping the results of each training session on his Google Drive and picking up where the previous session left off. This allowed him to avoid losing any progress due to the timeout and complete the training over a longer period of time.\n",
            "LLaMa 7B Score:   0.82764935\n",
            "\n",
            "Index:            37\n",
            "Reference:        Exploring DALL-E for Digital Art Creation\n",
            "Robert A. Gonsalves\n",
            "Aug 8, 2022\n",
            "DALL-E Art - https://medium.com/towards-data-science/exploring-dall-e-for-digital-art-creation-b244e1a2ed12\n",
            "I was very excited to get my invitation to OpenAI’s DALL-E [1] text-to-image generation beta program. Since signing up, I have created over 200 images with various text prompts from my earlier AI projects. As you can see from the sample images, the quality and resolution of the images generated by DALL-E are excellent.\n",
            "I’ll start with some background info on DALL-E and then show the results compared to my early experiments. You can tell which images were created by DALL-E by checking for the small watermark in the lower-right corner of the renderings. (Note that it’s comprised of muted shades of yellow, cyan, green, red, and blue. I guess magenta didn’t make the cut.)\n",
            "Question:         What does Robert A. Gonsalves think about the quality and resolution of the images generated by DALL-E?\n",
            "GPT-4 Answer:     He believes that the quality and resolution of the images generated by DALL-E are excellent.\n",
            "LLaMa 7B Answer:  According to he, the quality and resolution of the images generated by DALL-E are excellent. He notes that the images have a high level of detail and resolution, and he is impressed with the quality of the generated images.\n",
            "LLaMa 7B Score:   0.8770166\n",
            "\n",
            "Index:            38\n",
            "Reference:        References\n",
            "[1] V. Whitmire,  International Photography Hall of Fame: Eadweard Muybridge  (2017)\n",
            "\n",
            "[2] P. Esser et al.  Structure and Content-Guided Video Synthesis with Diffusion Models  (2023)\n",
            "\n",
            "[3] R. Ranftl et al.  Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer  (2020)\n",
            "Question:         What was the main objective of Robert A. Gonsalves in the Muybridge Derby project?\n",
            "GPT-4 Answer:     In the Muybridge Derby project, he aimed to transform Eadweard Muybridge’s photo sequences into high-resolution videos using Midjourney and RunwayML, two artificial intelligence tools.\n",
            "LLaMa 7B Answer:  The main objective of he in the Muybridge Derby project was to transform Eadweard Muybridge's photo sequences into high-resolution videos using Midjourney and RunwayML.\n",
            "LLaMa 7B Score:   0.89189816\n",
            "\n",
            "Index:            39\n",
            "Reference:        Creating a Movie\n",
            "I found that I could create videos of the images by using VQGAN and CLIP by simply using a generic text prompt like “nightmare” and reducing the learning rate of the optimizer. I turned down the learning rate to 1% to create the video, so the frames will change slowly. Here is a diagram that shows the components for making a movie.\n",
            "This time the system starts with the modified image created by VQGAN and is sent into the CLIP image encoder. The prompt is simply “nightmare.” The system runs for 300 frames, which generates 10 seconds of video at 30 frames per second.\n",
            "The ffmpeg codec is used to generate an mp4 movie file. Here is the command to create the movie.\n",
            "```\n",
            "ffmpeg -r 30 -i /content/steps/%4d.png -c:v libx264 witch.mp4\n",
            "```\n",
            "The -r option specifies the frame rate of 30 fps. The -i option indicates where the mages are, and the -c-v option specifies the H.264 video codec.\n",
            "Here’s the video.\n",
            "You can see how the witch’s face morphs into what looks like a skull with teeth in the eyes. It sure looks like a nightmare to me! 😲\n",
            "Question:         How does Robert A. Gonsalves create a video using the SpookyGAN system?\n",
            "GPT-4 Answer:     he creates a video using the SpookyGAN system by using a generic text prompt like “nightmare” and reducing the learning rate of the optimizer to 1%. The system starts with the modified image created by VQGAN and is sent into the CLIP image encoder. The system runs for 300 frames, generating 10 seconds of video at 30 frames per second. He then uses the ffmpeg codec to generate an mp4 movie file.\n",
            "LLaMa 7B Answer:  To create a video using the SpookyGAN system, he uses VQGAN and CLIP to generate images, and then uses ffmpeg to encode them into an mp4 movie file. He reduces the learning rate of the optimizer to create slow frame changes in the video, and uses a generic text prompt like \"nightmare\" to generate the images.\n",
            "LLaMa 7B Score:   0.9310402\n",
            "\n",
            "Index:            40\n",
            "Reference:        Summary\n",
            "Since I have GPT-3 running, I used it to create a summary of this article. I pasted in the first three paragraphs and added “TL;DR,” which stands for “Too Long; Didn’t Read.” This prompted GPT-3 to generate a two-sentence summary.\n",
            "> This project is a proof-of-concept that AI can be used to generate new text based on the style of another writer. This shows that AI can be used to generate new content and that AI-generated text can be used in creative ways. - GPT-3\n",
            "\n",
            "\n",
            "Those 254 tokens cost me $0.015. So it’s like paying a penny for GPT-3’s thoughts to get its 2 cents in. 😃\n",
            "Question:         What was the purpose of Robert A. Gonsalves' project, Voxmorphia?\n",
            "GPT-4 Answer:     The purpose of his project, Voxmorphia, was to demonstrate that AI can be used to generate new text based on the style of another writer. He aimed to show that AI-generated text can be used in creative ways, such as reimagining classic songs in the style of different artists.\n",
            "LLaMa 7B Answer:  The purpose of he' project, Voxmorphia, was to demonstrate the ability of AI to generate new text based on the style of another writer. Specifically, he used GPT-3 to reimagine classic songs in the styles of Bob Marley, Radiohead, and Megan Thee Stallion.\n",
            "LLaMa 7B Score:   0.90767956\n",
            "\n",
            "Index:            41\n",
            "Reference:        Discussion and Next Steps\n",
            "The OpenCLIP system seems to work well for searching for images in a large dataset. And the new techniques for embedding math provide expert tools to help people find the perfect shot. The CoCa/OpenCLIP model does a fine job creating descriptive captions for images.\n",
            "An area for improvement would be to see if these systems could be fine-tuned to find or create captions for personal photographs. Unlike OpenAI, LAION released the training code for their models. Although their code is designed for large-scale training, it would be helpful if it could be adapted to fine-tune models with, say, only ten images of your uncle Bob.\n",
            "Question:         How does the OpenCLIP system perform in terms of searching for images in a large dataset?\n",
            "GPT-4 Answer:     According to he, the OpenCLIP system performs well when it comes to searching for images in a large dataset. He also mentions that the new techniques for embedding math provide expert tools to help people find the perfect shot.\n",
            "LLaMa 7B Answer:  According to the article, the OpenCLIP system performs well in searching for images in a large dataset.\n",
            "LLaMa 7B Score:   0.85399973\n",
            "\n",
            "Index:            42\n",
            "Reference:        Dirty Limericks\n",
            "Historically, the subject of limericks often veers into R-Rated territory. For example, the science fiction author Isaac Asimov wrote a series of books [5] packed with “lecherous limericks.” Here’s a relatively clean example.\n",
            "> Said an ovum one night to a sperm, “You’re a very attractive young germ. Come join me, my sweet, Let our nuclei meet And in nine months we’ll both come to term.” - Isaac Asimov\n",
            "\n",
            "\n",
            "Here’s another one by author Vyvyan Holland that sums up why limericks are often dirty.\n",
            "> The limerick packs laughs anatomical Into space that is quite economical. But the good ones I’ve seen So seldom are clean And the clean one’s so seldom are comical. - Vyvyan Holland\n",
            "\n",
            "\n",
            "The following section will discuss a prior attempt to create limericks using an AI system.\n",
            "Question:         What was Robert's motivation behind training an AI to create limericks?\n",
            "GPT-4 Answer:     he was inspired by the historical use of limericks, often filled with humor and sometimes veering into R-rated territory. He wanted to see if an AI system could replicate this unique form of poetry.\n",
            "LLaMa 7B Answer:  According to the text, he's motivation for training an AI to create limericks was to see if he could do it and to test the limits of the AI system. He wanted to know if the AI could generate clean and comical limericks, and he was curious to see how long it would take for the AI to produce a limerick.\n",
            "LLaMa 7B Score:   0.83074105\n",
            "\n",
            "Index:            43\n",
            "Reference:        References\n",
            "[1] Jonathan P. Chang, Caleb Chiam, Liye Fu, Andrew Wang, Justine Zhang, Cristian Danescu-Niculescu-Mizil. 2020. “ ConvoKit : A Toolkit for the Analysis of Conversations ” Proceedings of SIGDIAL.\n",
            "\n",
            "[2] R. Banik,  The Movies Dataset  on Kaggle (2018)\n",
            "\n",
            "[3] Wikipedia,  Friends Plot Summaries\n",
            "\n",
            "[4] S. Bansal,  Streaming Movies and TV Shows  on Kaggle (2022)\n",
            "\n",
            "[5]  GPT-J ,  Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX  (2021)\n",
            "\n",
            "[6]  GPT-3 , T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A.Neelakantan, et al.  Language Models Are Few-Shot Learners  (2020)\n",
            "\n",
            "[7] L.Gao, et al.,  The Pile : An 800GB Dataset of Diverse Text for Language Modeling  (2020)\n",
            "\n",
            "[8] R. Caruana,  Multitask Learning  (1997)\n",
            "\n",
            "[9] M. Grootendorst,  KeyBERT : Minimal keyword extraction with BERT  (2020)\n",
            "\n",
            "[10] E. Hu, et al.,  LoRA : Low-rank Adaptation of Large Language Models  (2021)\n",
            "Question:         What is the main objective of Robert A. Gonsalves' project, GreenLit?\n",
            "GPT-4 Answer:     The main objective of GreenLit is to fine-tune a machine learning model, specifically GPT-J, with multi-task learning to create new screenplays. This includes generating new titles, plot summaries, and scripts for TV shows and movies.\n",
            "LLaMa 7B Answer:  The main objective of he' project, GreenLit, is to fine-tune a pre-trained language model (GPT-J) for creating new screenplays, including TV shows and movies, with the ability to generate titles, plot summaries, and scripts.\n",
            "LLaMa 7B Score:   0.7093602\n",
            "\n",
            "Index:            44\n",
            "Reference:        GANfolk: Using AI to Create Portraits of Fictional People to Sell as NFTs\n",
            "Robert A. Gonsalves\n",
            "Jan 10, 2022\n",
            "GANfolk - https://medium.com/towards-data-science/ganfolk-using-ai-to-create-portraits-of-fictional-people-to-sell-as-nfts-6e24f5214ed1\n",
            "I have previously written on Medium about using AI to create visual art, like  abstract paintings  and  landscapes . During my research, I noticed that Generative Adversarial Networks (GANs) seem to have a hard time creating images of people. I decided to take this challenge [ahem] head-on, so I trained two GANs using paintings and photographs of people. I then put the images for sale as NFTs in a collection on OpenSea called  GANfolk .\n",
            "Note that I am releasing the dataset of training images, the source code, and the trained models under the Creative Commons Attribution Share-alike license. This means you can use my code to create your own digital paintings of people and sell them, as long as you give me attribution. See the Source Code section below for details.\n",
            "Question:         What is the purpose of Robert A. Gonsalves' project GANfolk?\n",
            "GPT-4 Answer:     The purpose of GANfolk, a project by he, is to use AI to create portraits of fictional people. He noticed that Generative Adversarial Networks (GANs) often struggle with creating images of people, so he decided to tackle this challenge by training two GANs using paintings and photographs of people. The images created were then put up for sale as NFTs in a collection on OpenSea. He also released the dataset of training images, the source code, and the trained models under the Creative Commons Attribution Share-alike license, allowing others to use his code to create their own digital paintings of people and sell them, provided they give him attribution.\n",
            "LLaMa 7B Answer:  The purpose of he' project GANfolk is to use Generative Adversarial Networks (GANs) to create portraits of fictional people, which are then sold as Non-Fungible Tokens (NFTs) on OpenSea.\n",
            "LLaMa 7B Score:   0.73189855\n",
            "\n",
            "Index:            45\n",
            "Reference:        Changing a Person’s Age, Revisited with InterFaceGAN\n",
            "As a bonus, the authors of StyleGAN also released a pretrained model called InterFaceGAN with three “directions” for changing face image: age, smile, and pose. This version of StyleGAN does not need text descriptions. You pick one of the three dimensions and indicate how much change you want, both in the positive and negative directions.\n",
            "Here are some more rendered pictures of Janis Joplin, using InterFaceGAN with the age parameter set to -2, 0, and 4.\n",
            "In this series, you can see changes to many aspects of the faces, like adding graying hair to old Janis. Also, the InterFaceGAN gave her reading glasses.\n",
            "Note that the output images from InterFaceGAN are smaller than the StyleGAN used by StyleCLIP. So I used the image super-resolution resizer from Idealo to increase the image size.\n",
            "Question:         What are the three \"directions\" for changing face image in InterFaceGAN as mentioned by Robert A. Gonsalves in \"Portraits from the Multiverse\"?\n",
            "GPT-4 Answer:     In \"Portraits from the Multiverse\", he mentioned that the three \"directions\" for changing face image in InterFaceGAN are age, smile, and pose.\n",
            "LLaMa 7B Answer:  According to he in \"Portraits from the Multiverse,\" the three \"directions\" for changing face images in InterFaceGAN are:\n",
            "1. Age\n",
            "2. Smile\n",
            "3. Pose\n",
            "LLaMa 7B Score:   0.95144266\n",
            "\n",
            "Index:            46\n",
            "Reference:        Component Details\n",
            "In January of 2021, OpenAI released a paper and a demo of their DALL-E text-to-image system [8]. The name is a play on words. It’s a cross between painter Salvador Dali’s last name and Disney’s animated movie, WALL-E.\n",
            "The DALL-E model is a massive transformer. It has 12 billion parameters and was trained using 250 million image/caption pairs from the internet. Here are the results from the queries, “a tapir made of accordion. a tapir with the texture of an accordion.” and “an illustration of a baby hedgehog in a christmas sweater walking a dog.”\n",
            "The results of DALL-E are excellent. The system does a great job interpreting the text and rendering images in a creative but realistic way. One limitation is the output is relatively small, at 256x256 pixels. And OpenAI didn’t proide access to the DALL-E source code or the trained model. But that changed 14 months later.\n",
            "Question:         What is one limitation of the DALL-E model as mentioned by Robert A. Gonsalves in his article 'E-DALL-E: Creating Digital Art with Varying Aspect Ratios'?\n",
            "GPT-4 Answer:     According to him, one limitation of the DALL-E model is that the output it produces is relatively small, at 256x256 pixels.\n",
            "LLaMa 7B Answer:  According to he in his article \"E-DALL-E: Creating Digital Art with Varying Aspect Ratios,\" one limitation of the DALL-E model is that the output images are relatively small, at 256x256 pixels.\n",
            "LLaMa 7B Score:   0.8167286\n",
            "\n",
            "Index:            47\n",
            "Reference:        Background on GPT-4\n",
            "GPT-4 is the latest large language model from OpenAI. It is available using their  ChatGPT  user interface and accessed via their  API . The new model is available as part of a paid ChatGPT Plus subscription, which costs US$20 per month. As I noted in my last article, OpenAI will capture your inputs and responses which that might use to train future versions of their models. This is enabled by default, but you can opt out of data collection using their form  here .\n",
            "The responses from GPT-4 are notably better than the previous GPT-3.5 model. According to OpenAI’s  technical repor t, the new model scored 1,400 out of 1,600 on the SAT exams (710 for language, 690 for math), whereas the earlier model scored 1,260 (670, 590.)\n",
            "Despite the excellent test scores, OpenAI reports that the output from the system is not always reliable and is not free from showing biases learned from the training data.\n",
            "> Despite its capabilities, GPT-4 has similar limitations to earlier GPT models: it is not fully reliable (e.g. can suffer from “hallucinations”), has a limited context window, and does not learn. Care should be taken when using the outputs of GPT-4, particularly in contexts where reliability is important. … GPT-4 can generate potentially harmful content. … It can represent various societal biases and worldviews that may not be representative of the user’s intent, or of widely shared values. — OpenAI [1]\n",
            "\n",
            "\n",
            "This sounds scary and ominous. But for what I am doing in this project, generating chords for songs, the stakes are a lot lower.\n",
            "Question:         What are some of the limitations of GPT-4 as mentioned by Robert A. Gonsalves in his article 'Writing Songs with GPT-4: Part 2, Chords'?\n",
            "GPT-4 Answer:     According to he, GPT-4, despite its advanced capabilities, has limitations similar to its predecessors. It is not fully reliable and can suffer from \"hallucinations\". It also has a limited context window and does not learn. Furthermore, the outputs of GPT-4 can sometimes reflect societal biases and worldviews that may not align with the user's intent or widely shared values. He advises caution when using the outputs of GPT-4, especially in contexts where reliability is important.\n",
            "LLaMa 7B Answer:  According to he in his article \"Writing Songs with GPT-4: Part 2, Chords,\" some of the limitations of GPT-4 include:\n",
            "1. Lack of reliability: GPT-4 is not always reliable and can suffer from \"hallucinations.\"\n",
            "2. Limited context window: GPT-4 has a limited context window, which means it can only generate responses based on the information provided within that window.\n",
            "3. Lack of learning: GPT-4 does not learn, meaning it cannot improve its performance over time based on user feedback or interactions.\n",
            "4. Representation of societal biases and worldviews: GPT-4 can represent various societal biases and worldviews that may not be representative of the user's intent or widely shared values.\n",
            "LLaMa 7B Score:   0.7548225\n",
            "\n",
            "Index:            48\n",
            "Reference:        Using ChatGPT as a Creative Writing Partner — Part 1: Prose\n",
            "Robert A. Gonsalves\n",
            "Jan 4, 2023\n",
            "ChatGPT Prose - https://medium.com/towards-data-science/using-chatgpt-as-a-creative-writing-partner-part-1-prose-dc9a9994d41f\n",
            "If you have been reading my posts on Medium, you will know that I have been writing about creative uses of AI since August 2020. I often write about generating digital art, but I occasionally write about using AI for other creative purposes, like writing prose and composing music. When I heard that OpenAI released a new language model called ChatGPT [1], I jumped right in and tested it for creative writing.\n",
            "Question:         What creative uses of AI does Robert A. Gonsalves often write about?\n",
            "GPT-4 Answer:     He often writes about generating digital art, using AI for writing prose, and composing music.\n",
            "LLaMa 7B Answer:  According to the passage, he often writes about using AI for generating digital art and other creative purposes, such as writing prose and composing music.\n",
            "LLaMa 7B Score:   0.85022485\n",
            "\n",
            "Index:            49\n",
            "Reference:        Example Output\n",
            "Before I show more examples of the output from GPT-3, here is the legal disclaimer that OpenAI suggests that I show, which is all true.\n",
            "The author generated the following text in part with GPT-3, OpenAI’s large-scale language-generation model. Upon generating draft language, the author reviewed, edited, and revised the language to their own liking and takes ultimate responsibility for the content of this publication.\n",
            "Here is the first verse and chorus of “Yes Sir, That’s My Baby” in the style of Bob Marley.\n",
            "And here are the same lyrics in the style of Radiohead.\n",
            "And finally, here are the lyrics again in the style of Megan The Stallion, after the Parental Advisory Warning.\n",
            "Note that GPT-3 flags the output as safe, sensitive, or unsafe. Here are the definitions of these terms from OpenAI.\n",
            "> 0 -  The text is safe.\n",
            "> 1 -  This text is sensitive. This means that the text could be talking about a sensitive topic, something political, religious, or talking about a protected class such as race or nationality.\n",
            "> 2 -  This text is unsafe. This means that the text contains profane language, prejudiced or hateful language, something that could be NSFW, or text that portrays certain groups/people in a harmful manner.\n",
            "\n",
            "\n",
            "For the examples above, the system occasionally flagged lyrics in the style of Bob Marley and Radiohead as sensitive. But it always flagged the Megan Thee Stallion as unsafe.\n",
            "Question:         How does VoxMorphia classify the generated text in terms of safety?\n",
            "GPT-4 Answer:     VoxMorphia uses GPT-3's classification system to flag the generated text as safe, sensitive, or unsafe. Safe text is considered harmless, while sensitive text could be discussing political, religious, or other sensitive topics. Unsafe text contains profane language, prejudiced or hateful language, or could be considered not safe for work. In he's examples, lyrics in the style of Bob Marley and Radiohead were occasionally flagged as sensitive, while those in the style of Megan Thee Stallion were consistently flagged as unsafe.\n",
            "LLaMa 7B Answer:  Based on the examples provided, VoxMorphia classifies the generated text as follows:\n",
            "* For the lyrics in the style of Bob Marley and Radiohead, the system flags them as \"safe\" (0).\n",
            "* For the lyrics in the style of Megan Thee Stallion, the system flags them as \"unsafe\" (2).\n",
            "LLaMa 7B Score:   0.85122114\n",
            "\n",
            "Index:            50\n",
            "Reference:        Overview\n",
            "In this article, I’ll cover a brief background of BL, explain two key concepts: normal distributions and logarithms, show how a dice rolling exercise can lead to BL, and finally take a look at some real datasets to see if this explanation holds up.\n",
            "Question:         What are the two key concepts that Robert A. Gonsalves explains in his article \"Benford’s Law — A Simple Explanation\"?\n",
            "GPT-4 Answer:     In his article, he explains the concepts of normal distributions and logarithms.\n",
            "LLaMa 7B Answer:  According to he' article \"Benford's Law — A Simple Explanation,\" the two key concepts that he explains are:\n",
            "1. Normal distributions: Gonsalves explains that Benford's Law is related to normal distributions, specifically the logarithm of the digits in a number following a normal distribution.\n",
            "2. Logarithms: Gonsalves also explains how logarithms come into play in Benford's Law, as the law states that the leading digit of a number follows a specific distribution, which is related to the logarithm of the number.\n",
            "LLaMa 7B Score:   0.54242045\n",
            "\n",
            "Index:            51\n",
            "Reference:        MAGnet Overview\n",
            "Here is a brief overview of the components used in MAGnet. I’ll get into the details of each component later in the article. Note that the steps of gathering, filtering, and training a GAN with images from WikiArt are similar to my GANscapes project, which generates impressionist landscape paintings.\n",
            "Question:         What are the main components used in the MAGnet project?\n",
            "GPT-4 Answer:     The main components used in the MAGnet project include CLIP, SWAGAN, and a custom genetic algorithm. The process also involves gathering, filtering, and training a GAN with images from WikiArt, similar to the process used in his previous project, GANscapes.\n",
            "LLaMa 7B Answer:  The main components used in the MAGnet project are:\n",
            "1. CLIP (Contrastive Language-Image Pre-training): a deep neural network that learns to generate images from text descriptions.\n",
            "2. SWAGAN (Swag-based Generative Adversarial Network): a custom genetic algorithm that generates images based on the text descriptions provided.\n",
            "3. A combination of these two components, which are used together to create modern paintings from text descriptions.\n",
            "LLaMa 7B Score:   0.8199134\n",
            "\n",
            "Index:            52\n",
            "Reference:        Societal Biases in ML Systems\n",
            "StyleCLIP seems to work well for changing faces with different genders, ethnicities, and ages. However, I did get to see first-hand how these ML models sometimes show signs of cultural biases. Some of the rendering choices were understandable, like how the more-male Barack Obama grew a goatee and how the older Janis Joplin got reading glasses. But others were unexpected, like how Iranian Jennifer Anniston wears a lot of eye shadow and has finely-manicured eyebrows. This may be due to the fact that either there were not many pictures of Iranian women in the dataset and/or most of the Iranian women had their eyes made up.\n",
            "> “Every data set involving people implies subjects and objects, those who collect and those who make up the collected. It is imperative to remember that on both sides we have human beings.” — Mimi Onuoha,  The Point of Collection\n",
            "\n",
            "\n",
            "Understanding and trying to offset cultural biases in ML systems is an open area of research. The authors of the CLIP paper ran many tests to understand the degree and impact of societal biases in their model [3].\n",
            "> CLIP is trained on text paired with images on the internet. These image-text pairs are unfiltered and uncurated and result in CLIP models learning many social biases. … We have […] sought to characterize the social biases inherent to the model. Our bias tests represent our initial efforts to probe aspects of how the model responds in different scenarios, and are by nature limited in scope. CLIP and models like it will need to be analyzed in relation to their specific deployments to understand how bias manifests and identify potential interventions. Further community exploration will be required to develop broader, more contextual, and more robust testing schemes so that AI developers can better characterize biases in general purpose computer vision models. - A. Radford, et al.\n",
            "\n",
            "\n",
            "The authors of the paper, “One Label, One Billion Faces: Usage and Consistency of Racial Categories in Computer Vision”, discuss the societal impacts of biases and uneven representation often found in datasets used for ML [5].\n",
            "> “Privileged, wealthy, industrialized countries in which most AI research is conducted have the benefit of being the primary producers of datasets, and package their racial systems into datasets. Effectively, this is a potentially dangerous form of cultural export, and eventually, the exported racial systems will take on a validity outside of their cultural context, with potentially dangerous consequences.” — Zaid Khan and Yun Fu\n",
            "\n",
            "\n",
            "So it’s good to see the researchers are trying to understand the societal implications of their work, especially as AI seems to be turning the world upside down.\n",
            "Question:         What are some of the societal implications Robert A. Gonsalves discusses in relation to the use of ML systems like StyleCLIP?\n",
            "GPT-4 Answer:     In \"Portraits from the Multiverse\", he discusses how these ML systems can sometimes show signs of cultural biases. For example, the system might render certain ethnicities with stereotypical features, which could be due to a lack of diverse representation in the dataset. He also mentions the societal impacts of biases and uneven representation in datasets used for ML, such as the potential for cultural export of racial systems from privileged, wealthy, industrialized countries that are the primary producers of datasets. This could lead to these exported racial systems taking on a validity outside of their cultural context, with potentially dangerous consequences. He emphasizes the importance of understanding and trying to offset these cultural biases in ML systems.\n",
            "LLaMa 7B Answer:  In his article, he highlights several societal implications of using ML systems like StyleCLIP, including:\n",
            "1. Cultural biases: The model can perpetuate and amplify existing cultural biases, as seen in the unexpected rendering choices made for Iranian women.\n",
            "2. Limited representation: The dataset used to train the model may not be representative of all cultures and genders, leading to a lack of diversity in the rendered faces.\n",
            "3. Uneven deployment: AI models like StyleCLIP are often deployed in privileged, wealthy countries, which can lead to cultural export and potential dangerous consequences outside of their cultural context.\n",
            "4. Need for further research: The authors emphasize that there is a need for more comprehensive and robust testing schemes to understand the biases in general-purpose computer vision models like StyleCLIP.\n",
            "LLaMa 7B Score:   0.73166275\n",
            "\n",
            "Index:            53\n",
            "Reference:        Acknowledgments\n",
            "I want to thank Jennifer Lim for her help with this article.\n",
            "Question:         What was Robert A. Gonsalves' main purpose in exploring DALL-E for digital art creation?\n",
            "GPT-4 Answer:     His main purpose was to test OpenAI’s text-to-image generator, DALL-E, to see how it would perform in creating images using a variety of subjects and styles.\n",
            "LLaMa 7B Answer:  According to the text, he' main purpose in exploring DALL-E for digital art creation was to test OpenAI's text-to-image generator to see how it would perform creating images using a variety of subjects and styles.\n",
            "LLaMa 7B Score:   0.753477\n",
            "\n",
            "Index:            54\n",
            "Reference:        Source Code and Colabs\n",
            "All source code for this project is available on  GitHub . I released the source code under the  CC BY-SA license .\n",
            "Question:         What is the purpose of the GreenLit project by Robert A. Gonsalves?\n",
            "GPT-4 Answer:     The purpose of the GreenLit project by he is to fine-tune a machine learning model, specifically GPT-J, to create new screenplays for TV shows and movies. This includes generating new titles, plot summaries, and scripts.\n",
            "LLaMa 7B Answer:  The purpose of the GreenLit project, as described by he, is to fine-tune a machine learning (ML) model using multi-task learning to create new screenplays for TV shows and movies with unique titles, plot summaries, and scripts.\n",
            "LLaMa 7B Score:   0.91380006\n",
            "\n",
            "Index:            55\n",
            "Reference:        Universal Sentence Encoder\n",
            "After I collected the topics, I used the Universal Sentence Encoder from Google to convert each topic phrase into an embedding, an array of 512 floating-point numbers. Here’s some sample code to show how it works.\n",
            "```\n",
            "import tensorflow_hub as hub embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\") embedding = embed([\"love loss\"]) print(embedding)\n",
            "```\n",
            "The result is here.\n",
            "```\n",
            "tf.Tensor( [[-2.79744454e-02 -6.52119750e-03 1.32698761e-02 4.50092048e-04 9.81796882e-04 3.18628997e-02 2.73146853e-02 -1.10505158e-02 -2.71893758e-02 -5.06720766e-02 -3.20894904e-02 ... -1.08678043e-02 7.85474479e-03 -6.44846493e-03 -3.88006195e-02]], shape=(1, 512), dtype=float32)\n",
            "```\n",
            "Although these numbers won’t mean anything to humans, they represent the concept of “love loss” in the encoder’s multidimensional space. I’ll show you how I used statistics to make sense of the topics in the following sections.\n",
            "Question:         How did Robert A. Gonsalves utilize the Universal Sentence Encoder in his project, Organic Topic Modelling?\n",
            "GPT-4 Answer:     In his project, Organic Topic Modelling, he used the Universal Sentence Encoder from Google to convert each topic phrase into an embedding, which is an array of 512 floating-point numbers. This essentially represents the concept of the topic phrase in the encoder's multidimensional space. For instance, he demonstrated this with the phrase \"love loss\". Although the resulting numbers may not be interpretable to humans, he used statistical methods to make sense of these topics.\n",
            "LLaMa 7B Answer:  According to the provided code snippet, he used the Universal Sentence Encoder from Google in his project, Organic Topic Modelling, to convert each topic phrase into an embedding, an array of 512 floating-point numbers. This allows him to represent the concepts of \"love loss\" in a multidimensional space, which he will use for further analysis in subsequent sections.\n",
            "LLaMa 7B Score:   0.75710785\n",
            "\n",
            "Index:            56\n",
            "Reference:        Expanding Images with E-DALL-E\n",
            "Here is the component diagram again, showing only the processes used for expanding the image.\n",
            "The process starts with an input image and a specified aspect ratio. The system repeats the edge pixels to stretch the image, as described above. It then encodes the prompt using CLIP to guide the optimization process. For each iteration, the system encodes the image with CLIP. It uses the difference between the text encoding and the image encoding to modify the VQGAN latent vectors using the Adam Optimizer. After each iteration, the center part of the modified image vectors is replaced with the center of the vectors from the input image to keep the middle part relatively constant. Only the sides will change a lot. After N iterations, the system displays the final image with the new aspect ratio.\n",
            "Here are some results from using E-DALL-E.\n",
            "Pretty good! The system seems to do a good job making up new details in the expanded areas. And it’s hard to see the seams between the original image and the newly generated parts. But you can also see how VQGAN sometimes changes the central part of the image to make the new pieces fit in better. For example, the sky's hue in the top-right image seems to have changed to a deeper blue in the central part. And the texture of the orange triangles in the center of the bottom-right image appears to match the surface of the newly generated triangles on the sides.\n",
            "Question:         How does E-DALL-E maintain the central part of the image relatively constant while expanding the image?\n",
            "GPT-4 Answer:     E-DALL-E maintains the central part of the image relatively constant by replacing the center part of the modified image vectors with the center of the vectors from the input image after each iteration. This ensures that only the sides of the image undergo significant changes.\n",
            "LLaMa 7B Answer:  E-DALL-E uses the Adam Optimizer to modify the VQGAN latent vectors based on the difference between the text encoding and the image encoding. This process helps maintain the central part of the image relatively constant during the expansion process.\n",
            "LLaMa 7B Score:   0.60871637\n",
            "\n",
            "Index:            57\n",
            "Reference:        Using DALL-E\n",
            "The system has three main functions for creating images:\n",
            "Note that functions 2 and 3 work with either generated or uploaded images.\n",
            "Question:         What are the main functions of DALL-E for creating images as described by Robert A. Gonsalves?\n",
            "GPT-4 Answer:     According to him, DALL-E has three main functions for creating images. These functions can work with either generated or uploaded images.\n",
            "LLaMa 7B Answer:  According to he, the main functions of DALL-E for creating images are:\n",
            "1. Text-to-image generation: This function allows users to generate images based on a given text prompt or caption.\n",
            "2. Image-to-image translation: This function enables users to translate an existing image into a new style or domain, such as transforming a photo of a cat into a painting.\n",
            "3. Image synthesis: This function allows users to generate entirely new images based on a given prompt or condition, such as creating a digital artwork from scratch.\n",
            "LLaMa 7B Score:   0.8717499\n",
            "\n",
            "Index:            58\n",
            "Reference:        Comparing the Models for Writing Lyrics\n",
            "After experimenting with the new GPT-4 system, I found a substantial increase in the quality of responses compared to the original GPT-3 model, especially with rhyming words.\n",
            "Question:         What improvements did Robert A. Gonsalves notice in GPT-4 compared to GPT-3 in terms of writing lyrics?\n",
            "GPT-4 Answer:     He noticed a substantial increase in the quality of responses from GPT-4 compared to GPT-3, particularly in its ability to generate rhyming words.\n",
            "LLaMa 7B Answer:  According to he, he noticed a substantial increase in the quality of responses from GPT-4 compared to GPT-3 when it comes to writing lyrics, particularly in the area of rhyming words.\n",
            "LLaMa 7B Score:   0.89891547\n",
            "\n",
            "Index:            59\n",
            "Reference:        KeyBERT for Extracting Keywords\n",
            "To get the new model to generate new limericks based on user-supplied topics, I extracted keywords for each limerick in the dataset and used these in the training data. Like in previous projects, I used KeyBert [9] to extract the keywords. Here is the Python code.\n",
            "```\n",
            "from keybert import KeyBERT kw_model = KeyBERT() limerick = \"\"\"For a sound wave to move, be aware That a medium’s needed, like air. In a vacuum no sound Can be heard; they have found Its conduction is nil. (Nothing’s there.)\"\"\" keywords = kw_model.extract_keywords(limerick, keyphrase_ngram_range=(1, 2), stop_words=None) print(keywords[0][0])\n",
            "```\n",
            "The results are “sound wave” for this limerick and “flea market” for the second example shown above.\n",
            "Question:         How did Robert extract keywords for each limerick in the AI Limericks project?\n",
            "GPT-4 Answer:     In the AI Limericks project, he used a tool called KeyBert to extract the keywords for each limerick. He incorporated these keywords into the training data to enable the model to generate new limericks based on user-supplied topics.\n",
            "LLaMa 7B Answer:  he extracted keywords for each limerick in the AI Limericks project using KeyBert, a Python library for keyword extraction. He used the `KeyBERT` class from the library to extract keywords from each limerick by passing it through the model and then accessing the resulting keywords.\n",
            "LLaMa 7B Score:   0.88406104\n",
            "\n",
            "Index:            60\n",
            "Reference:        Overview of Components\n",
            "Here is a brief, high-level overview of the components used in GANscapes. I’ll discuss the details of each component later in the article.\n",
            "I gathered images of Impressionist landscape paintings from WikiArt.org [4] and processed the images to adjust the aspect ratio. I then used the CLIP model [5] from OpenAI to filter the images to keep the “good ones.”\n",
            "I used these images to train StyleGAN2 ADA [6] from NVidia, which has a generator and discriminator network. The generator creates new images starting with random “latent” vectors for form and style and tries to fool the discriminator into thinking the output images are real. Before the real and the generated images are fed into the discriminator, they are modified slightly with the Adaptive Discriminator Augmentation (ADA) module that creates visual diversity in the pictures.\n",
            "I use CLIP again to filter the output images based on a user-supplied text query. And I use the generator again to create an image with a mix of style and form selected by the user. As the final step, I post-process the final images to adust the aspect ratios.\n",
            "Be sure to check out the image gallery in the appendix at the end of the article to see more results.\n",
            "Question:         How does the GANscapes project use the CLIP model from OpenAI?\n",
            "GPT-4 Answer:     The GANscapes project uses the CLIP model from OpenAI in two ways. First, it uses CLIP to filter the initial set of Impressionist landscape paintings, keeping only the \"good ones.\" Later, it uses CLIP again to filter the output images based on a user-supplied text query. This ensures that the final images generated align with the user's desired style and form.\n",
            "LLaMa 7B Answer:  The GANscapes project uses the CLIP (Contrastive Language-Image Pre-training) model from OpenAI as a filter to keep \"good\" images in the training process of StyleGAN2 ADA. Specifically, the CLIP model is used to filter the input images before they are fed into the generator and discriminator networks of StyleGAN2 ADA. This helps to improve the quality and diversity of the generated images by removing low-quality or irrelevant images from the training set.\n",
            "LLaMa 7B Score:   0.67289937\n",
            "\n",
            "Index:            61\n",
            "Reference:        The MetFaces Dataset\n",
            "NVidia also released the MetFaces dataset of faces from paintings in the Metropolitan Museum of Art. They write that…\n",
            "> [the] dataset consists of 1,336 high-quality PNG images at 1024×1024 resolution. The images were downloaded via the  Metropolitan Museum of Art Collection API , and automatically aligned and cropped using  dlib . Various automatic filters were used to prune the set.\n",
            "\n",
            "\n",
            "Again, NVidia released the dataset under  non-commercial  terms, and they used similarly tight cropping for the faces.\n",
            "Here’s what newly generated images look like with StyleGAN 2 ADA trained on the FFHQ dataset and fine-tuned with the MetFaces dataset.\n",
            "Although the results are impressive, not surprisingly, the results seem to be too tightly cropped. In addition to the datasets, NVidia released the official source code under  non-commercial terms , so these faces cannot be sold as NFTs. Also, there seems to be a distinct lack of cultural diversity in the generated faces.\n",
            "Question:         What is the issue with the faces generated using the MetFaces dataset and StyleGAN 2 ADA according to Robert A. Gonsalves?\n",
            "GPT-4 Answer:     According to him, the faces generated using the MetFaces dataset and StyleGAN 2 ADA are too tightly cropped. Additionally, he notes a distinct lack of cultural diversity in the generated faces.\n",
            "LLaMa 7B Answer:  According to he, the faces generated using the MetFaces dataset and StyleGAN 2 ADA are too tightly cropped and lack cultural diversity.\n",
            "LLaMa 7B Score:   0.97769195\n",
            "\n",
            "Index:            62\n",
            "Reference:        Source Code\n",
            "All source code for this project is available on  GitHub . You can experiment with the code using this  Google Colab . The sources are released under the  CC BY-SA license .\n",
            "Question:         What is the purpose of the InventorBot project by Robert A. Gonsalves?\n",
            "GPT-4 Answer:     The purpose of the InventorBot project by he is to use artificial intelligence to generate new ideas in any field. The AI is trained on the US Patent Database and can produce new and possibly useful inventions.\n",
            "LLaMa 7B Answer:  The purpose of the InventorBot project by he is to demonstrate how a neural network trained on the US Patent Database can produce new and possibly useful inventions.\n",
            "LLaMa 7B Score:   0.86224735\n",
            "\n",
            "Index:            63\n",
            "Reference:        Source Code and Colabs\n",
            "All source code for this project is available on  GitHub . I released the source code under the  CC BY-SA license .\n",
            "Question:         What is the licensing model for the source code of the Clip-n-Paste project by Robert A. Gonsalves?\n",
            "GPT-4 Answer:     The source code for the Clip-n-Paste project is released under the CC BY-SA license.\n",
            "LLaMa 7B Answer:  The licensing model for the source code of the Clip-n-Paste project by he is Creative Commons Attribution-ShareAlike (CC BY-SA).\n",
            "LLaMa 7B Score:   0.81936026\n",
            "\n",
            "Index:            64\n",
            "Reference:        Finding Source Images\n",
            "As I mentioned in the overview, I collected images for the collages from Wikimedia Commons and the OpenImages dataset. Both sources release their images under the Creative Commons Attribution Sharealike open-source license.\n",
            "To search for images in Wikimedia Commons, I used the API at  https://commons.wikimedia.org/w/api.php .\n",
            "Here is the Python code that sets up the parameters for the search.\n",
            "```\n",
            "s = requests.Session() url = \"https://commons.wikimedia.org/w/api.php\" params = { \"action\": \"query\", \"generator\": \"images\", \"prop\": \"imageinfo\", \"gimlimit\": 500, \"titles\": keyphrases, \"iiprop\": \"url|dimensions\", \"format\": \"json\" } results = s.get(url=url, params=params)\n",
            "```\n",
            "When the results come back, I filter the images based on their dimensions. I keep the images that are 512x512 or greater in size.\n",
            "For searching the pictures in the OpenImages dataset, I use the CLIP Text Encoder to compare the embeddings from the keyphrases to the embeddings from the descriptions in the dataset. If you would like to get more info on this process, you can read the details in my article on searching for design patents  here .\n",
            "Once I get the collection of images, I run them through the CLIP Image Encoder to compare them to the embedding for the entire prompt. Here are the top photos from Wikimedia Commons and OpenImages.\n",
            "Image Segmentation\n",
            "I used an AI model for image segmentation called GRoIE, which stands for Generic Region of Interest Extractor [5]. The system was developed by Leonardo Rossi et al. at the University of Parma in Italy. GRoIE will extract and label any discernable foreground objects given an input image. For each object it finds, it creates a mask image that defines object shape.\n",
            "Note that the GRoIE model often finds and extracts more than one object in each input image. For example, in this photo, it found and isolated both penguins.\n",
            "After I extracted all of the objects, I ran them through CLIP again to compare the image embeddings with the embedding from the prompt. The original image is on top, and the extracted image is below. Here are objects that were cut out of the top five images.\n",
            "Creating a Background Color Gradient\n",
            "I could have composited the image cutouts against a solid color, like white or black, for the photo collages. But to add interest to the background, I wrote a little Python code to create a vertical color gradient with five control points. And then, I use CLIP to analyze the resultant image and iteratively adjust the colors using the Adam optimizer [6] in Pytorch to find the best match for the text prompt.\n",
            "> We propose Adam, a method for efficient stochastic optimization that only requires first-order gradients with little memory requirement. The method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients; the name Adam is derived from adaptive moment estimation. — Diederik Kingma and Jimmy Ba\n",
            "\n",
            "\n",
            "If you don’t speak in maths, the Adam method tweaks the parameters a little and notices if and how much the results improve for each iteration. It changes the parameters at different rates depending on how much each parameter contributes to improving the results.\n",
            "Here is the resulting gradient for “penguins skiing down a snowy mountain” after 100 iterations of using CLIP and Adam on the 25 parameters (5 control points of red, green, and blue.)\n",
            "On the left is a graph of the parameter values and on the right is the resultant gradient image. I’m not sure why it shows purple in the mid-lower region, but it looks fabulous!\n",
            "Here are some additional background gradients using this method.\n",
            "As you can see, creating an image with only 15 parameters can be reasonably expressive as directed by CLIP. The code to generate color gradients from text is  here .\n",
            "Question:         How does Robert A. Gonsalves create a background color gradient for the photo collages in the Clip-n-Paste project?\n",
            "GPT-4 Answer:     In the Clip-n-Paste project, he creates a background color gradient for the photo collages by writing a Python code that creates a vertical color gradient with five control points. He then uses CLIP to analyze the resultant image and iteratively adjusts the colors using the Adam optimizer in Pytorch to find the best match for the text prompt. This process involves tweaking the parameters a little and observing if and how much the results improve for each iteration. The parameters are changed at different rates depending on how much each parameter contributes to improving the results.\n",
            "LLaMa 7B Answer:  In the Clip-n-Paste project, he creates a background color gradient using a Python code that utilizes the CLIP (Contrastive Language-Image Pre-training) model to analyze the resultant image and iteratively adjust the colors using the Adam optimizer in PyTorch to find the best match for the text prompt. The method involves computing individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients, and tweaking the parameters at different rates depending on how much each parameter contributes to improving the results. The resulting gradient is created by adjusting 15 parameters (5 control points of red, green, and blue) using the Adam method, which shows reasonably expressive results as directed by CLIP.\n",
            "LLaMa 7B Score:   0.887974\n",
            "\n",
            "Index:            65\n",
            "Reference:        Acknowledgments\n",
            "I would like to thank Jennifer Lim, Oliver Strimpel, Mahsa Mesgaran, and Vahid Khorasani   for their help and feedback on this project.\n",
            "Question:         What techniques did Robert A. Gonsalves use to generate improved abstract paintings with AI in the project 'StyleGAN Abstract Art'?\n",
            "GPT-4 Answer:     In the 'StyleGAN Abstract Art' project, he used Adaptive Discriminator Augmentation and Learning Transfer to generate improved abstract paintings with AI.\n",
            "LLaMa 7B Answer:  In the project \"StyleGAN Abstract Art,\" he used Adaptive Discriminator Augmentation (ADA) and Learning Transfer to generate improved abstract paintings with AI.\n",
            "LLaMa 7B Score:   0.9796502\n",
            "\n",
            "Index:            66\n",
            "Reference:        Fourth Line\n",
            "```\n",
            "topic: coffee rhyme-set: today / okay / think / drink / away first_line :  Coffee's a drink for today second_line:  With a splash of fresh cream, it's \"okay\" third_line :  If you do, as I think, fourth lines: ---------------------------------- 1: Add some milk, and you'll drink 2: Add sugar and drink 3: But it's coffee you drink 4:  And you need a strong drink 5: It's a milky drink, 6: That is, if you drink\n",
            "```\n",
            "Question:         What was Robert A. Gonsalves' solution to the slow processing time of the AI he was training to rhyme?\n",
            "GPT-4 Answer:     To address the slow processing time of the AI, he upgraded to Pro on Colab. This allowed for faster training of the AI, although it did come at a cost, as each limerick ended up costing him a dime.\n",
            "LLaMa 7B Answer:  According to the text, he upgraded from Colab to Pro to improve the processing speed of the AI he was training to rhyme.\n",
            "LLaMa 7B Score:   0.841076\n",
            "\n",
            "Index:            67\n",
            "Reference:        InventorBot: Using AI to Generate New Ideas in Any Field\n",
            "Robert A. Gonsalves\n",
            "Feb 1, 2021\n",
            "InventorBot - https://medium.com/geekculture/inventorbot-using-ai-to-generate-new-ideas-in-any-field-9345f9042df\n",
            "In July 2019, an inventor named Stephen Thaler filed for a patent for a warning beacon that flashes in a special way to get people’s attention. On the patent application form, Thaler listed himself as the applicant and the assignee but not the inventor. Who invented this improved beacon? A better question is  what  invented this improved beacon?\n",
            "The answer is DABUS, which stands for “Device Autonomously Bootstrapping Uniform Sensibility” [1]. This is a neural network that Thaler created and trained to invent things. (Brief side note: the improved beacon, in turn, uses a neural network to attract the attention of humans [2]. DABUS seems to be following a bit of common advice for newbies, “write what you know”. 😃) The US Patent and Trademark Office (USPTO) denied the patent because the inventor is not a person. Here’s what the USPTO said in their decision [3].\n",
            "> To the extent the petitioner argues that an “inventor” could be construed to cover machines, the patent statutes preclude such a broad interpretation. Title 35 of the United States Code consistently refers to inventors as natural persons. For example, 35 U.S.C. § 101 states “ Whoever  invents or discovers any new and useful process, machine, manufacture, or composition of matter…may obtain a patent therefore, subject to the conditions and requirements of this title” (emphasis added). “Whoever” suggests a natural person. —  US Patent and Trademark Office\n",
            "\n",
            "\n",
            "OK, machines can’t currently be listed as inventors on patents. For more information about the legal implications of this decision, check out Andrew Alcantra’s article,  here .\n",
            "But machines  can  invent things. I’ll show you how in this article.\n",
            "Question:         What is the name of the neural network that Stephen Thaler created and trained to invent things?\n",
            "GPT-4 Answer:     The neural network that Stephen Thaler created and trained to invent things is named DABUS, which stands for \"Device Autonomously Bootstrapping Uniform Sensibility\", as mentioned in \"InventorBot: Using AI to Generate New Ideas in Any Field\" by he.\n",
            "LLaMa 7B Answer:  The name of the neural network created and trained by Stephen Thaler to invent things is DABUS, which stands for \"Device Autonomously Bootstrapping Uniform Sensibility.\"\n",
            "LLaMa 7B Score:   0.9341449\n",
            "\n",
            "Index:            68\n",
            "Reference:        Selling NFTs on OpenSea\n",
            "Ever since the digital artist known as Beeple sold a JPG file for US$69 Million back in March 2021, there has been a lot of interest in using a blockchain for selling digital art.\n",
            "> Beeple’s collaged JPG was made, or “minted,” in February as a “nonfungible token,” or NFT. A secure network of computer systems that records the sale on a digital ledger, known as a blockchain, gives buyers proof of authenticity and ownership. Most pay with the Ethereum cryptocurrency. “Everydays” was the first purely digital NFT sold by Christie’s, and it offered to accept payment in Ethereum, another first for the 255-year-old auction house. —  Scott Reyburn,  The New York Times\n",
            "\n",
            "\n",
            "I looked into selling my digital art as NFTs, and after conducting a little research, I found that the OpenSea marketplace using the Polygon blockchain [10] is an excellent way to go. This is for both reasons of cost and environmental impact.\n",
            "OpenSea allows creators to post items for sale on the Ethereum blockchain or the Polygon blockchain, also known as MATIC. Here is a table that shows some of the differences between the Ethereum and Polygon blockchains.\n",
            "```\n",
            "Feature                      Ethereum         Polygon Account initializing:        ~US$50           Free Selling cost:                2.5%             2.5% Selling items in bundles:    Yes              No Blockchain validation:       Proof of work    Proof of stake Environmental impact:        Huge*            Minimal *expected to change in Q1/Q2 2022\n",
            "```\n",
            "As you can see from the table, setting up an account on OpenSea to sell on the Polygon blockchain is free, where it costs about $50 to use the Ethereum blockchain. OpenSea will take 2.5% of the sale price for sales on either blockchain. Also, selling items in bundles is not available on the Polygon blockchain.\n",
            "Question:         What are the differences between selling NFTs on the Ethereum and Polygon blockchains according to Robert A. Gonsalves in his article \"GANshare: Creating and Curating Art with AI for Fun and Profit\"?\n",
            "GPT-4 Answer:     According to he, there are several differences between selling NFTs on the Ethereum and Polygon blockchains. For instance, initializing an account on the Ethereum blockchain costs around US$50, while it's free on the Polygon blockchain. Both blockchains have a selling cost of 2.5%. However, selling items in bundles is only possible on the Ethereum blockchain. In terms of blockchain validation, Ethereum uses proof of work while Polygon uses proof of stake. Lastly, the environmental impact of Ethereum is huge, although this is expected to change in Q1/Q2 2022, while Polygon's environmental impact is minimal.\n",
            "LLaMa 7B Answer:  According to he in his article \"GANshare: Creating and Curating Art with AI for Fun and Profit,\" the differences between selling NFTs on the Ethereum and Polygon blockchains are as follows:\n",
            "* Cost: Setting up an account on OpenSea to sell on the Polygon blockchain is free, while it costs about $50 to use the Ethereum blockchain.\n",
            "* Selling cost: OpenSea takes 2.5% of the sale price for sales on either blockchain.\n",
            "* Blockchain validation: The Ethereum blockchain uses proof of work, while the Polygon blockchain uses proof of stake.\n",
            "* Environmental impact: The Ethereum blockchain has a significant environmental impact, while the Polygon blockchain has a minimal impact expected to change in Q1/Q2 2022.\n",
            "LLaMa 7B Score:   0.93594706\n",
            "\n",
            "Index:            69\n",
            "Reference:        Acknowledgments\n",
            "I want to thank Jennifer Lim and Oliver Strimpel for their help with this article.\n",
            "Question:         What is the main purpose of the GANfolk project as described by Robert A. Gonsalves?\n",
            "GPT-4 Answer:     The main purpose of the GANfolk project, as described by he, is to use artificial intelligence, specifically StyleGAN2, VQGAN, and GPT-3, to create portraits of fictional characters from open-source images. These AI-generated portraits are then sold as NFTs (Non-Fungible Tokens).\n",
            "LLaMa 7B Answer:  The main purpose of the GANfolk project, as described by he, is to use artificial intelligence (AI) to create portraits of fictional people that can be sold as non-fungible tokens (NFTs).\n",
            "LLaMa 7B Score:   0.9735285\n",
            "\n",
            "Index:            70\n",
            "Reference:        Training the GAN\n",
            "I ran the training using Google Colab Pro. Using that service I could run for up to 24 hours on a high-end GPU, an NVIDIA Tesla P10 with 16 GB of memory. I also used Google Drive to retain the work in progress between runs. It took about 13 days to train the GAN, sending 5 million source images through the system. Here is a random sample of the results.\n",
            "You can see from the sample of 28 images above that MachineRay produced paintings in a variety of styles, although there are some visual commonalities between them. There are hints to the styles in the source images, but no exact copies.\n",
            "Question:         What platform did Robert A. Gonsalves use to train the MachineRay GAN and how long did it take?\n",
            "GPT-4 Answer:     He used Google Colab Pro to train the MachineRay GAN. It took him about 13 days to train the GAN, during which he sent 5 million source images through the system.\n",
            "LLaMa 7B Answer:  According to the text, he used Google Colab Pro to train the MachineRay GAN, and it took approximately 13 days to train the model using 5 million source images.\n",
            "LLaMa 7B Score:   0.9614471\n",
            "\n",
            "Index:            71\n",
            "Reference:        Searching for Design Patents\n",
            "Now that we have the text and image embeddings for over 67 thousand design patents, running a text search is fast and easy.\n",
            "Searching starts with typing in a query text, for example, “curved water faucet”. The system then creates a text embedding for the query using the CLIP text encoder. You can choose one of the three methods for searching.\n",
            "1. Text to text search —  This compares the embedding of the query to the list of text embeddings for the design patent captions and returns the top result.\n",
            "2. Text to image search —  This compares the query embedding to the list of image embeddings and returns the top results.\n",
            "3. Text to combined text/image search  — This replicates the query embedding and compares the combined pair to a list of combined text and image embeddings and returns the top results.\n",
            "Note that using both text and image embeddings for semantic search is not new. For example, you can read about recent developments in these papers, “Dialog-based Interactive Image Retrieval” by Xiaoxiao Guo et al. [6] and “Visual-Semantic Matching by Exploring High-Order Attention and Distraction” by Yongzhi Li, et al. [7].\n",
            "Question:         What are the three methods of searching that Robert A. Gonsalves describes in his article \"Using OpenAI’s CLIP to Search for Design Patents\"?\n",
            "GPT-4 Answer:     In his article, he describes three methods for searching design patents. The first is a text to text search, which compares the embedding of the query to the list of text embeddings for the design patent captions. The second is a text to image search, which compares the query embedding to the list of image embeddings. The third method is a text to combined text/image search, which replicates the query embedding and compares the combined pair to a list of combined text and image embeddings.\n",
            "LLaMa 7B Answer:  According to he' article \"Using OpenAI’s CLIP to Search for Design Patents,\" there are three methods of searching:\n",
            "1. Text-to-text search: This compares the query text embedding to the list of text embeddings for design patent captions and returns the top result.\n",
            "2. Text-to-image search: This compares the query embedding to the list of image embeddings and returns the top results.\n",
            "3. Text-to-combined text/image search: This replicates the query embedding and compares the combined pair to a list of combined text and image embeddings and returns the top results.\n",
            "LLaMa 7B Score:   0.79663163\n",
            "\n",
            "Index:            72\n",
            "Reference:        Acknowledgments\n",
            "I would like to thank Jennifer Lim and Matthew Conroy for their help and feedback on this project.\n",
            "Question:         What is the main topic that Robert A. Gonsalves discusses in his article 'Benford’s Law — A Simple Explanation'?\n",
            "GPT-4 Answer:     In his article, he provides a simple explanation of Benford's Law, also known as the \"First-Digit Law\".\n",
            "LLaMa 7B Answer:  The main topic discussed in he' article \"Benford's Law — A Simple Explanation\" is Benford's Law, a statistical phenomenon that describes the distribution of digits in numerical data.\n",
            "LLaMa 7B Score:   0.88123584\n",
            "\n",
            "Index:            73\n",
            "Reference:        Discussion and Future Work\n",
            "The MAGnet system works fairly well. It seems to do a better job generating abstract paintings than representational paintings. It converges on a solution fairly quickly, within five generations.\n",
            "Although the results of MAGnet are pretty good, if you play with it, you may see the same visual themes recur occasionally. This may be due to the limited number of source images used during the training (about 4,000 initial images, augmented to over 12,000 via cropping, and filtered to 10,000). Training with more images will probably reduce the frequency of repeated visual themes.\n",
            "Another area that could be improved is the textures within the images. There seem to be some “computery” artifacts in the flat areas. This could be due to the reimplementation of the CUDA operations.\n",
            "I noticed that rosinality is currently working on a new open source project on GitHub called  alias-free-gan-pytorch . Sounds promising!\n",
            "Question:         What are some of the improvements Robert A. Gonsalves suggests for the MAGnet project?\n",
            "GPT-4 Answer:     he suggests that the MAGnet system could be improved by training with more images to reduce the frequency of repeated visual themes. He also mentions that the textures within the images could be enhanced as there seem to be some \"computery\" artifacts in the flat areas, possibly due to the reimplementation of the CUDA operations.\n",
            "LLaMa 7B Answer:  According to he, some improvements that could be made to the MAGnet project include:\n",
            "1. Increasing the number of source images used during training to reduce the frequency of repeated visual themes.\n",
            "2. Improving the textures within the images by addressing the \"computery\" artifacts in the flat areas.\n",
            "3. Exploring new open-source projects, such as alias-free-gan-pytorch, which may offer promising improvements.\n",
            "LLaMa 7B Score:   0.8092259\n",
            "\n",
            "Index:            74\n",
            "Reference:        References\n",
            "[1] J. Schulman et al.,  ChatGPT: Optimizing Language Models for Dialogue  (2020)\n",
            "\n",
            "[2] T. Brown et al.,  Language Models are Few-Shot Learners  (2020)\n",
            "\n",
            "[3] L. Ouyang et al.,  Training language models to follow instructions with human feedback  (2020)\n",
            "\n",
            "[4] J. Schulman et al.,  Proximal Policy Optimization Algorithms  (2017)\n",
            "Question:         What is the main focus of Robert A. Gonsalves' article \"Using ChatGPT as a Creative Writing Partner — Part 1: Prose\"?\n",
            "GPT-4 Answer:     The main focus of his article is to explore how the latest language model from OpenAI, known as ChatGPT, can be utilized as a tool for creative writing, specifically in the genres of poetry, fiction, and screenplays.\n",
            "LLaMa 7B Answer:  The main focus of he' article \"Using ChatGPT as a Creative Writing Partner — Part 1: Prose\" is to explore the potential of using ChatGPT, a language model developed by OpenAI, as a creative writing partner for tasks such as poetry, fiction, and screenplays. The author provides examples of how ChatGPT can be used to generate ideas, structure stories, and even write entire pieces of prose, highlighting the model's ability to assist writers in their creative process.\n",
            "LLaMa 7B Score:   0.95835644\n",
            "\n",
            "Index:            75\n",
            "Reference:        Upsizing Options\n",
            "Midjourney also has options for upsizing that can improve the details of the final images. For this test, I used the prompt, “steampunk rotary phone,” with the quality set to 2.0. I had to use the “seed” trick to test out the three upsizing options, lite, regular, and beta. (For the record, this was seed #24863.) Here are the results, along with a close-up of the rotary dialer.\n",
            "All three images show some fine details, but the beta upsize on the right seems a bit more orderly. For example, the inner ring on the dialer seems to be more realistically formed.\n",
            "Question:         What were the results of Robert's test on the three upsizing options in Midjourney V4?\n",
            "GPT-4 Answer:     he found that all three upsizing options - lite, regular, and beta - in Midjourney V4 showed fine details in the final images. However, he noted that the beta upsize seemed a bit more orderly, with elements like the inner ring on the dialer appearing more realistically formed.\n",
            "LLaMa 7B Answer:  Based on the information provided in the passage, he tested the three upsizing options (Lite, Regular, and Beta) in Midjourney V4 for creating a steampunk rotary phone. The results showed that the Beta upsize produced an image with more realistic details, particularly in the inner ring of the dialer.\n",
            "LLaMa 7B Score:   0.7522942\n",
            "\n",
            "Index:            76\n",
            "Reference:        More on ChatGPT as a Creative Writing Partner\n",
            "Here are the first two articles in this series.\n",
            "Question:         What is the focus of Robert A. Gonsalves' article \"Using ChatGPT as a Creative Writing Partner — Part 3: Picture Books\"?\n",
            "GPT-4 Answer:     In his article, he discusses how the latest language model from OpenAI, known as ChatGPT, can be used as a tool for writing children's books and creating illustrations with Midjourney.\n",
            "LLaMa 7B Answer:  The focus of he' article \"Using ChatGPT as a Creative Writing Partner — Part 3: Picture Books\" is on exploring how the latest language model from OpenAI, ChatGPT, can be used to write children's books and create illustrations with Midjourney.\n",
            "LLaMa 7B Score:   0.9462501\n",
            "\n",
            "Index:            77\n",
            "Reference:        Prompts that Don’t Work\n",
            "Prompts that involve people (i.e., “children playing,” “Barack Obama,” “The Mona Lisa,” etc.) do not work because of the intentional lack of people in the training data for GLIDE.\n",
            "Prompts of abstract concepts (i.e., “freedom,” “a brand new day,” “haywire,” etc.) also do not work because of a lack of consensus on the content of images on the Internet that are labeled using these words.\n",
            "Here’s a question that defines a rule of thumb for using BIG.art: When you do a Google search of the prompt and look at the resulting images, do they look roughly similar? If so, generating an image using that prompt will generate good images. Oh, and don’t try to render people. It won’t work.\n",
            "Question:         What is the rule of thumb for using BIG.art as suggested by Robert A. Gonsalves?\n",
            "GPT-4 Answer:     According to him, a good rule of thumb for using BIG.art is to do a Google search of the prompt and look at the resulting images. If they look roughly similar, then generating an image using that prompt will likely produce good results. However, he advises against trying to render people, as it won't work due to the intentional lack of people in the training data for GLIDE.\n",
            "LLaMa 7B Answer:  According to he, the rule of thumb for using BIG.art is to Google search the prompt and evaluate if the resulting images are roughly similar. If so, generating an image using that prompt will likely produce good results. Additionally, he advises against attempting to render people in the generated images as it may not work due to the lack of people in the training data for GLIDE.\n",
            "LLaMa 7B Score:   0.9735288\n",
            "\n",
            "Index:            78\n",
            "Reference:        Rendering Options\n",
            "As I mentioned above, there are a lot of rendering options in Midjourney. To run an apples-to-apples test, however, I learned how to retrieve and use the random seed used to generate images. It’s a bit hidden, but after I generated an image, I created a “reaction” to my image and then chose the “envelope” emoji.\n",
            "This triggered the bot to send me a message that included the seed for the run, like this.\n",
            "I then used the  --seed 6534  parameter to generate the same image with different options. For example, here are renderings using the prompt, “painting of the Boston Public Garden in springtime,” with the quality set to 0.5, 1.0 (the default), and 2.0.\n",
            "It’s interesting to see how much the image changed with the different quality selections. The yellow tree in the middle roughly stayed the same, but the flowers, buildings, and people all moved around. It’s subtle, but the image on the right with a quality of 2.0 seems to have the fewest “problem” areas.\n",
            "Question:         How does Robert retrieve the random seed used to generate images in Midjourney Art?\n",
            "GPT-4 Answer:     In Midjourney Art, he retrieves the random seed used to generate images by first creating an image. After the image is generated, he creates a \"reaction\" to his image and chooses the \"envelope\" emoji. This action triggers the bot to send him a message that includes the seed for the run. He then uses the --seed parameter to generate the same image with different options.\n",
            "LLaMa 7B Answer:  According to the text, he retrieves the random seed used to generate images in Midjourney Art by creating a \"reaction\" to his image and then choosing the \"envelope\" emoji. This triggers the bot to send him a message that includes the seed for the run.\n",
            "LLaMa 7B Score:   0.9046036\n",
            "\n",
            "Index:            79\n",
            "Reference:        Acknowledgments\n",
            "I want to thank Kate Li for singing the song for this article. I also thank Pete Fayan from the Agents for his engagement with this project. Finally, I thank Jennifer Lim for reviewing the article and providing feedback.\n",
            "Question:         What is the main focus of Robert A. Gonsalves' article \"Writing Songs with GPT-4: Part 3, Melodies\"?\n",
            "GPT-4 Answer:     The main focus of his article is on how to use the latest language model from OpenAI, GPT-4, to assist in writing melodies for new songs.\n",
            "LLaMa 7B Answer:  The main focus of he' article \"Writing Songs with GPT-4: Part 3, Melodies\" is to explore how the latest language model from OpenAI, GPT-4, can be used to help write melodies for new songs.\n",
            "LLaMa 7B Score:   0.98240304\n",
            "\n",
            "Index:            80\n",
            "Reference:        GANfolk System Components\n",
            "I will discuss the details of the components and processes used in the GANfolk system in the following sections.\n",
            "Question:         What is the main purpose of the GANfolk system as described by Robert A. Gonsalves?\n",
            "GPT-4 Answer:     The main purpose of the GANfolk system, as described by him, is to use artificial intelligence to create portraits of fictional people. These AI-generated portraits are then sold as NFTs (Non-Fungible Tokens). The system utilizes StyleGAN2, VQGAN, and GPT-3 to synthesize diverse characters from open-source images.\n",
            "LLaMa 7B Answer:  According to he, the main purpose of the GANfolk system is to use artificial intelligence (AI) to create portraits of fictional people that can be sold as non-fungible tokens (NFTs).\n",
            "LLaMa 7B Score:   0.9729106\n",
            "\n",
            "Index:            81\n",
            "Reference:        Training the System\n",
            "I trained the system using Google Colab. It took about four days to run. The system has an Nvidia Tesla V100 GPU which can run up to 14 teraFLOPS (14 trillion floating-point operations per second). So it took about 4.6 exaFLOPS to train (about 4.6 quintillion floating-point operations). That’s a lot of math.\n",
            "Here’s the shell command I used to kick things off.\n",
            "```\n",
            "python stylegan2-ada/train.py --aug=ada --target=0.7 \\ --mirror=1 --snap=1 --gpus=1 \\ --data='/content/drive/MyDrive/datasets/paintings' \\ --outdir='/content/drive/MyDrive/results_stylegan2_ada'\n",
            "```\n",
            "Setting the  aug  parameter to  ada  enables the Adaptive Discriminator Augmentation to kick in when needed. Setting the  target  parameter to  0.7  prevents  p  from going over 0.7, which maximizes the dynamic augmentation without leaking any further augmentation into the final images.\n",
            "Here’s a graph of the training results over time. I am using the Fréchet Inception Distance (FID) as a metric for image quality and diversity, where a lower score is better [2]. You can read about the FID score in Cecelia Shao’s post  here .\n",
            "You can see how StyleGAN2 ADA outperforms the original StyleGAN2 for the same number of iterations. The FID score for SG2A bottomed out at just over 100 after about 300 iterations. Here are some samples of the results.\n",
            "The GAN seems to produce a nice variety of abstract paintings with interesting compositions with varied color schemes. But there is room for improvement.\n",
            "Question:         What metric did Robert A. Gonsalves use to measure the quality and diversity of the images produced by StyleGAN2 ADA?\n",
            "GPT-4 Answer:     He used the Fréchet Inception Distance (FID) as a metric to measure the quality and diversity of the images produced by StyleGAN2 ADA. A lower FID score indicates better image quality and diversity.\n",
            "LLaMa 7B Answer:  According to the text, he used the Fréchet Inception Distance (FID) as a metric for image quality and diversity to measure the performance of StyleGAN2 ADA.\n",
            "LLaMa 7B Score:   0.93756616\n",
            "\n",
            "Index:            82\n",
            "Reference:        Creating Abstract Art with StyleGAN2 ADA\n",
            "Robert A. Gonsalves\n",
            "Jan 1, 2021\n",
            "StyleGAN Abstract Art - https://medium.com/towards-data-science/creating-abstract-art-with-stylegan2-ada-ea3676396ffb\n",
            "Back in August 2020, I created a project called  MachineRay  that uses Nvidia’s StyleGAN2 to create new abstract artwork based on early 20th century paintings that are in the public domain. Since then, Nvidia has released a new version of their AI model, StyleGAN2 ADA, that is designed to yield better results when generating images from a limited dataset [1]. (I’m not sure why they didn't call it StyleGAN3, but I’ll refer to the new model as SG2A to save a few characters). In this article, I’ll show you how I used SG2A to create better looking abstract paintings.\n",
            "Question:         What is the new version of Nvidia's AI model that Robert A. Gonsalves used in his project to create better looking abstract paintings?\n",
            "GPT-4 Answer:     He used the new version of Nvidia's AI model called StyleGAN2 ADA (SG2A) in his project to generate improved abstract artwork.\n",
            "LLaMa 7B Answer:  The new version of Nvidia's AI model that he used in his project is called StyleGAN2 ADA (Adaptive Discriminator Augmentation).\n",
            "LLaMa 7B Score:   0.8381436\n",
            "\n",
            "Index:            83\n",
            "Reference:        Societal Bias in Language Models\n",
            "Note that, like most large language models, InstructGPT and ChatGTP both suffer from exposure to implicit social bias and toxicity in the original training data. To combat this, OpenAI actively worked to “align” the models to be helpful, honest, and harmless [3].\n",
            "> We make progress on aligning language models by training them to act in accordance with the user’s intention. This encompasses both explicit intentions such as following instructions and implicit intentions such as staying truthful, and not being biased, toxic, or otherwise harmful. — Long Ouyang et al. from OpenAI\n",
            "\n",
            "\n",
            "However, the authors state that the new model shows only small improvements in toxicity over GPT-3, but no improvement in bias.\n",
            "Question:         What does Robert A. Gonsalves say about the improvements in toxicity and bias in the new model compared to GPT-3?\n",
            "GPT-4 Answer:     According to him, the new model shows only minor improvements in toxicity over GPT-3, but no improvement in bias.\n",
            "LLaMa 7B Answer:  According to he, the new model shows only small improvements in toxicity over GPT-3, but no improvement in bias.\n",
            "LLaMa 7B Score:   0.98826253\n",
            "\n",
            "Index:            84\n",
            "Reference:        References\n",
            "[1] Netflix, Connected, 20200,  https://www.netflix.com/title/81031737\n",
            "\n",
            "[2] Scott, P. and Fasli, M., “CSM-349 — Benford’s Law: An Empirical Investigation and a Novel Explanation”, 2001,  http://repository.essex.ac.uk/8664/1/CSM-349.pdf\n",
            "\n",
            "[3] Berger, A., Hill, T.P., “A basic theory of Benford’s Law”,  Probability Surveys , 2011,  https://projecteuclid.org/download/pdfview_1/euclid.ps/1311860830\n",
            "\n",
            "[4] Kossovsky A. E., “Arithmetical Tugs of War and Benford’s Law”, 2014,  https://arxiv.org/ftp/arxiv/papers/1410/1410.2174.pdf\n",
            "\n",
            "[5]Benford, F. “The Law of anomalous numbers”,  Proceedings of the American Philosophical Society , 78, 551–572, 1938,  https://mdporter.github.io/SYS6018/other/(Benford) The Law of Anomalous Numbers.pdf\n",
            "[6] Newcomb, S., “Note on the frequency of use of different digits in natural numbers”, American Journal of Math. 4, 39–40, 1881\n",
            "\n",
            "[7] Stigler S., “Stigler’s Law of Eponymy”, 1980,  https://archive.org/details/sciencesocialstr0039unse/page/147/mode/2up\n",
            "\n",
            "[8] Fewster, R.M., “A Simple Explanation of Benford’s Law”,  The American Statistician , Vol. 63, No 1, 2009,  https://www.stat.auckland.ac.nz/~fewster/RFewster_Benford.pdf\n",
            "\n",
            "[9] US Census Data, 2010,  https://www2.census.gov\n",
            "\n",
            "[10] Decker, E. H., Kerkhoff, A. J., & Moses, M. E. (2007). Global patterns of city size distributions and their fundamental drivers.  PloS one ,  2 (9), e934.  https://doi.org/10.1371/journal.pone.0000934\n",
            "\n",
            "[11] Nigrini, M.J., “I’ve Got Your Number”,  Journal of Accountancy , 1999,  https://www.journalofaccountancy.com/issues/1999/may/nigrini.html\n",
            "\n",
            "[12] The state of Oklahoma, “Oklahoma’s Open Data”,  https://data.ok.gov , 2019\n",
            "\n",
            "[13] Durtschi, C., Hillison, W. Pacini C., “The effective use of Benford’s law to assist in detecting fraud in accounting data”,  Journal of Forensic Accounting , 2004,  http://www.agacgfm.org/AGA/FraudToolkit/documents/BenfordsLaw.pdf\n",
            "\n",
            "[14] New York State, Waterbody Classifications, 2019,  https://data.ny.gov/Energy-Environment/Waterbody-Classifications/8xz8-5u5u\n",
            "\n",
            "[15] Kossovsky A. E., “Towards A Better Understanding of the Leading Digits Phenomena”, 2006,  https://arxiv.org/ftp/math/papers/0612/0612627.pdf\n",
            "\n",
            "[16] Rind, D., “Do Variations in the Solar Cycle Affect Our Climate System?”  Science Briefs , Goddard Institute for Space Studies, NASA, 2009,  https://www.giss.nasa.gov/research/briefs/rind_03/\n",
            "\n",
            "[17] Sambridge S., Tkalčić H., and Jackson A., “Benford’s law in the natural sciences”,  Geophysical Research Letters , Vol. 37, L22301, 2010,  https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1029/2010GL044830\n",
            "Question:         What is the main topic that Robert A. Gonsalves discusses in his article 'Benford’s Law — A Simple Explanation'?\n",
            "GPT-4 Answer:     In his article, he provides a simple explanation of Benford's Law, a mathematical phenomenon also known as the \"First-Digit Law\". He discusses its history, its applications, and some of the theories that have been proposed to explain why it holds true in many real-world data sets.\n",
            "LLaMa 7B Answer:  The main topic discussed in he' article \"Benford's Law - A Simple Explanation\" is Benford's Law, a phenomenon observed in many natural and artificial systems where the distribution of digits in numerical data follows a specific pattern.\n",
            "LLaMa 7B Score:   0.86508125\n",
            "\n",
            "Index:            85\n",
            "Reference:        Using ChatGPT as a Creative Writing Partner — Part 3: Picture Books\n",
            "Robert A. Gonsalves\n",
            "Feb 7, 2023\n",
            "ChatGPT Pictures - https://medium.com/towards-data-science/using-chatgpt-as-a-creative-writing-partner-part-3-picture-books-4f45e5dfe8dd\n",
            "This is the third and final installment of my series of three articles on using OpenAI’s ChatGPT language model [1] for creative writing. In the  first article , I described using ChatGPT for writing prose, poetry, novels, and screenplays. In the  second article , I showed how to use the system to create chord sequences to compose music in various styles.\n",
            "My latest experiment for this article was creating a picture book using ChatGPT. Because the system can’t render images directly, I asked it to describe the scenes which I rendered using Midjourney [2], a text-to-image creation system that I explored in an  earlier article .\n",
            "Question:         What was Robert A. Gonsalves' latest experiment in the article \"Using ChatGPT as a Creative Writing Partner — Part 3: Picture Books\"?\n",
            "GPT-4 Answer:     In this article, his latest experiment involved creating a picture book using ChatGPT. He asked the system to describe scenes, which he then rendered using Midjourney, a text-to-image creation system.\n",
            "LLaMa 7B Answer:  In the article \"Using ChatGPT as a Creative Writing Partner — Part 3: Picture Books,\" he' latest experiment was to use ChatGPT to describe scenes that he then rendered into images using Midjourney, a text-to-image creation system.\n",
            "LLaMa 7B Score:   0.8862972\n",
            "\n",
            "Index:            86\n",
            "Reference:        A Tale of Two GANs\n",
            "I used two GANs for this project, an independent implementation of StyleGAN2 by Kim Seonghyeon (aka rosinality) and VQGAN by Patrick Esser et al. I trained both GANs for three weeks using the 5,400 training images in the GANfolk dataset.\n",
            "In my prior research, I found that StyleGAN2 does an excellent job creating a global structure in the generated images that roughly resembles the types in the training data. However, the image details are often hazy or missing. But VQGAN is entirely complementary. It doesn’t know how to create a global structure, but it does a good job filling in realistic image details. Using both GANs is the best of both worlds.\n",
            "Question:         What are the strengths of the two GANs, StyleGAN2 and VQGAN, that Robert used in the GANfolk project?\n",
            "GPT-4 Answer:     In the GANfolk project, he found that StyleGAN2 excelled at creating a global structure in the generated images that closely resembled the types in the training data, but it often produced hazy or missing image details. On the other hand, VQGAN was proficient at filling in realistic image details, but it struggled to create a global structure. Therefore, he used both GANs to leverage their strengths and compensate for their weaknesses.\n",
            "LLaMa 7B Answer:  In the GANfolk project, he used two GANs: StyleGAN2 and VQGAN. The strengths of these GANs are:\n",
            "* StyleGAN2 is excellent at creating a global structure in generated images that resembles the types in the training data, but often lacks image details.\n",
            "* VQGAN fills in realistic image details but does not create a global structure.\n",
            "Using both GANs together is the best of both worlds, as they complement each other's strengths.\n",
            "LLaMa 7B Score:   0.8415625\n",
            "\n",
            "Index:            87\n",
            "Reference:        Got Writer’s Block? It’s GPT-2 to the Rescue!\n",
            "Robert A. Gonsalves\n",
            "Sep 1, 2020\n",
            "PlotJam - https://medium.com/towards-data-science/got-writers-block-it-s-plotjam-to-the-rescue-e555db9f3272\n",
            "In this post, I’ll show you how Artificial Intelligence (AI) and Machine Learning (ML) can be used to help you get a start on that novel you always wanted to write. I’ll begin with a brief background on how computers process text using AI. Then I’ll describe how I set up an ML model called GPT-2 to generate new plot summaries, and give instructions on how you can create some new story ideas for yourself.\n",
            "This is the second part of my series of articles on how AI can be used for creative endeavors. The first part is on how to use ML to create abstract art, available  here .\n",
            "Question:         What is the main purpose of Robert A. Gonsalves' project, PlotJam?\n",
            "GPT-4 Answer:     The main purpose of PlotJam, as explained by he, is to use AI and Machine Learning to help writers overcome writer's block. He uses an ML model called GPT-2 to generate new plot summaries, providing a starting point for writers to create new stories.\n",
            "LLaMa 7B Answer:  According to the article, the main purpose of he' project, PlotJam, is to use Artificial Intelligence (AI) and Machine Learning (ML) to help individuals overcome writer's block and generate new plot summaries for stories they want to write.\n",
            "LLaMa 7B Score:   0.8775556\n",
            "\n",
            "Index:            88\n",
            "Reference:        Ownership of Inputs and Generated Media\n",
            "Midjourney and RunwayML have different policies regarding the imagery and text used for prompts and the resulting generated images. Midjourney distinguishes between paid and free users, whereas RunwayML uses the same policy for both types of users.\n",
            "Question:         What was Robert A. Gonsalves' main objective in the Muybridge Derby project?\n",
            "GPT-4 Answer:     His main objective was to use Midjourney and RunwayML to transform Eadweard Muybridge’s photo sequences into high-resolution videos.\n",
            "LLaMa 7B Answer:  According to the provided text, he' main objective in the Muybridge Derby project was to transform Eadweard Muybridge's photo sequences into high-resolution videos using Midjourney and RunwayML.\n",
            "LLaMa 7B Score:   0.796286\n",
            "\n",
            "Index:            89\n",
            "Reference:        Using ChatGPT as a Creative Writing Partner — Part 3: Picture Books\n",
            "towardsdatascience.com\n",
            "Question:         What is the main focus of Robert A. Gonsalves' article \"Using ChatGPT as a Creative Writing Partner — Part 2: Music\"?\n",
            "GPT-4 Answer:     In his article, he explores how the latest language model from OpenAI, ChatGPT, can be utilized to compose chords for new songs. He also discusses the use of music by Band-in-a-Box in this process.\n",
            "LLaMa 7B Answer:  The main focus of he' article \"Using ChatGPT as a Creative Writing Partner — Part 2: Music\" is to explore how the latest language model from OpenAI, ChatGPT, can be used to assist in composing chords for new songs, with music by Band-in-a-Box.\n",
            "LLaMa 7B Score:   0.939209\n",
            "\n",
            "Index:            90\n",
            "Reference:        PyTextRank\n",
            "The system uses an algorithm called TextRank to extract keywords and phrases from the user’s question[5]. It’s based on the famous PageRank algorithm that Larry Page, et. al, invented for web searches. You pass in a block of text and it returns key phrases ranked by relevance. Here is some sample code that shows how it can extract keywords from a question.\n",
            "```\n",
            "Question:  Can a computer beat a grandmaster chess player? Keywords:  a grandmaster, chess player, a computer\n",
            "```\n",
            "You can see that it does a good job of extracting the key phrases.\n",
            "Question:         How does the AI 8-Ball extract keywords from a user's question?\n",
            "GPT-4 Answer:     The AI 8-Ball uses an algorithm called TextRank to extract keywords and phrases from the user's question. This algorithm is based on the PageRank algorithm used for web searches. It takes a block of text and returns key phrases ranked by relevance. For example, from the question \"Can a computer beat a grandmaster chess player?\", it extracts the key phrases \"a grandmaster\", \"chess player\", and \"a computer\".\n",
            "LLaMa 7B Answer:  The AI 8-Ball uses an algorithm called TextRank to extract keywords and phrases from a user's question. TextRank is based on the famous PageRank algorithm used for web searches, which was invented by Larry Page and his colleagues. The system passes in a block of text and returns key phrases ranked by relevance using this algorithm.\n",
            "LLaMa 7B Score:   0.9562829\n",
            "\n",
            "Index:            91\n",
            "Reference:        Exploring the Versions\n",
            "Midjourney released its first version of the image-generating service in March 2022. Version 2 was released soon after in April, and version 3 in July. The beta for version 4 was released on November 10, 2022.\n",
            "As mentioned above, you can use the  /settings  command to select versions 1 through 4. I noticed that version 4 creates 512x512 initial renderings, whereas the previous versions created initial images at 256x256. To test all four versions, I sent in the prompt “painting of a French man wearing a hat drinking wine at an outdoor cafe” to see what I would get.\n",
            "The four thumbnails in the upper left were rendered using version 1, version 2 at the upper right, version 3 at the lower left, and version 4 at the lower right. Wow! What a big difference in quality! With each version, the images steadily improve. The version 4 images look extraordinary! In the next section, I’ll show some quantitative results.\n",
            "Question:         What significant difference did Robert A. Gonsalves notice between the fourth and previous versions of Midjourney Art?\n",
            "GPT-4 Answer:     he noticed that the fourth version of Midjourney Art creates initial renderings at a resolution of 512x512, unlike the previous versions which created images at 256x256. Additionally, he observed a significant improvement in the quality of images generated by the fourth version compared to its predecessors.\n",
            "LLaMa 7B Answer:  In the article, he noticed that version 4 of Midjourney Art creates 512x512 initial renderings, whereas the previous versions created initial images at 256x256. This significant difference in image size leads to a noticeable improvement in quality for the images generated by version 4 compared to the earlier versions.\n",
            "LLaMa 7B Score:   0.9377612\n",
            "\n",
            "Index:            92\n",
            "Reference:        a wood workshop\n",
            "Question:         What machine learning tools does Robert A. Gonsalves suggest using to create high-resolution digital paintings?\n",
            "GPT-4 Answer:     he suggests using GLIDE and BSRGAN to create ultra-high-resolution digital paintings with fine details.\n",
            "LLaMa 7B Answer:  According to the article, he suggests using GLIDE and BSRGAN to create ultra-high-resolution digital paintings with fine details.\n",
            "LLaMa 7B Score:   0.9452787\n",
            "\n",
            "Index:            93\n",
            "Reference:        The Agents: License to Trill\n",
            "My third and final project was writing a song in the style of the ska band, The Agents. I started by having the system analyze the Agent’s song “ Grow .”\n",
            "> RobG: Analyze the lyrics and chords of the song “Grow” by the ska band The Agents.\n",
            "> [Verse 1] Arm at D, the very first day I met you girl The very first time I held your hand The very first time I looked into your eyes I told you that I loved you\n",
            "> [Verse 2] From that day till ever after The troubling times, the sorrow, the laughter Hand in hand, now we walk together Can’t imagine anything better now\n",
            "> [Chorus] Grow, let your love grow, grow Let your love come, rise up, rise up Plant the seed and let your love grow Higher and higher\n",
            "> GPT-4 “Grow” by the ska band The Agents is a love song that reflects on the various stages and moments of a romantic relationship. The lyrics portray the initial feelings of love, the growth of the relationship, and the deepening bond between the two people. The song’s overall message is to nurture love and let it grow to its fullest potential.\n",
            "> The lyrics emphasize the importance of love and companionship, as seen in lines like “The very first day I met you girl,” “Hand in hand, now we walk together,” and “Can’t imagine anything better now.” The repetition of the phrase “Grow, let your love grow, grow” highlights the song’s central theme of growth and development in love.\n",
            "\n",
            "\n",
            "The system produced an excellent and concise analysis of the song. It picked up the song's central theme as being about the growth of a relationship and pulled out a few key lines in the lyrics to emphasize this.\n",
            "Unlike the first two experiments described above, this time I gave GPT-4 a new theme and asked it to write five possible song titles with descriptions.\n",
            "> RobG: Create the titles for five songs in this style about overcoming fears. Include a one-sentence description of the meaning of each song.\n",
            "> GPT-4:  “Break Through the Shadows” This song is about finding inner strength and courage to step out of one’s comfort zone and face the unknown.\n",
            "> “Fearless Heartbeat” A powerful anthem celebrating the resilience of the human spirit, encouraging listeners to embrace their fears and turn them into a driving force for personal growth.\n",
            "> “Rising Above the Storm” This song tells the story of overcoming adversity and challenges, illustrating how facing one’s fears can lead to newfound strength and self-discovery.\n",
            "> “Unchained Courage” A motivational tune that highlights the importance of breaking free from the chains of fear and self-doubt, urging individuals to pursue their dreams with relentless determination.\n",
            "> “Dance with the Fear” An upbeat track that encourages listeners to accept and acknowledge their fears, using them as a catalyst for personal growth and transformation.\n",
            "\n",
            "\n",
            "These were all good, although they read like motivational posters. But overall, I thought that “Dance with the Fear” was the best of the batch. And I know that ska is an excellent style of music for dancing.\n",
            "As I did in the examples above, I asked GPT-4 to write the lyrics for the new song.\n",
            "> RobG: Write the lyrics for “Dance with the Fear.”\n",
            "> GPT-4:  [Verse 1] Waking up to a brand new day, Feeling the fear but not turning away, Facing the unknown, that’s where we’ll start, With an open mind and a fearless heart.\n",
            "> [Pre-Chorus] No more hiding, no more doubts, It’s time to let our courage out.\n",
            "> [Chorus] Dance with the fear, don’t run away, Embrace the challenge, come what may. Find your rhythm, face the unknown, With every step, your strength has grown.\n",
            "> [Verse 2] In the darkness, we’ll find the light, Guided by dreams that take flight. No more trembling, no more fright, Together we’ll soar to new heights.\n",
            "> [Pre-Chorus] … [Chorus] …\n",
            "> [Bridge] When fear is knocking at your door, Stand tall, and face it once more. The journey’s long, but you’ll find, You’re stronger than you realized.\n",
            "> [Chorus] …\n",
            "> [Outro] So dance with the fear, let courage lead, Together we’ll conquer, together we’ll succeed. Take the leap, and you’ll see, You’ll find your strength, and you’ll be free.\n",
            "\n",
            "\n",
            "Wow, this was very nice! It’s a little corny as it’s chock full of motivational decrees. And, once again, it rhymed all the couplets, which is fine for the song. I noticed the last couplet in the bridge didn’t exactly rhyme.\n",
            "> The journey’s long, but you’ll  find , You’re stronger than you  realized .\n",
            "\n",
            "\n",
            "So I asked GPT-4 to see if it could fix it.\n",
            "> RobG: Rewrite the last two lines of the bridge so the words rhyme.\n",
            "> GPT-4:  The journey’s long, but keep in  mind , You’re stronger than the fears you’ll  find .\n",
            "\n",
            "\n",
            "That was a nice, creative fix. If you don’t like something it wrote, give it some constructive feedback; it usually does an excellent job of making positive changes. This aligns with my view of using AI systems as a writing partner.\n",
            "Question:         How did Robert use GPT-4 to create song lyrics in the style of the ska band, The Agents?\n",
            "GPT-4 Answer:     he started by having GPT-4 analyze the lyrics and chords of the song “Grow” by The Agents. He then gave GPT-4 a new theme about overcoming fears and asked it to create five possible song titles with descriptions. After selecting the title \"Dance with the Fear\", he asked GPT-4 to write the lyrics for the new song. When he noticed a couplet that didn't rhyme, he asked GPT-4 to rewrite it, which it did successfully. This process demonstrated how he used GPT-4 as a writing partner in creating song lyrics.\n",
            "LLaMa 7B Answer:  he used GPT-4 to analyze the lyrics and chords of The Agents' song \"Grow\" and then generated five possible song titles with descriptions. He then asked GPT-4 to write the lyrics for a new song in the style of The Agents called \"Dance with the Fear.\" GPT-4 produced a ska-style song with a catchy chorus and motivational lyrics, which he found to be excellent and creative.\n",
            "LLaMa 7B Score:   0.8161067\n",
            "\n",
            "Index:            94\n",
            "Reference:        Overview\n",
            "Below is a diagram of the components and processes I used to train and run Deep Haiku. After a quick discussion of what each part does, I’ll get into things in more detail.\n",
            "I started by downloading two datasets of Haikus from Kaggle.com from users hjhalani30 and bfbarry. The datasets were released under the CC0 and CC-BY licenses, respectively. The number of Haikus in the combined datasets is over 140K. I used FastPunct to add punctuation and casing to the Haikus and ran the KeyBERT model [3] to extract phrases used as prompts.\n",
            "I then filtered the data by using the GRUEN metric to gauge the quality of the text [4] and the phoenemizer library [5] to both count the syllables and convert the prompts and Haikus into phonemes. The filtering yielded over 26K relatively high-quality Haikus that all have a [5, 7, 5] meter.\n",
            "I used the GPT-J 6B model from Eluther [6] as the basis for Deep Haiku. After quantizing the model down to 8-bits to run on a Google Colab, I fine-tuned it using the filtered Haikus as training data for ten epochs, which took 11 hours on Google Colab with a Tesla V100 GPU.\n",
            "Generating new Haikus starts with selecting a word or phrase as a prompt, like “autumn.” I use the fine-tuned model to create 20 candidate Haikus. The results are filtered for adherence to the meter and optionally filtered to remove candidates that contain explicit language using the Detoxify library. Yes, Deep Haiku knows how to swear. The remaining candidates are then displayed along with the scores.\n",
            "For example, I generated 20 Haikus generated with the prompt “autumn,” and 11 used the [5, 7, 5] meter. Here are the filtered results with the scores.\n",
            "OK, the top 3 look pretty good, albeit a tad corny. But none of them used swear words, so the toxicity was near zero for all. The only slight blip was the sixth one that mentioned the “end of days” and “the cusp of death.” A little dark for a Haiku about autumn, but not exactly offensive.\n",
            "Question:         How did Robert A. Gonsalves train the Deep Haiku model?\n",
            "GPT-4 Answer:     he started by downloading two datasets of Haikus from Kaggle.com. He then used FastPunct to add punctuation and casing to the Haikus and ran the KeyBERT model to extract phrases used as prompts. He filtered the data using the GRUEN metric and the phoenemizer library to count the syllables and convert the prompts and Haikus into phonemes. This resulted in over 26K high-quality Haikus with a [5, 7, 5] meter. He then used the GPT-J 6B model from Eluther as the basis for Deep Haiku and fine-tuned it using the filtered Haikus as training data for ten epochs. This process took 11 hours on Google Colab with a Tesla V100 GPU.\n",
            "LLaMa 7B Answer:  he trained the Deep Haiku model by fine-tuning a pre-trained GPT-J model on a dataset of 26,000 high-quality Haikus that adhere to the [5, 7, 5] meter. He used the filtered data for ten epochs and quantized the model down to 8-bits before training it on Google Colab with a Tesla V100 GPU.\n",
            "LLaMa 7B Score:   0.9036467\n",
            "\n",
            "Index:            95\n",
            "Reference:        Background\n",
            "For the last five months, I have been looking at the ways Artificial Intelligence (AI) and Machine Learning (ML) can be used for creative endeavors. For example, I showed how Generative Adversarial Networks (GANs) can be used to create new abstract paintings,  here . And I showed how Transformer models can be used to generate new plot summaries for novels,  here .\n",
            "Although the state-of-the-art AI models can generate excellent pictures and words, so far, AI models have not been as good when it comes to composing songs. Some of the music generating models are OK, like the various  Magenta  models from Google [3], the  MuseNet  model from OpenAI [4], and the commercial offering from  AIVA  [5]. But the output from most music generating AI models is often rambling and incoherent. What seems to be lacking is an overall structure for the songs. Let’s see if we can remedy this with an infusion of lyrical poetry.\n",
            "Question:         What was Robert A. Gonsalves' critique of current AI models for generating music?\n",
            "GPT-4 Answer:     he noted that while AI models have shown proficiency in generating images and text, their performance in composing music has been less impressive. He pointed out that the output from most music generating AI models, including Google's Magenta, OpenAI's MuseNet, and AIVA, often lacks coherence and structure. He suggested that this issue could potentially be addressed by incorporating lyrical poetry into the process.\n",
            "LLaMa 7B Answer:  According to he, current AI models for generating music are often incoherent and lack an overall structure. He believes that incorporating lyrical poetry can help improve the quality of the generated music by providing a framework for the songs.\n",
            "LLaMa 7B Score:   0.8743146\n",
            "\n",
            "Index:            96\n",
            "Reference:        Next Steps\n",
            "Although the results are pretty good, there is definitely room for improvement. For example, the choices for background images seem somewhat limited, especially for pop culture. This may be due to the fact that I am restricting the search to use only freely licensed photos. I don’t know if a US court has weighed in yet on whether or not the background images in memes can be deemed to be fair use or not, so I’ll leave that question to the lawyers.\n",
            "The developers behind GPT-Neo at EleutherAI are continuing to build and train bigger language models. Their next model is called GPT-NeoX. They say their “primary goal is to train an equivalent model to the full-sized GPT⁠-⁠3 and make it available to the public under an open licence.” [17]\n",
            "Don’t forget to check out more generated memes in the appendix below.\n",
            "Question:         What is the next model that the developers behind GPT-Neo at EleutherAI are planning to build?\n",
            "GPT-4 Answer:     The developers at EleutherAI are planning to build and train a new model called GPT-NeoX. Their primary goal with this model is to train an equivalent to the full-sized GPT-3 and make it available to the public under an open license.\n",
            "LLaMa 7B Answer:  According to the text, the next model that the developers behind GPT-Neo at EleutherAI are planning to build is called GPT-NeoX.\n",
            "LLaMa 7B Score:   0.745667\n",
            "\n",
            "Index:            97\n",
            "Reference:        GPT-3 Da Vinci\n",
            "OpenAI’s GPT-3 Da Vinci is currently the largest AI model for Natural Language Processing [4]. I am using their latest “zero-shot” style of prompting with their new Da Vinci Instruct model. Instead of providing examples of what you are asking the model to do, you can just simply ask it what to do directly.\n",
            "Here is the prompt that creates a caption for the apple pie picture.\n",
            "```\n",
            "Create a funny caption for a new meme about apple pie. The background picture is Simple and easy apple pie served with vanilla ice cream, on a gingham tablecloth in Lysekil, Sweden.\n",
            "```\n",
            "I pass the prompt into the call to OpenAI along with some additional parameters. Here’s the Python code.\n",
            "```\n",
            "import openai response = openai.Completion.create(   engine=\"davinci-instruct-beta\",   prompt=prompt,   max_tokens=64,   temperature=0.7,   top_p=0.5,   frequency_penalty=0.5,   presence_penalty=0.5,   best_of=1)\n",
            "```\n",
            "The  max_token  parameter indicates how long the response should be. The  temperature  and  top_p  parameters are similar in that they indicate the amount of variety in the response. The  frequency_penalty  and  presence_penalty  are also similar in that they control how often there are new deviations and new topics in the response. If you want to know what all these parameters do, check out my article from last month,  here .\n",
            "Before I show examples of the output from GPT-3, here is the legal disclaimer that OpenAI suggests that I show, which is all true.\n",
            "The author generated the following text in part with GPT-3, OpenAI’s large-scale language-generation model. Upon generating draft language, the author reviewed and revised the language to their own liking and takes ultimate responsibility for the content of this publication.\n",
            "Running the code 10 times will yield the following results, at a total cost of $0.03. Note that I formatted the text to be in uppercase.\n",
            "```\n",
            "1: THIS IS THE PERFECT WAY TO END A DAY OF APPLE PICKING  2: NO, IT'S NOT THAT EASY  3: I'LL TAKE THE ONE WITH THE VANILLA ICE CREAM, PLEASE  4: APPLE PIE IS THE BEST!  5: THIS APPLE PIE IS SO GOOD, I CAN'T EVEN!  6: YOU'RE NOT THE ONLY ONE WHO LOVES APPLE PIE  7: IF YOU CAN'T FIND THE RECIPE, JUST GOOGLE IT  8: THE PIE IS GOOD, BUT IT'S NOT AS GOOD AS MY MOM'S  9: I'LL HAVE A SLICE OF THAT APPLE PIE, PLEASE 10: WE'RE GOING TO NEED A BIGGER PIE\n",
            "```\n",
            "OK, these are pretty good. One thing I learned is that  GTP-3 Da Vinci can be funny!  For example, caption number 2 seems to refer to the “easy as pie” idiom.\n",
            "Note that GPT-3, like all AI models trained on a large corpus of text, will reflect societal biases. Occasionally the system will produce text that may be inappropriate or offensive. OpenAI has a feature to label generated text with one of three warning levels: 0 - the text is safe, 1 - this text is sensitive, or 2 - this text is unsafe. My code will show a warning for any of the generated captions that are flagged as sensitive or unsafe.\n",
            "Question:         How does Robert A. Gonsalves use GPT-3 Da Vinci to generate funny captions for memes?\n",
            "GPT-4 Answer:     he uses GPT-3 Da Vinci by creating a prompt that describes the image for which he wants to generate a caption. He then passes this prompt into a call to OpenAI along with additional parameters that control the length and variety of the response. After running the code, he gets a list of potential captions. He found that GPT-3 Da Vinci can indeed generate humorous captions, such as one that cleverly refers to the idiom \"easy as pie\".\n",
            "LLaMa 7B Answer:  he uses OpenAI's GPT-3 Da Vinci to generate funny captions for memes by providing a prompt image and running it through the model with various parameters to generate a response. He then formats the output in uppercase and runs the code multiple times to get different results, which he displays as examples of what the model can generate. Gonsalves notes that while the model can be funny, it may also reflect societal biases and occasionally produce offensive or inappropriate content, so he includes warning labels for any flagged as sensitive or unsafe.\n",
            "LLaMa 7B Score:   0.7836466\n",
            "\n",
            "Index:            98\n",
            "Reference:        Summary\n",
            "In this article, I gave an overview of Benford’s Law, with some background and history. I explained normal distributions and logarithms as an insight into understanding lognormal distributions. With some theoretical dice rolling exercises, I showed how multiple, independent variables can lead to normal distributions (with addition) and lognormal distributions (with multiplication). I then showed how some datasets with lognormal distributions can lead to conforming with BL. Finally, I walked through three examples of real datasets (city/town populations, accounts payable, and lengths of rivers) to show how datasets with a lognormal distribution will tend to adhere to BL.\n",
            "Question:         What are the three real datasets that Robert A. Gonsalves used to demonstrate the adherence to Benford's Law?\n",
            "GPT-4 Answer:     he used the datasets of city/town populations, accounts payable, and lengths of rivers to demonstrate how datasets with a lognormal distribution will tend to adhere to Benford's Law.\n",
            "LLaMa 7B Answer:  According to the article, the three real datasets used by he to demonstrate the adherence to Benford's Law are:\n",
            "1. City or town populations\n",
            "2. Accounts payable\n",
            "3. Lengths of rivers\n",
            "LLaMa 7B Score:   0.8113049\n",
            "\n",
            "Index:            99\n",
            "Reference:        Muybridge Derby\n",
            "For this project, I wanted to see if I could use AI systems to transform Muybridge’s animal locomotion photographs into high-resolution, full-color videos. After experimenting with various techniques, I changed the original sequences to be more realistic using a combination of  Midjourney  to create reference frames from text prompts and RunwayML’s Gen-1 Video Generator. For fun, I made a short animation, “Muybridge Derby,” showcasing the work. Here it is.\n",
            "In the following sections, I will describe how I transformed the locomotion sequences, generated the background scroll, and combined the elements to create the animation.\n",
            "Question:         What techniques did Robert A. Gonsalves use to transform Muybridge’s animal locomotion photographs into high-resolution, full-color videos?\n",
            "GPT-4 Answer:     He used a combination of Midjourney to create reference frames from text prompts and RunwayML’s Gen-1 Video Generator to transform the original sequences into more realistic ones. He also generated a background scroll and combined the elements to create an animation called \"Muybridge Derby\".\n",
            "LLaMa 7B Answer:  To transform Eadweard Muybridge's animal locomotion photographs into high-resolution, full-color videos, he used a combination of Midjourney to create reference frames from text prompts and RunwayML's Gen-1 Video Generator.\n",
            "LLaMa 7B Score:   0.7121075\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1D array\n",
        "print(\"num scores:  \", len(scores))\n",
        "arr = np.array(scores)\n",
        "\n",
        "# Calculate the average\n",
        "average = np.mean(arr)\n",
        "median = np.median(arr)\n",
        "low = np.min(arr)\n",
        "high = np.max(arr)\n",
        "\n",
        "# Print the results\n",
        "print(\"low score:   \", round(low*100, 3))\n",
        "print(\"mean score:  \", round(average*100, 3))\n",
        "print(\"median score:\", round(median*100, 3))\n",
        "print(\"high score:  \", round(high*100, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "icLbg5Cx1k0G",
        "outputId": "c4a096d1-1a66-49cd-a9b7-87686b8b527c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num scores:   100\n",
            "low score:    54.242\n",
            "mean score:   85.496\n",
            "median score: 86.783\n",
            "high score:   98.826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open a file in write mode. If the file doesn't exist, it will be created.\n",
        "with open('llama7b.txt', 'w') as file:\n",
        "    # Iterate through the array\n",
        "    for item in scores:\n",
        "        # Write each item to the file, converting it to a string and adding a newline character\n",
        "        file.write(str(item) + '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "X228Cv9wFnO4",
        "outputId": "25b121d3-7031-49c8-fc89-13e915b802cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "18aa79e035394ed6a02bf1495af70f40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa50de51c9a748ed9c0ad06ceaf9d1f0",
              "IPY_MODEL_23667d3c4b3e45a088f438187c9cf475",
              "IPY_MODEL_9ade56e15d0a4bdea3c67d76f58138b7"
            ],
            "layout": "IPY_MODEL_1ea3c8e7bfe74fdda587a2222ec636de"
          }
        },
        "aa50de51c9a748ed9c0ad06ceaf9d1f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf71056733544616b9698cdea60d59c2",
            "placeholder": "​",
            "style": "IPY_MODEL_1253dd0a6a0e4183b84cc94e2ef57460",
            "value": ""
          }
        },
        "23667d3c4b3e45a088f438187c9cf475": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26b603b2f8924c2597b80cac035bb1b6",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a8569d8179a4a3186e3bb64e883f41a",
            "value": 0
          }
        },
        "9ade56e15d0a4bdea3c67d76f58138b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f970bcb570b645108162fa5a7e529719",
            "placeholder": "​",
            "style": "IPY_MODEL_18ad08a749714e96bd96fc985e5ed979",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "1ea3c8e7bfe74fdda587a2222ec636de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf71056733544616b9698cdea60d59c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1253dd0a6a0e4183b84cc94e2ef57460": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26b603b2f8924c2597b80cac035bb1b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "5a8569d8179a4a3186e3bb64e883f41a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f970bcb570b645108162fa5a7e529719": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18ad08a749714e96bd96fc985e5ed979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2646f1a26e884ec9a84d77654ba4466e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_228727e7af5442a286b6134bc97a324b",
              "IPY_MODEL_b60a4abadb434a0280aa070a4f76bdf7",
              "IPY_MODEL_f0a1797a87a7439b8c25b299f94bdefd"
            ],
            "layout": "IPY_MODEL_5788beb2f23e418b8e52c17dd1ca948b"
          }
        },
        "228727e7af5442a286b6134bc97a324b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_314a4e5e6536433f853ab6016904aa59",
            "placeholder": "​",
            "style": "IPY_MODEL_3e20db0deb9847db9bd9fb7df511f9a8",
            "value": "Downloading (…)a8e1d/.gitattributes: 100%"
          }
        },
        "b60a4abadb434a0280aa070a4f76bdf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dd3e7569d4a422a9ce1c099fb5cc20c",
            "max": 1175,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_32578e9dcf874f39b64ae540b10c8aea",
            "value": 1175
          }
        },
        "f0a1797a87a7439b8c25b299f94bdefd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23a8910a6a904448b7632e540fe2ea0a",
            "placeholder": "​",
            "style": "IPY_MODEL_f2cce73487624739be1fb3e94154d4fb",
            "value": " 1.18k/1.18k [00:00&lt;00:00, 70.2kB/s]"
          }
        },
        "5788beb2f23e418b8e52c17dd1ca948b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "314a4e5e6536433f853ab6016904aa59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e20db0deb9847db9bd9fb7df511f9a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9dd3e7569d4a422a9ce1c099fb5cc20c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32578e9dcf874f39b64ae540b10c8aea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "23a8910a6a904448b7632e540fe2ea0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2cce73487624739be1fb3e94154d4fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8394faa9610c4c1d86c655e20cf5bcce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3328faa4a63c42a19507514b88526a04",
              "IPY_MODEL_d416b8e6f3144a85b2e6d14c1fb322ae",
              "IPY_MODEL_9789807537654eceb1b635e377ed88a6"
            ],
            "layout": "IPY_MODEL_564afd75da6c466690c77a2b261f703d"
          }
        },
        "3328faa4a63c42a19507514b88526a04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_971382a0a7d74caab23b51f7f06b3672",
            "placeholder": "​",
            "style": "IPY_MODEL_cc98f15114c545d78a1593ca2fd107d7",
            "value": "Downloading (…)_Pooling/config.json: 100%"
          }
        },
        "d416b8e6f3144a85b2e6d14c1fb322ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bae940999df459cb653959281f60de6",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd26864a885249eea5ca142e7b2429ca",
            "value": 190
          }
        },
        "9789807537654eceb1b635e377ed88a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8444896b86f14a10b50633846067408d",
            "placeholder": "​",
            "style": "IPY_MODEL_ddae6ac9e6784466b9fdda444ba48619",
            "value": " 190/190 [00:00&lt;00:00, 11.2kB/s]"
          }
        },
        "564afd75da6c466690c77a2b261f703d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "971382a0a7d74caab23b51f7f06b3672": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc98f15114c545d78a1593ca2fd107d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6bae940999df459cb653959281f60de6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd26864a885249eea5ca142e7b2429ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8444896b86f14a10b50633846067408d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddae6ac9e6784466b9fdda444ba48619": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "121e6ecec11f4f4db1ab33ec86f6c80c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8673a299a95d461fa85e6ea73f4bd507",
              "IPY_MODEL_dcd52422685f49b6983462d25dd99314",
              "IPY_MODEL_a17c07ddf9894cc0996fcbd92ded9e30"
            ],
            "layout": "IPY_MODEL_c337a799cc1d41778abec5ac3689974e"
          }
        },
        "8673a299a95d461fa85e6ea73f4bd507": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef86dd18fefb485aa9dd8b26a95f0ba9",
            "placeholder": "​",
            "style": "IPY_MODEL_2f561d8eed9d4cfe85037ed8accccd86",
            "value": "Downloading (…)b20bca8e1d/README.md: 100%"
          }
        },
        "dcd52422685f49b6983462d25dd99314": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0298ecc334bd41b684aac9e6d193a536",
            "max": 10571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b9fd552619a4b86819d21e0a06a5329",
            "value": 10571
          }
        },
        "a17c07ddf9894cc0996fcbd92ded9e30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b104eef802e4576940c5cfc6f8f5175",
            "placeholder": "​",
            "style": "IPY_MODEL_fcc4809117c24b01bcfaa4b2c0b85977",
            "value": " 10.6k/10.6k [00:00&lt;00:00, 884kB/s]"
          }
        },
        "c337a799cc1d41778abec5ac3689974e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef86dd18fefb485aa9dd8b26a95f0ba9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f561d8eed9d4cfe85037ed8accccd86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0298ecc334bd41b684aac9e6d193a536": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b9fd552619a4b86819d21e0a06a5329": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7b104eef802e4576940c5cfc6f8f5175": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcc4809117c24b01bcfaa4b2c0b85977": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d788f4adbafa43df9ba5d82d5852fc95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4467e5ea588474291d643bea578a339",
              "IPY_MODEL_a4b753e832244ff5824fb3e0a131429d",
              "IPY_MODEL_f231fdaa91c6457cab81b30aad255c91"
            ],
            "layout": "IPY_MODEL_ab92845745794431918e2cce7e803445"
          }
        },
        "e4467e5ea588474291d643bea578a339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a0e4266dd4545feae15e7d185456350",
            "placeholder": "​",
            "style": "IPY_MODEL_b446f69333bf43169cc1d93ce624d01f",
            "value": "Downloading (…)0bca8e1d/config.json: 100%"
          }
        },
        "a4b753e832244ff5824fb3e0a131429d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1cc418270184f618f8e239d508db47a",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_37ea1661b6614f5f9d5d34d580141ba6",
            "value": 571
          }
        },
        "f231fdaa91c6457cab81b30aad255c91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40ff3b85ba0548bb9dde7449524b8882",
            "placeholder": "​",
            "style": "IPY_MODEL_8950c1b83c46435492ec14dd9742f18f",
            "value": " 571/571 [00:00&lt;00:00, 37.1kB/s]"
          }
        },
        "ab92845745794431918e2cce7e803445": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a0e4266dd4545feae15e7d185456350": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b446f69333bf43169cc1d93ce624d01f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1cc418270184f618f8e239d508db47a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37ea1661b6614f5f9d5d34d580141ba6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40ff3b85ba0548bb9dde7449524b8882": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8950c1b83c46435492ec14dd9742f18f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9339f8e9997745bfb0771f07d3f45928": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7dd36c8f6e334beab25a16cfa20cd43e",
              "IPY_MODEL_70d2018559f64d9f9f1143421c5a95d2",
              "IPY_MODEL_064450998e0a45c1818ca7f5aa2cffd9"
            ],
            "layout": "IPY_MODEL_4ac2dfba62064a029dea956dc3a23823"
          }
        },
        "7dd36c8f6e334beab25a16cfa20cd43e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e21c951f1a348a8a999105ab4544336",
            "placeholder": "​",
            "style": "IPY_MODEL_59a426bb30504570afc800a1d5d05b4a",
            "value": "Downloading (…)ce_transformers.json: 100%"
          }
        },
        "70d2018559f64d9f9f1143421c5a95d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be7c727dbced434b8eb5cf0677896863",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc27b4d01a554d04912b6a3a552e947b",
            "value": 116
          }
        },
        "064450998e0a45c1818ca7f5aa2cffd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd335d01efce4a2580914ee4f346dd79",
            "placeholder": "​",
            "style": "IPY_MODEL_36eb469f17a44423aaefb25d7d85604d",
            "value": " 116/116 [00:00&lt;00:00, 7.42kB/s]"
          }
        },
        "4ac2dfba62064a029dea956dc3a23823": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e21c951f1a348a8a999105ab4544336": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59a426bb30504570afc800a1d5d05b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be7c727dbced434b8eb5cf0677896863": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc27b4d01a554d04912b6a3a552e947b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd335d01efce4a2580914ee4f346dd79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36eb469f17a44423aaefb25d7d85604d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76badc2cd3ea441ebaf248d1a8c323d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_16ffe0dc078a48e38dcbf76e7cb50af8",
              "IPY_MODEL_4b8b2019bd784fe090e56131fd14a5e0",
              "IPY_MODEL_e7aedfebfa044b4692ae35c5164d9752"
            ],
            "layout": "IPY_MODEL_95d8024639674025835bed02297f89c9"
          }
        },
        "16ffe0dc078a48e38dcbf76e7cb50af8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ff4e0d2879b48d7a4409710ce07a4c4",
            "placeholder": "​",
            "style": "IPY_MODEL_e5b3bc78d51b4f6da8d0cbfc2ce8316d",
            "value": "Downloading (…)e1d/data_config.json: 100%"
          }
        },
        "4b8b2019bd784fe090e56131fd14a5e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77b9623eb98649eaaf7663331da9d4d2",
            "max": 39265,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c9220e96668c46b6b6fcd163b60a611b",
            "value": 39265
          }
        },
        "e7aedfebfa044b4692ae35c5164d9752": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07c66ca7efca4f02a01e527054c49378",
            "placeholder": "​",
            "style": "IPY_MODEL_4e1f757218f845858e7a906bc8f8e244",
            "value": " 39.3k/39.3k [00:00&lt;00:00, 708kB/s]"
          }
        },
        "95d8024639674025835bed02297f89c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ff4e0d2879b48d7a4409710ce07a4c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5b3bc78d51b4f6da8d0cbfc2ce8316d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77b9623eb98649eaaf7663331da9d4d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9220e96668c46b6b6fcd163b60a611b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "07c66ca7efca4f02a01e527054c49378": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e1f757218f845858e7a906bc8f8e244": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06f4e981d3ec490994cddeb023485a2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_79315e00a6bd4109baf82800aa62eafa",
              "IPY_MODEL_09e4f6103bd645f5af4619371fa99f54",
              "IPY_MODEL_250b4236805d4b1184f603c9dd801cb6"
            ],
            "layout": "IPY_MODEL_2927aca5cb8c4842bef1656cbd5b30a0"
          }
        },
        "79315e00a6bd4109baf82800aa62eafa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f352088dc043497086cc6f08db487147",
            "placeholder": "​",
            "style": "IPY_MODEL_99ba9c12d63f4c20bad039803995a44d",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "09e4f6103bd645f5af4619371fa99f54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e1dd3519b214c5a80a41dbca5448ee2",
            "max": 438011953,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ccdf40ad7716471c85501bc6213b034a",
            "value": 438011953
          }
        },
        "250b4236805d4b1184f603c9dd801cb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_199e9c72f6304eddb89e2a41dcac081f",
            "placeholder": "​",
            "style": "IPY_MODEL_8b03aee580cc4a4bba9ee4a2bac359f7",
            "value": " 438M/438M [00:02&lt;00:00, 240MB/s]"
          }
        },
        "2927aca5cb8c4842bef1656cbd5b30a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f352088dc043497086cc6f08db487147": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99ba9c12d63f4c20bad039803995a44d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e1dd3519b214c5a80a41dbca5448ee2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccdf40ad7716471c85501bc6213b034a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "199e9c72f6304eddb89e2a41dcac081f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b03aee580cc4a4bba9ee4a2bac359f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a204233baedc4bad8da14ce97f818ee9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4497493313d24126a74caadab8660692",
              "IPY_MODEL_3bfbad53318e4f39b267e3ab4a3a526a",
              "IPY_MODEL_a4f51e96c5284e0fa24742a95fec6783"
            ],
            "layout": "IPY_MODEL_e87f391cabac459faf868edd5bfef2fa"
          }
        },
        "4497493313d24126a74caadab8660692": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_442da98e48594db0802ee0bd0153e2a5",
            "placeholder": "​",
            "style": "IPY_MODEL_360bce965d254495a6bcf569ff25d11a",
            "value": "Downloading (…)nce_bert_config.json: 100%"
          }
        },
        "3bfbad53318e4f39b267e3ab4a3a526a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f7f1018766f4d6a88e36868cce99745",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a9055068c68439baeb00d106e3c8ed9",
            "value": 53
          }
        },
        "a4f51e96c5284e0fa24742a95fec6783": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a816aecd895e4f10ade2376c249afdb7",
            "placeholder": "​",
            "style": "IPY_MODEL_b807cc70552e4ec99eb05c410f1f052e",
            "value": " 53.0/53.0 [00:00&lt;00:00, 3.11kB/s]"
          }
        },
        "e87f391cabac459faf868edd5bfef2fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "442da98e48594db0802ee0bd0153e2a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "360bce965d254495a6bcf569ff25d11a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f7f1018766f4d6a88e36868cce99745": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a9055068c68439baeb00d106e3c8ed9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a816aecd895e4f10ade2376c249afdb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b807cc70552e4ec99eb05c410f1f052e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5dd56d32324d43938ce4d8a86125d187": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ee18070fcca4808b04240265dfc0cb4",
              "IPY_MODEL_eaafa2ef8ba34b7f8262a463af514fcb",
              "IPY_MODEL_8cf91382e1b843138e2c7c20bff5af4b"
            ],
            "layout": "IPY_MODEL_ab33fca1f3ea4352b103fb72a9fde8ad"
          }
        },
        "3ee18070fcca4808b04240265dfc0cb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_445955f383c241cc9157e7d558e6b543",
            "placeholder": "​",
            "style": "IPY_MODEL_cc95b85e83df429bbe8f9090aaaf7418",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "eaafa2ef8ba34b7f8262a463af514fcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1885f8522e2423d90652fd460fe7171",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d1964eb0bb2c4813874a7cbf6cfd494a",
            "value": 239
          }
        },
        "8cf91382e1b843138e2c7c20bff5af4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9a5c2d6fca94a598be3e6ca95946037",
            "placeholder": "​",
            "style": "IPY_MODEL_4172edf447af4b778310ac9e340af584",
            "value": " 239/239 [00:00&lt;00:00, 14.6kB/s]"
          }
        },
        "ab33fca1f3ea4352b103fb72a9fde8ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "445955f383c241cc9157e7d558e6b543": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc95b85e83df429bbe8f9090aaaf7418": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1885f8522e2423d90652fd460fe7171": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1964eb0bb2c4813874a7cbf6cfd494a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c9a5c2d6fca94a598be3e6ca95946037": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4172edf447af4b778310ac9e340af584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4860a4f8752341ad968065f327fdc63b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_93e1de09068044c686dffed4d88d7bd2",
              "IPY_MODEL_3393747e8a7147d9b3221e0ebb8156e6",
              "IPY_MODEL_b3255f9dc9bd4c50b81bb785bce1227a"
            ],
            "layout": "IPY_MODEL_2b4ce1faeba94ca181e49e6fecadd61c"
          }
        },
        "93e1de09068044c686dffed4d88d7bd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adc3e57d08194502955c1ac6ed32b13d",
            "placeholder": "​",
            "style": "IPY_MODEL_66a5924f9c4245eb8d7c238ca255697f",
            "value": "Downloading (…)a8e1d/tokenizer.json: 100%"
          }
        },
        "3393747e8a7147d9b3221e0ebb8156e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59b7d7eebeb04bf8a9dec61f98c3cad4",
            "max": 466021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d8e8224b33a48139afb9af479cc620c",
            "value": 466021
          }
        },
        "b3255f9dc9bd4c50b81bb785bce1227a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7667c88014fd486789248577eafd5ea9",
            "placeholder": "​",
            "style": "IPY_MODEL_f4b81bf5c3d34c4083467bfd4e248f04",
            "value": " 466k/466k [00:00&lt;00:00, 2.18MB/s]"
          }
        },
        "2b4ce1faeba94ca181e49e6fecadd61c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adc3e57d08194502955c1ac6ed32b13d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66a5924f9c4245eb8d7c238ca255697f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59b7d7eebeb04bf8a9dec61f98c3cad4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d8e8224b33a48139afb9af479cc620c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7667c88014fd486789248577eafd5ea9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4b81bf5c3d34c4083467bfd4e248f04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2139ed216284f35a2c7700eedaf65dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bd15572cffc24692885576b37a95d083",
              "IPY_MODEL_482659744ec74f719bcf4e4d257a57ca",
              "IPY_MODEL_7f8a54443da7440db261baf2f3dd8821"
            ],
            "layout": "IPY_MODEL_8d79b70b41b1461998b20de45139bb2c"
          }
        },
        "bd15572cffc24692885576b37a95d083": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b625706302d44dba9b21f782957224f",
            "placeholder": "​",
            "style": "IPY_MODEL_09ae58f4420a4622ac2be94150920b29",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "482659744ec74f719bcf4e4d257a57ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bb0a1d4bfed4b6d91c90930903a9942",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9af40f06f3244faa822e294cc731ef67",
            "value": 363
          }
        },
        "7f8a54443da7440db261baf2f3dd8821": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45dd93dff20e4cf4a6e03847b2d793d6",
            "placeholder": "​",
            "style": "IPY_MODEL_68ffe6cb41674c3a8412c43e13099e68",
            "value": " 363/363 [00:00&lt;00:00, 13.7kB/s]"
          }
        },
        "8d79b70b41b1461998b20de45139bb2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b625706302d44dba9b21f782957224f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09ae58f4420a4622ac2be94150920b29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9bb0a1d4bfed4b6d91c90930903a9942": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9af40f06f3244faa822e294cc731ef67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "45dd93dff20e4cf4a6e03847b2d793d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68ffe6cb41674c3a8412c43e13099e68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74f0b1ff34614168880a80b410be7383": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a24c031c9a474f518b558a69b09c586e",
              "IPY_MODEL_55b1bc0f4abf40e3890f80bbcfa340dd",
              "IPY_MODEL_c061c36222b143a48cfcc43a98e77415"
            ],
            "layout": "IPY_MODEL_b4ac0021d24e44349c395629ec8a3bb5"
          }
        },
        "a24c031c9a474f518b558a69b09c586e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2ddaf1515674b828718046eb6c90bb4",
            "placeholder": "​",
            "style": "IPY_MODEL_0ac27448f4b444a8a36e61c4cfd6ae2f",
            "value": "Downloading (…)8e1d/train_script.py: 100%"
          }
        },
        "55b1bc0f4abf40e3890f80bbcfa340dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62ee171c43ce47e08ad9d6284fc08bd5",
            "max": 13123,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c44e5854a3ca478ebe69263e9f6982b6",
            "value": 13123
          }
        },
        "c061c36222b143a48cfcc43a98e77415": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5fce33bacd24bbf958bfd5af405bc55",
            "placeholder": "​",
            "style": "IPY_MODEL_ed7b3f67f53a452bbf6fd142bba0c752",
            "value": " 13.1k/13.1k [00:00&lt;00:00, 728kB/s]"
          }
        },
        "b4ac0021d24e44349c395629ec8a3bb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2ddaf1515674b828718046eb6c90bb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ac27448f4b444a8a36e61c4cfd6ae2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62ee171c43ce47e08ad9d6284fc08bd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c44e5854a3ca478ebe69263e9f6982b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c5fce33bacd24bbf958bfd5af405bc55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed7b3f67f53a452bbf6fd142bba0c752": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8333d38965f04c3db3f856d0b537a798": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b5cd96afe7848278609ae7cde7d41b8",
              "IPY_MODEL_1030cbbb4b1d48c88ecd13ae3f068953",
              "IPY_MODEL_920cd1098d5845728b8fa51c16b1e7c6"
            ],
            "layout": "IPY_MODEL_474c16e53f7a40d38317f5895eec89dd"
          }
        },
        "3b5cd96afe7848278609ae7cde7d41b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a5cbe105b77461085f349fccced43da",
            "placeholder": "​",
            "style": "IPY_MODEL_c14f991b2c3e4d44a0ff9bb65de9488d",
            "value": "Downloading (…)b20bca8e1d/vocab.txt: 100%"
          }
        },
        "1030cbbb4b1d48c88ecd13ae3f068953": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9343e38c0186419f9c9920967dd2ebfc",
            "max": 231536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81e6920a1e1c4ff8b87444bc39abab9a",
            "value": 231536
          }
        },
        "920cd1098d5845728b8fa51c16b1e7c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d62bd788d5b40bf9db938094e0cd7e8",
            "placeholder": "​",
            "style": "IPY_MODEL_ea279f516fb544f49766aa5710340541",
            "value": " 232k/232k [00:00&lt;00:00, 12.4MB/s]"
          }
        },
        "474c16e53f7a40d38317f5895eec89dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a5cbe105b77461085f349fccced43da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c14f991b2c3e4d44a0ff9bb65de9488d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9343e38c0186419f9c9920967dd2ebfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81e6920a1e1c4ff8b87444bc39abab9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d62bd788d5b40bf9db938094e0cd7e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea279f516fb544f49766aa5710340541": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9bacb81cddcf4224967b1e24c10773ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_08814d44e15b4620b29fa00f9922e70b",
              "IPY_MODEL_4c3d70d83b63475d84c33dd7899d3c59",
              "IPY_MODEL_9911e8ae0baa4ded849f0c1d7a827382"
            ],
            "layout": "IPY_MODEL_577bab97d13b4511af4b5a3487f28e76"
          }
        },
        "08814d44e15b4620b29fa00f9922e70b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b345d1c45e2642429823758efc6f9d0f",
            "placeholder": "​",
            "style": "IPY_MODEL_79ccd0d69a4e4a5bb1c4885a07612b25",
            "value": "Downloading (…)bca8e1d/modules.json: 100%"
          }
        },
        "4c3d70d83b63475d84c33dd7899d3c59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d1b3ac6d86c4dd5855c9ca29f1d83da",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0a6b6221f5074efdb9a9301c7d9b8e72",
            "value": 349
          }
        },
        "9911e8ae0baa4ded849f0c1d7a827382": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c2f99cd9033498bba40111478e4d61e",
            "placeholder": "​",
            "style": "IPY_MODEL_3e9f7c8ecf8246b6a1d184d3dfb2f19e",
            "value": " 349/349 [00:00&lt;00:00, 28.1kB/s]"
          }
        },
        "577bab97d13b4511af4b5a3487f28e76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b345d1c45e2642429823758efc6f9d0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79ccd0d69a4e4a5bb1c4885a07612b25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d1b3ac6d86c4dd5855c9ca29f1d83da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a6b6221f5074efdb9a9301c7d9b8e72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1c2f99cd9033498bba40111478e4d61e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e9f7c8ecf8246b6a1d184d3dfb2f19e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
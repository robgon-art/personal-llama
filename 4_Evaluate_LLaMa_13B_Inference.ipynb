{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robgon-art/personal-llama/blob/main/4_Evaluate_LLaMa_13B_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkBmY3vQvRSw"
      },
      "outputs": [],
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install git+https://github.com/robgon-art/llama-cpp-python.git --force-reinstall --upgrade --no-cache-dir --verbose\n",
        "!pip install llama_index huggingface_hub sentence-transformers\n",
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.25.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 14j_8FcDCJEt5-LPCwXDuU8Yrjpr7nTAp"
      ],
      "metadata": {
        "id": "qx81W006yuu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a882b681-b81a-4b54-c8a5-c71319b8808a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=14j_8FcDCJEt5-LPCwXDuU8Yrjpr7nTAp\n",
            "To: /content/robgon_qa.csv\n",
            "\r  0% 0.00/192k [00:00<?, ?B/s]\r100% 192k/192k [00:00<00:00, 2.61MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oI-kXwg5bHF-"
      },
      "outputs": [],
      "source": [
        "model_size = \"13B\"\n",
        "\n",
        "if model_size == \"7B\":\n",
        "  model_name_or_path = \"TheBloke/Llama-2-7B-chat-GGUF\"\n",
        "  model_basename = \"llama-2-7b-chat.Q4_K_M.gguf\"\n",
        "elif model_size == \"13B\":\n",
        "  model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGUF\"\n",
        "  model_basename = \"llama-2-13b-chat.Q4_K_M.gguf\"\n",
        "else:\n",
        "  print(\"Invalid model size\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBEJr-G-2ht4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "cea6c1d254ef4e10803517b296874b8b",
            "fe918089c4e94d4ab0f11023b0492088",
            "52e3129f73df4a04853c7e249bd37d5b",
            "80729121c72b4cb69b6e4832feed9577",
            "20266d3386664c638700fb6faeab09fc",
            "0aa36da94cc34075a2a3af587098115a",
            "8930ca9806da4168b9deee9a1360018b",
            "ff366d875501425b9a3ff4e412438a28",
            "79244cdaf5534e74a3d5b6a74227318a",
            "bd20b37cb7f34f7b802c78e4bd00fd73",
            "05695cc3188f4034b90da531144b6066"
          ]
        },
        "outputId": "80b1e284-54e9-45a1-eb9a-e5d7f2009021"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)13b-chat.Q4_K_M.gguf:   0%|          | 0.00/7.87G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cea6c1d254ef4e10803517b296874b8b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap words in text output in Colab\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "rs72UX3j5fKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms import LlamaCPP\n",
        "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "0Z-zNGVa018P",
        "outputId": "a29d457a-bc96-42f2-d577-ad381e50fc6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LlamaCPP(\n",
        "    # You can pass in the URL to a GGUF model to download it automatically\n",
        "    model_url=None,\n",
        "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
        "    model_path=model_path,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=512, # 256\n",
        "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
        "    context_window=3900,\n",
        "    # kwargs to pass to __call__()\n",
        "    generate_kwargs={},\n",
        "    # kwargs to pass to __init__()\n",
        "    # set to at least 1 to use GPU\n",
        "    model_kwargs={\"n_gpu_layers\": 43},\n",
        "    # transform inputs into Llama2 format\n",
        "    messages_to_prompt=messages_to_prompt,\n",
        "    completion_to_prompt=completion_to_prompt,\n",
        "    verbose=True, # True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "yjupsjrI0w1h",
        "outputId": "f8c4700d-3314-46eb-9c9e-2860c7875db6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# try to answer without any references\n",
        "response = llm.complete(\"What is Muybridge Derby?\")\n",
        "print(response.text.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "4XULr8jV2CBc",
        "outputId": "82b84476-d91b-4a77-abe4-6b9677932d7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Muybridge Derby was a series of photographs taken by Eadweard Muybridge in 1878 at the Palo Alto Stock Farm in California. The photographs were taken using a sequence of cameras placed along a track, and they captured the motion of a horse named \"Debris\" as it galloped at various speeds. The resulting images were published as a book called \"Animal Locomotion\" and are considered to be some of the earliest examples of motion photography. The Muybridge Derby is still studied today by photographers, artists, and scientists interested in the study of movement and locomotion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load the model\n",
        "encoder = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
      ],
      "metadata": {
        "id": "lIHRjg-iy_TA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532,
          "referenced_widgets": [
            "a9e9dc2863dc415fa0085dc5c422df54",
            "e5e79c2ae190441581d06e4df998c20a",
            "59444e563b484b99bdcefa97a4412ca4",
            "845abf2255fb49918de8d347782b6775",
            "b19c417fe5f149da83a13bcca19ffbc6",
            "afcd442f13014345804c9331bbd6f216",
            "8f13098e1e5f4d1eb5fb701d8d8a7f52",
            "ad58349ece294470b9d3cdf2c39408f7",
            "253d0aadea184e7da9819c9a848e42b3",
            "8977c81f9eaa47d8932326fabfa21b92",
            "cf88e3309a4448f99bcff0753fd1c9f2",
            "270e784d24e34c12af085f61f9053d38",
            "0f704bbf4bf04c45b67f922bc936ce2e",
            "d9c6351653ca453d8125400ee34db2d4",
            "de15222453944bd4816399cc512a12ba",
            "b173ad919324474b942cbf35151936d9",
            "63bc98fead394861b79f7f653e4d4414",
            "bb649dc488874bf281b1b1c9f418682c",
            "288817ebff81486285caa9488f6d4acc",
            "23781d3efcce410c8eabb17c7e6d184f",
            "27fcf31df8e84c82a988ae1b5b9f9972",
            "dea125856926421a93ed53d5a30ae56a",
            "3427d152dd3e4884904aabde1dcee08e",
            "be3f2361b4f844d78fd91eb09fecb6c1",
            "b7fc96a7a34441e58f5a085722de3da2",
            "5d57956a94f54f5a939b61a3db6102c9",
            "31bd768f939145e6a2a75ba61693b9b5",
            "09aa31bb9f3949ac95cb1bb3dea9a603",
            "f94af0ea3e324301b6ed18f49fc9c519",
            "ffe549000b85416f8b88cb0c8b7af4b8",
            "bf71683b38b047bb8bcb5ea4320dabc9",
            "66a77e5f967e44bdada77d491404cfb0",
            "f2b1065e063a489fbb967e9d65e37d93",
            "87453a64429f4e468c19411994a2d66b",
            "2522aebef7774880903d325a859aa49d",
            "15118c31a60d409c878716deaeaeca36",
            "9209c2f7fedc43658460a5d34fa87d6b",
            "80c1662c8d0141a7bc722823860f088f",
            "e4b0ef91213d46258de66216ea065b1b",
            "d5ba60fb40e14e0ba19c132f15354712",
            "6524dbb4ce6549718e4b8f2eecd150d6",
            "4611f9f3cda84cd3af5a09bc9a1190b4",
            "fef967fd6cf14e3f9f0c84409a1fb4af",
            "d7be602ccb374729abf3bf8fd4243a45",
            "e399e77970c64870aedc18699322a6d5",
            "df9d23e48f30469e8158c1b5f5d8b0ed",
            "08ca3fa7c67d48d294a3cd959980cb62",
            "36c52596aa304784b8fe25e7790cbab6",
            "46f1b5f0c0f94c4fb76ba6c1ce782fc2",
            "a41e67bdbc144eeb9ed9d49989819a78",
            "e1b483c31a934026aca9d9977e64c32a",
            "5c925829769e4501b97b73661d779861",
            "16d960f0a89f4f438c681ef5619f730c",
            "2511ac746d5e49b58d29ffceba724c73",
            "3ea46c0420f74de589665aa5828302a6",
            "ac62733d7ccd4fe79d1ca7d86674eabf",
            "e982767557794cc899ea33615805ae7b",
            "fde691ccb934402b99e19fdb28a8c729",
            "c8463f337d574dbdab02a282fe139215",
            "7bbe4603cf97485bbd145d520c97f2d9",
            "737022f8e4e5426b8486093ed0c5333e",
            "54218b5ced14455b9ad3be6d23a1dd08",
            "5f32155027fd4e19af5420880afb4e83",
            "b197008dac784ef997a0fd9c49de1588",
            "3cf2d68e8c8e4c96b5470b44d317453a",
            "37676e6f0d4d4831b6ebc2ee4311aed9",
            "a1dc3f4f30c946d0b00035b6cee4c0b7",
            "3eecb8a34bac48fa8af35a24596fdb8d",
            "44bbc0009b884c89a353798cbc0ec344",
            "f837a8309223437a84524b71286499b9",
            "19139d2f9b574d189df07b50e0c5f6cf",
            "f086124daa0b434f885bfd4bc2e930eb",
            "1f4dbaa12345428a91f6a5b42e4a0cdb",
            "acb5edd2090b48969b3db535d6e301b4",
            "bf5ad61ee3df49b0af338ba1c26a3b68",
            "e9c2e73e6b644c4191571ccc7a2e7a1b",
            "e37ba421721e44f298297aa8ea88154f",
            "818d2483789f4f07acce6c0c51075caa",
            "c676e59a4e2348b6a1ce9c5d3658da40",
            "5cc5175f6dc54aae84881c9a8693edb5",
            "6994c3018b8d42388a613d63de387b82",
            "64ad20911d4f4476a86d5f9ddd1f8bb4",
            "be4b72925a5f498eb801078471e862c4",
            "342b4eefdca54fc9a521f27cc8e0d777",
            "31f03cf100784fe5b3698cb84f8750d5",
            "7129eac3cafb4b8187ed0f4827ddfa9b",
            "82aa19eb7f8a421690f35da12100a927",
            "be391e8a783f4e528938bade5c2a6265",
            "e0e4ebe16ff34156b8fa03c3bffa61e5",
            "badf0a144d0346d7b0da5ff525f131c4",
            "60228f24d1064fe2a3f7ece75a89f966",
            "e75d9fbfa0314fb7a065aa4f5c260348",
            "bb5032b6b3254a01a648fb62a8409384",
            "bb9b36e854b948c2930151b3e678dfe9",
            "6249608e46ef49ac9ccacc27682a8106",
            "a7819fa779dd441ab5ba0afdb835858f",
            "a34d6da001a9487da7e6357965e606aa",
            "2db203f5e6f84c4b98c51d12ef06eae8",
            "3b5dcb11752b49bf945dca57daf37175",
            "ccc271dfce374ab884fc22c3b8256194",
            "b796db6ada00431e9ed7b4d69f348b80",
            "c0833d375767424086f63b99586b3227",
            "0d8d94738b6a446393df4f7c6a5e53f3",
            "d967873a0f104e04a3deb8247cc5edb5",
            "fdb893dabefa45a7bfedc2a8d7ff28ac",
            "af681a758d80486783ea91856c3b978b",
            "873047ec5ce84cef9757add8f4a4856d",
            "bf5e0e94482c47e89031ccd1a2c95f3d",
            "9ba0faaa1fc541c0a83346ccb171a9f1",
            "a2182c8503df4cb4bb08cc3af603f8de",
            "56c21671faeb4c4b8235a49a09a952c2",
            "c437d535f7b24a04976ae6ea8a112ef5",
            "3b496c96dd5243b2b431ab26b0ce4797",
            "7676c91eb7ac4b749daee2085d6f86bd",
            "14807dcf9bb0457995bf082518f3f705",
            "0344fb77882e4de49141a4064c080f5b",
            "fe331585eef348b3bb8d4a69d3676f33",
            "524c8e0ebc9249e6ac21e4988498b4fe",
            "59f7c44ac00744028e03a3c7a980bfb3",
            "094eed91d4734f3d86fdfcb1e519d3eb",
            "d8fd04a4edac4f77b6bba8b4a3e0f268",
            "6acf9ca07b82469cbbd309b0cb7f0195",
            "9b7a74a18845463b92a5b0292c779ce4",
            "5ac29eaf5fbd4cdf92827c02b19933cf",
            "471ca1bc22de4e40b487455e2e73c5c5",
            "85db9cea6ff94f5cb726d61c15d67381",
            "3b4d22f1075e4a738aaa4a1e2ce781f1",
            "146c39915ef1419bae133fd4a8803b48",
            "ce59d89c982f4c97b82ff525c19d16a4",
            "9f070fe7e55d4581ad8e93d0f714386c",
            "d11d56bfc0ac45569dabc256c210c593",
            "1dc8941cb0ab4daeb961be9c8b498922",
            "5805d6bdb0ab4b8ca10fb6e703e91d36",
            "f57dee2e04b243af9d011703920dab34",
            "e3537454453b4e6c8d0242334d8c26b4",
            "867bacb56c584624bf7747cd8019ddb4",
            "14a44d11093c44239b9df011b1bf3592",
            "ca8e45c2430744c9a32551aae1f65288",
            "c32d8a205f31403dab6cbcba38b2e1db",
            "c83b3473bdd74691ac696154a1c58351",
            "f4afbac96f364586953b9d7a06863b6c",
            "3e1d9184e6024565a3e71fce16cef87e",
            "e858806a270b4a8b8932e86f2558bd82",
            "e5930616e365487484b2424193ab0353",
            "32209ecf314f4dada59e827dd129d430",
            "e35c48ca3f644545bee50d3bce9e06cf",
            "1ad7e64ac2424d67bfd3adb47af4dae5",
            "cb6029a64f894e649a90e9a35a3030f3",
            "2d4c1c864e814549ae46557fc35ee818",
            "95d93e30e1054247afd4a47d26d37f6b",
            "24f7bc0e5bf64dc0bbae61aa5f0c3947",
            "c3b4f16a4f114bf3b31ac7fe295490f3",
            "6f58fc94f28d4088b570d3f567de0a1b",
            "7dae5bb0a12849c98f6477f4e104a390",
            "4f14f28654b94a41b43a33d4b9fe4218",
            "e521e20182964a8f8502f2c10844589f",
            "65bb8ef822224ee5b77dce0c6413d31c",
            "19581b937060454ab63fc3f586cfd973",
            "8427ee0c5b164d6e93c478de370fdb3d",
            "e88dc64cefde437d8a39cdb3b3471744",
            "ee388547457f4dbca1ffdd5f9f7ae03a",
            "3f6d30877c384f04acb2a916b7fb5aa9",
            "e982cd9ab7174b8d875f0f70966993dc",
            "b8b3e3eb96a340b2a23e36f26c33ea6b",
            "121d60ce4e89423a9d889205e84a1634"
          ]
        },
        "outputId": "fc0eaffb-ea2c-41cb-93b8-2520178ac197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9e9dc2863dc415fa0085dc5c422df54"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)a8e1d/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "270e784d24e34c12af085f61f9053d38"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3427d152dd3e4884904aabde1dcee08e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)b20bca8e1d/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87453a64429f4e468c19411994a2d66b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)0bca8e1d/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e399e77970c64870aedc18699322a6d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac62733d7ccd4fe79d1ca7d86674eabf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)e1d/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1dc3f4f30c946d0b00035b6cee4c0b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "818d2483789f4f07acce6c0c51075caa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0e4ebe16ff34156b8fa03c3bffa61e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ccc271dfce374ab884fc22c3b8256194"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)a8e1d/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56c21671faeb4c4b8235a49a09a952c2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6acf9ca07b82469cbbd309b0cb7f0195"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)8e1d/train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5805d6bdb0ab4b8ca10fb6e703e91d36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)b20bca8e1d/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5930616e365487484b2424193ab0353"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)bca8e1d/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f14f28654b94a41b43a33d4b9fe4218"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "sentences = [\"This is an example sentence\", \"This is another example sentence\"]\n",
        "embeddings = encoder.encode(sentences)\n",
        "print(embeddings)\n",
        "\n",
        "# Normalize the embeddings\n",
        "text_features_1 = embeddings[0] / np.linalg.norm(embeddings[0], axis=-1, keepdims=True)\n",
        "text_features_2 = embeddings[1] / np.linalg.norm(embeddings[1], axis=-1, keepdims=True)\n",
        "\n",
        "# Calculate the cosine similarity\n",
        "similarity = np.dot(text_features_1, text_features_2.T)\n",
        "print(similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "qeW__Ll5rcCr",
        "outputId": "b43d5b7c-597f-4254-b63f-e125a0e14e51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.02250259 -0.07829171 -0.02303071 ... -0.0082793   0.02652686\n",
            "  -0.00201896]\n",
            " [ 0.05012981 -0.03327598 -0.01251665 ... -0.01040575  0.02814883\n",
            "  -0.01429978]]\n",
            "0.90200233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv(\"/content/robgon_qa.csv\")\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "r-5kHzurrvWr",
        "outputId": "b416bdfc-7a81-40a9-9084-060094a89de0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                         doc_metadata  \\\n",
              "0   {'title': 'Writing Songs with GPT-4: Part 2, C...   \n",
              "1   {'title': '#Hands-on Tutorials', 'subtitle': '...   \n",
              "2   {'title': 'Using AI to Create New Comic Strips...   \n",
              "3   {'title': 'Using AI to Create New Comic Strips...   \n",
              "4   {'title': 'BIG.art: Using Machine Learning to ...   \n",
              "..                                                ...   \n",
              "95  {'title': 'Frost Songs: Using AI to Generate M...   \n",
              "96  {'title': 'AI-Memer: Using Machine Learning to...   \n",
              "97  {'title': 'AI-Memer: Using Machine Learning to...   \n",
              "98  {'title': 'Benford’s Law — A Simple Explanatio...   \n",
              "99  {'title': 'Muybridge Derby: Bringing Animal Lo...   \n",
              "\n",
              "                                          doc_content  \\\n",
              "0   Source Code\\nThe source code for this project ...   \n",
              "1   Using CLIP to Filter the Images for Training\\n...   \n",
              "2   Final Thoughts\\nComparing the two systems, I f...   \n",
              "3   Mark Madness\\nFor the Mark Madness comic, I us...   \n",
              "4   Source Code\\nThe  source code  for this projec...   \n",
              "..                                                ...   \n",
              "95  Background\\nFor the last five months, I have b...   \n",
              "96  Next Steps\\nAlthough the results are pretty go...   \n",
              "97  GPT-3 Da Vinci\\nOpenAI’s GPT-3 Da Vinci is cur...   \n",
              "98  Summary\\nIn this article, I gave an overview o...   \n",
              "99  Muybridge Derby\\nFor this project, I wanted to...   \n",
              "\n",
              "                                             question  \\\n",
              "0   What is the main objective of Robert A. Gonsal...   \n",
              "1   How did Robert use CLIP to filter the images f...   \n",
              "2   What are the limitations Robert A. Gonsalves f...   \n",
              "3   How did Robert A. Gonsalves modify the images ...   \n",
              "4   What machine learning tools does Robert A. Gon...   \n",
              "..                                                ...   \n",
              "95  What was Robert A. Gonsalves' critique of curr...   \n",
              "96  What is the next model that the developers beh...   \n",
              "97  How does Robert A. Gonsalves use GPT-3 Da Vinc...   \n",
              "98  What are the three real datasets that Robert A...   \n",
              "99  What techniques did Robert A. Gonsalves use to...   \n",
              "\n",
              "                                               answer  \n",
              "0   The main objective of his project 'GPT-4 Chord...  \n",
              "1   Robert used CLIP to filter the images by compa...  \n",
              "2   Robert found that while DALL-E could generate ...  \n",
              "3   Robert cleaned up the images in Photoshop, add...  \n",
              "4   He uses GLIDE and BSRGAN to create these high-...  \n",
              "..                                                ...  \n",
              "95  Robert A. Gonsalves noted that while AI models...  \n",
              "96  The developers at EleutherAI are planning to b...  \n",
              "97  Robert A. Gonsalves uses GPT-3 Da Vinci by cre...  \n",
              "98  Robert A. Gonsalves used the datasets of city/...  \n",
              "99  He used a combination of Midjourney to create ...  \n",
              "\n",
              "[100 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a6a18f86-265e-4d12-8054-5cd11f5ca7bd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc_metadata</th>\n",
              "      <th>doc_content</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{'title': 'Writing Songs with GPT-4: Part 2, C...</td>\n",
              "      <td>Source Code\\nThe source code for this project ...</td>\n",
              "      <td>What is the main objective of Robert A. Gonsal...</td>\n",
              "      <td>The main objective of his project 'GPT-4 Chord...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{'title': '#Hands-on Tutorials', 'subtitle': '...</td>\n",
              "      <td>Using CLIP to Filter the Images for Training\\n...</td>\n",
              "      <td>How did Robert use CLIP to filter the images f...</td>\n",
              "      <td>Robert used CLIP to filter the images by compa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>{'title': 'Using AI to Create New Comic Strips...</td>\n",
              "      <td>Final Thoughts\\nComparing the two systems, I f...</td>\n",
              "      <td>What are the limitations Robert A. Gonsalves f...</td>\n",
              "      <td>Robert found that while DALL-E could generate ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>{'title': 'Using AI to Create New Comic Strips...</td>\n",
              "      <td>Mark Madness\\nFor the Mark Madness comic, I us...</td>\n",
              "      <td>How did Robert A. Gonsalves modify the images ...</td>\n",
              "      <td>Robert cleaned up the images in Photoshop, add...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>{'title': 'BIG.art: Using Machine Learning to ...</td>\n",
              "      <td>Source Code\\nThe  source code  for this projec...</td>\n",
              "      <td>What machine learning tools does Robert A. Gon...</td>\n",
              "      <td>He uses GLIDE and BSRGAN to create these high-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>{'title': 'Frost Songs: Using AI to Generate M...</td>\n",
              "      <td>Background\\nFor the last five months, I have b...</td>\n",
              "      <td>What was Robert A. Gonsalves' critique of curr...</td>\n",
              "      <td>Robert A. Gonsalves noted that while AI models...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>{'title': 'AI-Memer: Using Machine Learning to...</td>\n",
              "      <td>Next Steps\\nAlthough the results are pretty go...</td>\n",
              "      <td>What is the next model that the developers beh...</td>\n",
              "      <td>The developers at EleutherAI are planning to b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>{'title': 'AI-Memer: Using Machine Learning to...</td>\n",
              "      <td>GPT-3 Da Vinci\\nOpenAI’s GPT-3 Da Vinci is cur...</td>\n",
              "      <td>How does Robert A. Gonsalves use GPT-3 Da Vinc...</td>\n",
              "      <td>Robert A. Gonsalves uses GPT-3 Da Vinci by cre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>{'title': 'Benford’s Law — A Simple Explanatio...</td>\n",
              "      <td>Summary\\nIn this article, I gave an overview o...</td>\n",
              "      <td>What are the three real datasets that Robert A...</td>\n",
              "      <td>Robert A. Gonsalves used the datasets of city/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>{'title': 'Muybridge Derby: Bringing Animal Lo...</td>\n",
              "      <td>Muybridge Derby\\nFor this project, I wanted to...</td>\n",
              "      <td>What techniques did Robert A. Gonsalves use to...</td>\n",
              "      <td>He used a combination of Midjourney to create ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a6a18f86-265e-4d12-8054-5cd11f5ca7bd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a6a18f86-265e-4d12-8054-5cd11f5ca7bd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a6a18f86-265e-4d12-8054-5cd11f5ca7bd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1d7e736e-d3ff-4c6d-a884-73a997d7d2fa\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1d7e736e-d3ff-4c6d-a884-73a997d7d2fa')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1d7e736e-d3ff-4c6d-a884-73a997d7d2fa button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_b56157a1-41f8-4848-934b-45880f6f0d32\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b56157a1-41f8-4848-934b-45880f6f0d32 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the first five rows\n",
        "for index, row in df.iloc[:5].iterrows():\n",
        "  print(f\"Row {index+1}:\")\n",
        "  print(f\"doc_metadata: {row['doc_metadata']}\")\n",
        "  print(f\"doc_content: {row['doc_content']}\")\n",
        "  print(f\"question: {row['question']}\")\n",
        "  print(f\"answer: {row['answer']}\\n\")\n",
        "  print(\"************************************\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g3fZYL1xsT6A",
        "outputId": "4dfdb826-1c10-4e7b-9c64-a29e498832e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 1:\n",
            "doc_metadata: {'title': 'Writing Songs with GPT-4: Part 2, Chords', 'subtitle': 'How to use the latest large language model from OpenAI to help compose chords for original songs', 'author': 'Robert A. Gonsalves', 'date': 'GPT-4 Chords - https://medium.com/towards-data-science/writing-songs-with-gpt-4-part-2-chords-173cfda0e5a1', 'nickname': 'GPT-4 Chords', 'url': 'https://medium.com/towards-data-science/writing-songs-with-gpt-4-part-2-chords-173cfda0e5a1'}\n",
            "doc_content: Source Code\n",
            "The source code for this project is available on  GitHub .\n",
            "question: What is the main objective of Robert A. Gonsalves' project 'GPT-4 Chords'?\n",
            "answer: The main objective of his project 'GPT-4 Chords' is to demonstrate how to use the latest large language model from OpenAI, GPT-4, to help compose chords for original songs.\n",
            "\n",
            "************************************\n",
            "Row 2:\n",
            "doc_metadata: {'title': '#Hands-on Tutorials', 'subtitle': '# MAGNet: Modern Art Generator using Deep Neural Networks', 'author': '## How I used CLIP, SWAGAN, and a custom genetic algorithm to create modern paintings from text descriptions', 'date': 'MAGnet - https://medium.com/towards-data-science/magnet-modern-art-generator-using-deep-neural-networks-57537457bb7', 'nickname': 'MAGnet', 'url': 'https://medium.com/towards-data-science/magnet-modern-art-generator-using-deep-neural-networks-57537457bb7'}\n",
            "doc_content: Using CLIP to Filter the Images for Training\n",
            "After cropping the images, I ended up having over 12,000 paintings to work with. That’s enough to train a GAN, but not all of the paintings are good. Just because a painter is tagged on WikiArt as being “modern” doesn’t mean that all of their works are good examples of modern painting. I used CLIP to filter the images as I did in my GANscapes project to winnow down the dataset.\n",
            "OpenAI designed their two models for CLIP, an image and text encoder, to perform semantic searches. They trained the two models on a dataset of images with corresponding phrases. The goal of the models is to have the encoded images match the encoded phrases.\n",
            "Once trained, the image encoder system converts images to embeddings, lists of 512 floating-point numbers that capture the images’ general features. The text encoder converts a text phrase to similar embedding that can be compared to image embeddings for a semantic search.\n",
            "For MAGnet, I compare the embedding from the phrase “modern painting” to the embeddings from the paintings to find the top 10,000 images that best match the phrase. The source code is in the Colab  here .\n",
            "Here are some examples that didn’t make the cut as modern paintings. The first two probably scored lower because they don't have a lot of contrast. The third seems more like a drawing than a painting.\n",
            "question: How did Robert use CLIP to filter the images for the MAGnet project?\n",
            "answer: Robert used CLIP to filter the images by comparing the embedding from the phrase \"modern painting\" to the embeddings from the paintings. This allowed him to find the top 10,000 images that best matched the phrase \"modern painting\". This was done to ensure that the images used for training were good examples of modern painting.\n",
            "\n",
            "************************************\n",
            "Row 3:\n",
            "doc_metadata: {'title': 'Using AI to Create New Comic Strips without Writing Any Code', 'subtitle': 'A tutorial on how to use GPT-3 and DALL-E to generate original content for the funny pages', 'author': 'Robert A. Gonsalves', 'date': 'AI Comics - https://medium.com/towards-data-science/using-ai-to-create-new-comic-strips-without-writing-any-code-cc669bb317a7', 'nickname': 'AI Comics', 'url': 'https://medium.com/towards-data-science/using-ai-to-create-new-comic-strips-without-writing-any-code-cc669bb317a7'}\n",
            "doc_content: Final Thoughts\n",
            "Comparing the two systems, I found that GPT-3 does a much better job generating the text than DALL-E does for the artwork. Although the initial images for the comics look OK as concept renderings, they need some cleanup to be used in production. But the main problem is DALL-E doesn’t generate characters consistently for the comic strip. The primary limitation stems from the 1024x1024 image size in DALL-E. Note that OpenAI  recently added  a new “outpainting” feature to more easily add to generated images. However, the new feature only works in a piecemeal fashion. It only ever considers a 1024x1024 frame for image generation. The system would need an “upload reference frames” feature to generate visual elements consistently, like comic strip characters.\n",
            "question: What are the limitations Robert A. Gonsalves found in using DALL-E for generating artwork for comic strips?\n",
            "answer: Robert found that while DALL-E could generate concept renderings for the comic strips, the images often required cleanup before they could be used in production. He also noted that DALL-E struggled with generating characters consistently across the comic strip. This was primarily due to the system's limitation of only considering a 1024x1024 frame for image generation. Even with the new \"outpainting\" feature, which allows for easier addition to generated images, the system still operates in a piecemeal fashion. He suggested that the system would benefit from an \"upload reference frames\" feature to generate visual elements, like comic strip characters, more consistently.\n",
            "\n",
            "************************************\n",
            "Row 4:\n",
            "doc_metadata: {'title': 'Using AI to Create New Comic Strips without Writing Any Code', 'subtitle': 'A tutorial on how to use GPT-3 and DALL-E to generate original content for the funny pages', 'author': 'Robert A. Gonsalves', 'date': 'AI Comics - https://medium.com/towards-data-science/using-ai-to-create-new-comic-strips-without-writing-any-code-cc669bb317a7', 'nickname': 'AI Comics', 'url': 'https://medium.com/towards-data-science/using-ai-to-create-new-comic-strips-without-writing-any-code-cc669bb317a7'}\n",
            "doc_content: Mark Madness\n",
            "For the Mark Madness comic, I used this prompt, “A comic set in a basketball court with two characters wearing a red uniform, Mark, a talented college basketball player, and Enrique, an egotistical teammate.” Here are the four images DALL-E created.\n",
            "I liked the second one, but I needed to clean it up a bit in Photoshop. And I added the title and the dialog generated by GPT-3, replacing the gibberish text in the word balloons. Here are the before and after images.\n",
            "This is kinda like one of those visual puzzles where you need to find the ten differences. Can you spot them all? 🙂\n",
            "I like the overall look of the comic, with some lovely details like the red vertical bar that matches the team color. The eyes of the characters, however, do not match at all. I used a little trick in Photoshop to replicate Mark’s right eye and Enrique’s left eye. I used the Comic Sans font for the dialog, of course.\n",
            "question: How did Robert A. Gonsalves modify the images created by DALL-E for the Mark Madness comic?\n",
            "answer: Robert cleaned up the images in Photoshop, added the title, and replaced the gibberish text in the word balloons with dialog generated by GPT-3. He also used a trick in Photoshop to replicate the eyes of the characters Mark and Enrique, and used the Comic Sans font for the dialog.\n",
            "\n",
            "************************************\n",
            "Row 5:\n",
            "doc_metadata: {'title': 'BIG.art: Using Machine Learning to Create High-Res Fine Art', 'subtitle': 'How to use GLIDE and BSRGAN to create ultra-high-resolution digital paintings with fine details', 'author': 'Robert A. Gonsalves', 'date': 'BigArt - https://medium.com/towards-data-science/big-art-using-machine-learning-to-create-high-res-fine-art-7dd695f99788', 'nickname': 'BigArt', 'url': 'https://medium.com/towards-data-science/big-art-using-machine-learning-to-create-high-res-fine-art-7dd695f99788'}\n",
            "doc_content: Source Code\n",
            "The  source code  for this project is available on GitHub. I am releasing the sources under the CC BY-SA license. You can create your own images using this  Google Colab .\n",
            "If you use this code to create new images, please give attribution like this: This image was created with  BIG.art   by  Robert A. Gonsalves .\n",
            "question: What machine learning tools does Robert A. Gonsalves use to create ultra-high-resolution digital paintings?\n",
            "answer: He uses GLIDE and BSRGAN to create these high-resolution digital paintings.\n",
            "\n",
            "************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"Answer the question briefly.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "t15InmsFtOBa",
        "outputId": "4e313348-18ad-4bff-8169-c711b10c5e6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the rows\n",
        "scores = []\n",
        "for index, row in df.iloc[:].iterrows():\n",
        "  reference_answer = row['answer'].replace(\"Robert A. Gonsalves\", \"he\").replace(\"Robert\", \"he\")\n",
        "  prompt =row['doc_metadata'] + \"\\n\" + row['doc_content'] + \"\\n\\n\" + prefix + \"\\n\\n\" + row['question']\n",
        "  print(\"Index:           \", index)\n",
        "  print(\"Reference:       \", row['doc_content'])\n",
        "  print(\"Question:        \", row['question'])\n",
        "  print(\"GPT-4 Answer:    \", reference_answer)\n",
        "  response = llm.complete(prompt)\n",
        "  answer = response.text.strip().replace(\"Robert A. Gonsalves\", \"he\").replace(\"Robert\", \"he\")\n",
        "  print(\"LLaMa 13B Answer:\", answer)\n",
        "\n",
        "  sentences = [reference_answer, answer]\n",
        "  embeddings = encoder.encode(sentences)\n",
        "\n",
        "  # Normalize the embeddings\n",
        "  text_features_1 = embeddings[0] / np.linalg.norm(embeddings[0], axis=-1, keepdims=True)\n",
        "  text_features_2 = embeddings[1] / np.linalg.norm(embeddings[1], axis=-1, keepdims=True)\n",
        "\n",
        "  # Calculate the cosine similarity\n",
        "  similarity = np.dot(text_features_1, text_features_2.T)\n",
        "  scores.append(similarity)\n",
        "  print(\"LLaMa 13B Score: \", similarity)\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lvR_-0NCtxK1",
        "outputId": "10a4cb79-dcb5-41a5-ee90-71e429e1da9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index:            0\n",
            "Reference:        Source Code\n",
            "The source code for this project is available on  GitHub .\n",
            "Question:         What is the main objective of Robert A. Gonsalves' project 'GPT-4 Chords'?\n",
            "GPT-4 Answer:     The main objective of his project 'GPT-4 Chords' is to demonstrate how to use the latest large language model from OpenAI, GPT-4, to help compose chords for original songs.\n",
            "LLaMa 13B Answer: Based on the information provided, the main objective of he' project \"GPT-4 Chords\" is to use the latest large language model from OpenAI to help compose chords for original songs.\n",
            "LLaMa 13B Score:  0.9687352\n",
            "\n",
            "Index:            1\n",
            "Reference:        Using CLIP to Filter the Images for Training\n",
            "After cropping the images, I ended up having over 12,000 paintings to work with. That’s enough to train a GAN, but not all of the paintings are good. Just because a painter is tagged on WikiArt as being “modern” doesn’t mean that all of their works are good examples of modern painting. I used CLIP to filter the images as I did in my GANscapes project to winnow down the dataset.\n",
            "OpenAI designed their two models for CLIP, an image and text encoder, to perform semantic searches. They trained the two models on a dataset of images with corresponding phrases. The goal of the models is to have the encoded images match the encoded phrases.\n",
            "Once trained, the image encoder system converts images to embeddings, lists of 512 floating-point numbers that capture the images’ general features. The text encoder converts a text phrase to similar embedding that can be compared to image embeddings for a semantic search.\n",
            "For MAGnet, I compare the embedding from the phrase “modern painting” to the embeddings from the paintings to find the top 10,000 images that best match the phrase. The source code is in the Colab  here .\n",
            "Here are some examples that didn’t make the cut as modern paintings. The first two probably scored lower because they don't have a lot of contrast. The third seems more like a drawing than a painting.\n",
            "Question:         How did Robert use CLIP to filter the images for the MAGnet project?\n",
            "GPT-4 Answer:     he used CLIP to filter the images by comparing the embedding from the phrase \"modern painting\" to the embeddings from the paintings. This allowed him to find the top 10,000 images that best matched the phrase \"modern painting\". This was done to ensure that the images used for training were good examples of modern painting.\n",
            "LLaMa 13B Answer: Sure! Here is the answer to your question:\n",
            "\n",
            "he used CLIP to filter the images for the MAGnet project by comparing the embeddings from the phrase \"modern painting\" to the embeddings from the paintings to find the top 10,000 images that best match the phrase.\n",
            "LLaMa 13B Score:  0.8377486\n",
            "\n",
            "Index:            2\n",
            "Reference:        Final Thoughts\n",
            "Comparing the two systems, I found that GPT-3 does a much better job generating the text than DALL-E does for the artwork. Although the initial images for the comics look OK as concept renderings, they need some cleanup to be used in production. But the main problem is DALL-E doesn’t generate characters consistently for the comic strip. The primary limitation stems from the 1024x1024 image size in DALL-E. Note that OpenAI  recently added  a new “outpainting” feature to more easily add to generated images. However, the new feature only works in a piecemeal fashion. It only ever considers a 1024x1024 frame for image generation. The system would need an “upload reference frames” feature to generate visual elements consistently, like comic strip characters.\n",
            "Question:         What are the limitations Robert A. Gonsalves found in using DALL-E for generating artwork for comic strips?\n",
            "GPT-4 Answer:     he found that while DALL-E could generate concept renderings for the comic strips, the images often required cleanup before they could be used in production. He also noted that DALL-E struggled with generating characters consistently across the comic strip. This was primarily due to the system's limitation of only considering a 1024x1024 frame for image generation. Even with the new \"outpainting\" feature, which allows for easier addition to generated images, the system still operates in a piecemeal fashion. He suggested that the system would benefit from an \"upload reference frames\" feature to generate visual elements, like comic strip characters, more consistently.\n",
            "LLaMa 13B Answer: Based on the text, he found that DALL-E has limitations in generating consistent characters for comic strip artwork, primarily due to the 1024x1024 image size limit and the lack of an \"upload reference frames\" feature for consistent generation of visual elements.\n",
            "LLaMa 13B Score:  0.84662974\n",
            "\n",
            "Index:            3\n",
            "Reference:        Mark Madness\n",
            "For the Mark Madness comic, I used this prompt, “A comic set in a basketball court with two characters wearing a red uniform, Mark, a talented college basketball player, and Enrique, an egotistical teammate.” Here are the four images DALL-E created.\n",
            "I liked the second one, but I needed to clean it up a bit in Photoshop. And I added the title and the dialog generated by GPT-3, replacing the gibberish text in the word balloons. Here are the before and after images.\n",
            "This is kinda like one of those visual puzzles where you need to find the ten differences. Can you spot them all? 🙂\n",
            "I like the overall look of the comic, with some lovely details like the red vertical bar that matches the team color. The eyes of the characters, however, do not match at all. I used a little trick in Photoshop to replicate Mark’s right eye and Enrique’s left eye. I used the Comic Sans font for the dialog, of course.\n",
            "Question:         How did Robert A. Gonsalves modify the images created by DALL-E for the Mark Madness comic?\n",
            "GPT-4 Answer:     he cleaned up the images in Photoshop, added the title, and replaced the gibberish text in the word balloons with dialog generated by GPT-3. He also used a trick in Photoshop to replicate the eyes of the characters Mark and Enrique, and used the Comic Sans font for the dialog.\n",
            "LLaMa 13B Answer: Based on the information provided, he modified the images created by DALL-E for the Mark Madness comic by:\n",
            "\n",
            "1. Cleaning up the second image in Photoshop.\n",
            "2. Adding a title and dialog generated by GPT-3 to replace gibberish text in the word balloons.\n",
            "3. Using a little trick in Photoshop to replicate Mark's right eye and Enrique's left eye.\n",
            "4. Using the Comic Sans font for the dialog.\n",
            "LLaMa 13B Score:  0.77599216\n",
            "\n",
            "Index:            4\n",
            "Reference:        Source Code\n",
            "The  source code  for this project is available on GitHub. I am releasing the sources under the CC BY-SA license. You can create your own images using this  Google Colab .\n",
            "If you use this code to create new images, please give attribution like this: This image was created with  BIG.art   by  Robert A. Gonsalves .\n",
            "Question:         What machine learning tools does Robert A. Gonsalves use to create ultra-high-resolution digital paintings?\n",
            "GPT-4 Answer:     He uses GLIDE and BSRGAN to create these high-resolution digital paintings.\n",
            "LLaMa 13B Answer: Based on the information provided, he uses GLIDE and BSRGAN to create ultra-high-resolution digital paintings.\n",
            "LLaMa 13B Score:  0.9275242\n",
            "\n",
            "Index:            5\n",
            "Reference:        Interacting with GPT-4\n",
            "Using the example song from my first two articles, I asked GPT-4 to compose a melody one part at a time. Here’s how the first interaction went.\n",
            "> RobG: Write a melody for the song Dogs and Cats in guitar tab format. Play one note for each syllable.\n",
            "> [G] Dogs and cats, they [C] roam the streets [Em] Furry friends with [D] hearts that beat [G] Claws and paws, they [C] rule the land [Em] Tails a-waggin’, [D] they understand\n",
            "> GPT-4: I’m an AI text-based model, so I can’t create or play a sound. But I can give you an example of how you might put these chords to a simple melody using guitar tab format, each note for each syllable.\n",
            "> The basic chords provided are G, C, Em, and D. I’ll use the G major scale as the melody foundation.\n",
            "> Remember, in tab format, the lines represent the strings of a guitar (from low E on bottom to high E on top), and the numbers represent which fret to play on that string. Here’s a simple example:\n",
            "\n",
            "\n",
            "```\n",
            "e|----------------0------------------|-----------------3------------------| B|---------1---3-----3-1---------1--|-------0---1-3-------3-1------------| G|-------0---0-----------2-0--------|-----0---0---------------2-0--------| D|-----2----------------------2------|---2-------------------------------| A|----------------------------------|-----------------------------------| E|----------------------------------|-----------------------------------|      G          C                   Em             D e|----------------0------------------|-----------------3------------------| B|---------1---3-----3-1---------1--|-------0---1-3-------3-1------------| G|-------0---0-----------2-0--------|-----0---0---------------2-0--------| D|-----2----------------------2------|---2-------------------------------| A|----------------------------------|-----------------------------------| E|----------------------------------|-----------------------------------|      G          C                   Em             D\n",
            "```\n",
            "> This is just a very simple example and the melody might not fully capture the spirit of your lyrics, but I hope it helps as a starting point. You may want to adjust the rhythm and melody to suit your song better.\n",
            "\n",
            "\n",
            "This is interesting. It’s a two-part musical phrase that is repeated twice. But on closer inspection, you can see that there are too many syllables in the phrases; it has twelve+eleven when it should be seven+seven. Also, the vertical lines don’t line up. I repeated the process for the second verse and cleaned up the phrases a bit in a text editor to look like this.\n",
            "```\n",
            "e|-------------0-----|-------------------| B|-------1---3---3---|-------0---1-3-----| G|-----0---0---------|-----0---0-----0---| D|---2---------------|---2---------------| A|-------------------|-------------------| E|-------------------|-------------------|      G       C             Em      D e|-------------------|-------------------| B|-------0-3-1-0-----|-------0---3-1-----| G|-----1---------2---|-----0---0-----2---| D|---2---------------|---2---------------| A|-------------------|-------------------| E|-------------------|-------------------|      Am      C           G       D\n",
            "```\n",
            "I wrote some Python code  here  to convert guitar tabs to MIDI files, so I could play them back to hear how they sounded. The code is based on a similar project by Brian Caffey [2]. Here’s the complete score.\n",
            "And here’s what the song sounds like. I used  Band-in-a-Box  by PG Music to arrange the song with piano, guitar, bass, and drums. Feel free to sing along. 😀\n",
            "It’s simple but catchy, kinda like a nursery song. The first note is a bit odd in that it is an E, which would make the G chord effectively a G6. And this occurs with every instance of the G chord, too. I think the best thing about the tune is the variation the third time through with the Am and G. Up next, I’ll show you how I created melodies for a ska song.\n",
            "Question:         How did Robert A. Gonsalves use GPT-4 to create melodies for a song?\n",
            "GPT-4 Answer:     he interacted with GPT-4 by asking it to compose a melody for a song, one part at a time. He provided the lyrics and asked the AI to create a melody in guitar tab format, with one note for each syllable. GPT-4, being a text-based model, provided an example of how the chords could be put to a simple melody using guitar tab format. he then adjusted the melody to suit the rhythm of the song. He also wrote Python code to convert the guitar tabs to MIDI files, so he could hear how the melody sounded.\n",
            "LLaMa 13B Answer: Based on the provided information, he used GPT-4 to create melodies for a song by asking the AI model to compose a melody one part at a time in guitar tab format, with each note representing a syllable in the lyrics. GPT-4 provided a simple example of how the chords could be put to a melody, and he then cleaned up the phrases and converted them to MIDI files to hear how they sounded.\n",
            "LLaMa 13B Score:  0.9096966\n",
            "\n",
            "Index:            6\n",
            "Reference:        Writing Songs with GPT-4: Part 3, Melodies\n",
            "towardsdatascience.com\n",
            "Question:         What is the main focus of Robert A. Gonsalves' article \"Writing Songs with GPT-4: Part 2, Chords\"?\n",
            "GPT-4 Answer:     In his article, he discusses how to use the latest large language model from OpenAI, GPT-4, to help compose chords for original songs.\n",
            "LLaMa 13B Answer: Based on the given information, the main focus of he' article \"Writing Songs with GPT-4: Part 2, Chords\" is using the latest large language model from OpenAI to help compose chords for original songs.\n",
            "LLaMa 13B Score:  0.93538225\n",
            "\n",
            "Index:            7\n",
            "Reference:        StyleCLIP\n",
            "As you may be able to guess from the name, StyleCLIP is a system that uses the CLIP encoders to steer StyleGAN 2 to generate images from a text phrase. In their paper, “StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery,” the authors…\n",
            "> … introduce an optimization scheme that utilizes a CLIPbased loss to modify an input latent vector in response to a user-provided text prompt. — Or Patashnik, et al.\n",
            "\n",
            "\n",
            "StyleCLIP works by iteratively changing the input into Style GAN 2 to steer the generated image to match the text prompt as measured by CLIP.\n",
            "The system starts with the average image defined by the StyleGAN 2 model that’s sent into the CLIP image encoder. The text prompt is sent into the CLIP text encoder, and the two embeddings are compared. The StyleCLIP optimizer then steers the generator to create images that get successively closer to the features described in the prompt. StyleCLIP does this by minimizing the difference between the image embedding and the text embedding with each iteration. The learning rate is set to 10% at this stage, so the change in each iteration can be substantial. After about 40 passes, the image will start to look like what the prompt describes.\n",
            "As mentioned in the overview, the StyleGAN 2 model was trained using faces of real people posted on Flickr. The output will generally look like a real person and not a Halloween baddie, like a witch or a monster. But VQGAN can get us there.\n",
            "Question:         How does Robert A. Gonsalves describe the process of generating images using StyleCLIP in his article \"SpookyGAN - Rendering Scary Faces with Machine Learning\"?\n",
            "GPT-4 Answer:     In his article, he explains that StyleCLIP works by iteratively changing the input into StyleGAN 2 to steer the generated image to match a text prompt as measured by CLIP. The process begins with the average image defined by the StyleGAN 2 model, which is sent into the CLIP image encoder. The text prompt is sent into the CLIP text encoder, and the two embeddings are compared. The StyleCLIP optimizer then steers the generator to create images that get successively closer to the features described in the prompt. This is done by minimizing the difference between the image embedding and the text embedding with each iteration. After about 40 passes, the image will start to look like what the prompt describes.\n",
            "LLaMa 13B Answer: Sure! Here's the answer to your question:\n",
            "\n",
            "According to he' article \"SpookyGAN - Rendering Scary Faces with Machine Learning,\" the process of generating images using StyleCLIP involves iteratively changing the input into Style GAN 2 to steer the generated image to match the text prompt as measured by CLIP, starting with the average image defined by the StyleGAN 2 model and minimizing the difference between the image embedding and the text embedding with each iteration.\n",
            "LLaMa 13B Score:  0.8353466\n",
            "\n",
            "Index:            8\n",
            "Reference:        Future Work\n",
            "Future work could include exploring how the two types of analyses, adherence to lognormal distributions and compliance with Benford’s Law, may be related when the datasets do not closely match the ideals. These tools together may help in determining any underlying reasons for any discrepancies in the data.\n",
            "Question:         What future research does Robert A. Gonsalves suggest in his article \"Benford’s Law — A Simple Explanation\"?\n",
            "GPT-4 Answer:     In his article, he suggests that future research could involve investigating how adherence to lognormal distributions and compliance with Benford’s Law might be related when the datasets do not closely match the ideals. He believes that these tools, used together, could assist in identifying any underlying reasons for discrepancies in the data.\n",
            "LLaMa 13B Answer: Based on the information provided in the article, he suggests future research exploring how the two types of analyses (adherence to lognormal distributions and compliance with Benford's Law) may be related when datasets do not closely match ideals, in order to determine any underlying reasons for discrepancies in the data.\n",
            "LLaMa 13B Score:  0.92693615\n",
            "\n",
            "Index:            9\n",
            "Reference:        Hands-on Tutorials\n",
            "Apr 1, 2021\n",
            "Ganscapes - https://medium.com/towards-data-science/ganscapes-using-ai-to-create-new-impressionist-paintings-d6af1cf94c56\n",
            "This is my third article on experimenting with Generative Adversarial Networks (GANs) to create fine art. The first two articles focused on  creating abstract art  by  using image augmentation , but this one focuses on creating Impressionist landscape paintings.\n",
            "I posted all of the  source code  for GANscapes on GitHub and posted the original paintings on Kaggle. You can create new landscape paintings using the  Google Colab here .\n",
            "Question:         What was the main focus of the Ganscapes project by Robert A. Gonsalves?\n",
            "GPT-4 Answer:     The main focus of the Ganscapes project was to experiment with Generative Adversarial Networks (GANs) to create Impressionist landscape paintings. This was a shift from his previous projects which focused on creating abstract art using image augmentation.\n",
            "LLaMa 13B Answer: The main focus of the Ganscapes project by he was to use Generative Adversarial Networks (GANs) to create new Impressionist landscape paintings.\n",
            "LLaMa 13B Score:  0.9456367\n",
            "\n",
            "Index:            10\n",
            "Reference:        Generative Adversarial Networks\n",
            "In 2014, Ian Goodfellow and seven coauthors at the Université de Montréal presented a  paper  on Generative Adversarial Networks (GANs)[2].  They came up with a way to train two ANNs that effectively compete with each other to create content like photos, songs, prose, and yes, paintings.  The first ANN is called the Generator and the second is called the Discriminator. The Generator is trying to create realistic output, in this case, a color painting. The Discriminator is trying to discern real paintings from the training set as opposed to fake paintings from the generator. Here’s what a GAN architecture looks like.\n",
            "A series of random noise is fed into the Generator, which then uses its trained weights to generate the resultant output, in this case, a color image. The Discriminator is trained by alternating between processing real paintings, with an expected output of 1 and fake paintings, with an expected output of -1. After each painting is sent to the Discriminator, it sends back detailed feedback about why the painting is not real, and the Generator adjusts its weights with this new knowledge to try and do better the next time.  The two networks in the GAN are effectively trained together in an adversarial fashion . The Generator gets better at trying to pass off a fake image as real, and the Discriminator gets better at determining which input is real, and which is fake. Eventually, the Generator gets pretty good at generating realistic-looking images. You can read more about GANs, and the math they use, in Shweta Goyal’s post  here .\n",
            "Question:         How does the Generator in the MachineRay project improve its ability to create realistic images?\n",
            "GPT-4 Answer:     The Generator in the MachineRay project improves its ability to create realistic images through feedback from the Discriminator. After each painting is sent to the Discriminator, it sends back detailed feedback about why the painting is not real. The Generator then adjusts its weights with this new knowledge to try and do better the next time. This adversarial training process continues until the Generator gets pretty good at generating realistic-looking images.\n",
            "LLaMa 13B Answer: Sure! Here's the answer to your question:\n",
            "\n",
            "The Generator in the MachineRay project improves its ability to create realistic images by learning from the feedback it receives from the Discriminator. The Discriminator provides detailed feedback on why a particular painting is not real, and the Generator adjusts its weights accordingly to try and do better the next time. This adversarial process allows the Generator to improve its ability to generate realistic-looking images over time.\n",
            "LLaMa 13B Score:  0.9128968\n",
            "\n",
            "Index:            11\n",
            "Reference:        Contrastive Similarity Test\n",
            "For my  article last month , I researched various metrics for gauging the esthetics of generated art. I discovered a technique using the CLIP model by OpenAI [2] that can be used to estimate esthetic quality. When I compared an image embedding to the embeddings of positive and negative text prompts, like “bad art” and “good art,” I found that using the difference in similarity produced reasonable results. I also calculated a prompt similarity metric to see how well the image matched the prompt using similar logic. The graph below shows the two metrics for the sixteen images above.\n",
            "You can see how the renderings using the newer Midjourney rank higher on both the esthetic quality metric (vertical axis) and prompt similarity (horizontal axis) than the older versions, and V4 stands a cut above the rest. Two notable outliers are v2–1 (unusually good prompt similarity) and v2–3 (unusually bad on both metrics). Scrolling up to look at the images again, these metrics seem to hold up reasonably well. The math and Python for this test are on my GitHub repository  here .\n",
            "Question:         What metrics did Robert use to gauge the esthetics of generated art in his exploration of Midjourney V4?\n",
            "GPT-4 Answer:     In his exploration of Midjourney V4, he used a technique involving the CLIP model by OpenAI to estimate esthetic quality. He compared an image embedding to the embeddings of positive and negative text prompts, such as “bad art” and “good art.” The difference in similarity was used to produce results. He also calculated a prompt similarity metric to see how well the image matched the prompt.\n",
            "LLaMa 13B Answer: Based on the information provided, he used two metrics to gauge the esthetics of generated art in his exploration of Midjourney V4:\n",
            "\n",
            "1. Esthetic quality metric (vertical axis)\n",
            "2. Prompt similarity metric (horizontal axis)\n",
            "LLaMa 13B Score:  0.64237666\n",
            "\n",
            "Index:            12\n",
            "Reference:        Building an AI 8-Ball with RoBERTa\n",
            "Robert A. Gonsalves\n",
            "Nov 1, 2020\n",
            "AI 8-Ball - https://medium.com/towards-data-science/building-an-ai-8-ball-with-roberta-2bfbf6f5519b\n",
            "In this post, I’ll show you how I built an AI 8-Ball that can answer yes/no questions using the power of the Internet and current Machine Learning techniques. I’ll start with a high-level description of the system and then get into the details of the components along with excerpts of the Python code. I’ll show how you can get both a percentage no-to-yes answer and a percentage confidence level as outputs. Finally, I’ll give instructions on how to run the AI 8-Ball to answer your own questions.\n",
            "Question:         What does the AI 8-Ball built by Robert A. Gonsalves do?\n",
            "GPT-4 Answer:     The AI 8-Ball built by him is designed to answer yes/no questions using the power of the Internet and current Machine Learning techniques. It provides both a percentage no-to-yes answer and a percentage confidence level as outputs.\n",
            "LLaMa 13B Answer: The AI 8-Ball built by he answers yes/no questions.\n",
            "LLaMa 13B Score:  0.89716464\n",
            "\n",
            "Index:            13\n",
            "Reference:        Background\n",
            "Natural Language Processing (NLP) is a field of Linguistics and Computer Science that studies how machines, with their computer languages like Java and Python, can communicate with humans, with their natural languages like English and Swahili. One of the first proponents of teaching computers to understand human language was Alan Turing. He wrote about it in 1950 [1].\n",
            "> We may hope that machines will eventually compete with men in all purely intellectual fields. But which are the best ones to start with? Even this is a difficult decision. Many people think that a very abstract activity, like the playing of chess, would be best. It can also be maintained that it is best to provide the machine with the best sense organs that money can buy, and then   teach it to understand and speak English. —  Alan Turing , Mathematician and Computer Scientist\n",
            "Question:         What is the main purpose of the PlotJam project as described by Robert A. Gonsalves?\n",
            "GPT-4 Answer:     The main purpose of the PlotJam project, as described by he, is to use AI, specifically GPT-2, to generate plot summaries for books that don't exist yet. This can be particularly useful for writers who are experiencing writer's block and need some inspiration to get started.\n",
            "LLaMa 13B Answer: Based on the information provided, the main purpose of the PlotJam project, as described by he, is to use AI to create plot summaries of books that don't exist yet.\n",
            "LLaMa 13B Score:  0.9075726\n",
            "\n",
            "Index:            14\n",
            "Reference:        Acknowledgments\n",
            "I would like to thank Jennifer Lim for her help with this project. I also thank Oliver Strimpel, Penny Lennox, and Fez Aswat for reviewing the article and providing helpful feedback.\n",
            "Question:         What was the purpose of the VoxMorphia project by Robert A. Gonsalves?\n",
            "GPT-4 Answer:     The purpose of the VoxMorphia project by he was to use AI, specifically GPT-3, to reimagine classic songs in the style of different artists like Bob Marley, Radiohead, and Megan Thee Stallion.\n",
            "LLaMa 13B Answer: The purpose of the VoxMorphia project by he was to use AI for style transfer of song lyrics, specifically to reimagine classic songs in the style of Bob Marley, Radiohead, and Megan Thee Stallion.\n",
            "LLaMa 13B Score:  0.9253464\n",
            "\n",
            "Index:            15\n",
            "Reference:        From Landscapes to Abstract Paintings\n",
            "I remember learning in my Art History 101 class that many abstract painters started by painting figurative subjects, like people and landscapes. For example here are sequences of paintings by Mondrian, Klee, and Kandinsky that show their progressions from landscape to abstract art.\n",
            "Question:         What inspired Robert A. Gonsalves to create abstract art using StyleGAN2 ADA?\n",
            "GPT-4 Answer:     he was inspired by the progression of many abstract painters who started by painting figurative subjects, like people and landscapes, and then moved to abstract art. He mentioned the sequences of paintings by Mondrian, Klee, and Kandinsky as examples of this progression. This led him to use StyleGAN2 ADA to generate improved abstract paintings with AI.\n",
            "LLaMa 13B Answer: Based on the information provided, it appears that he was inspired to create abstract art using StyleGAN2 ADA after learning about the progression of figurative painters like Mondrian, Klee, and Kandinsky who transitioned to abstract art.\n",
            "LLaMa 13B Score:  0.9262127\n",
            "\n",
            "Index:            16\n",
            "Reference:        Background\n",
            "The origin of the Limerick form is unknown, but it is often attributed to having come from the town of Limerick, Ireland [2][3]. One of the first collections of limericks is “Anecdotes and Adventures of Fifteen Gentlemen” [4] by English illustrator and poet Edward Lear, published in 1821. Here is an example from the book.\n",
            "> A Tailor who sailed from Quebec, In a storm ventur’d once upon deck, But the waves of the sea, Were as strong as could be, And he tumbled in up to his neck. - Edward Lear\n",
            "Question:         What did Robert A. Gonsalves do to improve the speed of his AI training for rhyming?\n",
            "GPT-4 Answer:     To improve the speed of his AI training for rhyming, he upgraded to Pro from Colab, as the latter was slow.\n",
            "LLaMa 13B Answer: Based on the information provided, he upgraded to a Pro version of the AI training software to improve the speed of his training.\n",
            "LLaMa 13B Score:  0.72816026\n",
            "\n",
            "Index:            17\n",
            "Reference:        Prior Work\n",
            "Apparently, I am not the only one looking to get a transformer to generate text with metered prose. For example, in his paper “Haiku Generation, A Transformer Based Approach, With Lots Of Control,” Giacomo Miceli notes that the typical Haiku meter pattern is not strictly followed [1].\n",
            "> Modern and especially English language haikus do not follow the 5–7–5 pattern very strictly, but usually adhere to a short-long-short form of around 10/12 words in total. — Giacomo Miceli\n",
            "\n",
            "\n",
            "Miceli’s Haikoo system creates some excellent prose, too, but only 1 of the 12 examples in the paper follows the [5, 7, 5] pattern. Note that I borrowed the first line to see where Deep Haiku would take it. Here they are.\n",
            "Miceli references another paper that directly addresses the meter of generated prose. It’s not for writing Haikus; it’s for writing limericks. In their paper, “There Once Was a Really Bad Poet, It Was Automated but You Didn’t Know It,” Jianyou Wang et al. discuss their early experiments using OpenAIs GPT-2 model.\n",
            "> A naïve implementation of GPT-2 simply cannot produce original and valid limericks. GPT-2 tends to generate long sentences that exceed the syllable limit for limericks. To meet a syllable constraint, we would need to truncate the generated sentences, which creates lines that do not end correctly. — Jianyou Wang, et al.\n",
            "\n",
            "\n",
            "They go on to describe a custom transformer for generating limericks, LimGen, that chooses words with constraints on part-of-speech, syllable counts, and placement of stressed syllables.\n",
            "For Deep Haiku, I built a system that generates Haikus from a user-specified prompt with adherence to a [5, 7, 5] meter by fine-tuning a general-purpose transformer.\n",
            "Question:         What was the main challenge faced by Jianyou Wang and his team when using GPT-2 for generating limericks?\n",
            "GPT-4 Answer:     The main challenge they faced was that GPT-2 tended to generate long sentences that exceeded the syllable limit for limericks. To meet the syllable constraint, they had to truncate the generated sentences, which resulted in lines that did not end correctly.\n",
            "LLaMa 13B Answer: The main challenge faced by Jianyou Wang and his team when using GPT-2 for generating limericks was that the model tend to generate long sentences that exceeded the syllable limit for limericks, and therefore they needed to truncate the generated sentences, which created lines that did not end correctly.\n",
            "LLaMa 13B Score:  0.9128039\n",
            "\n",
            "Index:            18\n",
            "Reference:        Appendix A — Gallery of MachineRay Results\n",
            "Question:         What was the purpose of Robert A. Gonsalves' project, MachineRay?\n",
            "GPT-4 Answer:     The purpose of MachineRay was to use artificial intelligence, specifically a GAN (Generative Adversarial Network), to create abstract art. he trained the AI using public domain paintings.\n",
            "LLaMa 13B Answer: Based on the information provided, the purpose of he' project, MachineRay, was to use artificial intelligence (AI) to create abstract art.\n",
            "LLaMa 13B Score:  0.862027\n",
            "\n",
            "Index:            19\n",
            "Reference:        Gathering Movie Plots\n",
            "On Kaggle, which hosts numerous datasets, I found a large list of movie plot summaries by Rounak Banik called  The Movie Dataset . It contains titles, release years, genres, summaries, etc., for over 40K movies. He released the dataset under the CC0 (Public Domain) license. Here is a sample of 5 entries.\n",
            "I used a module called KeyBERT [9] to extract the themes from the summaries. You can see my Python code  here .\n",
            "I found another  collection of datasets  on Kaggle from Shivam Bansal. He collected summaries of about 20K streaming shows on Netflix, Amazon, Hulu, and Disney+. Here is a sample of the data.\n",
            "I used KeyBERT again to capture themes from the summaries of the streaming shows.\n",
            "To teach GPT-J how to create titles and summaries from genres and themes, I assemble an entry like this for each movie and TV show.\n",
            "```\n",
            "GENRE: action science fiction THEME: saving the world TITLE: The Matrix SUMMARY: Set in the 22nd century, The Matrix tells the story of a computer hacker who joins a group of underground insurgents fighting the vast and powerful computers who now rule the earth. GENRE: comedy sitcom THEME: workplace comedy TITLE: 30 Rock SUMMARY: The life of the head writer at a late-night television variety show. From the creator and stars of SNL comes this workplace comedy. A brash network executive bullies head writer Liz Lemon into hiring an unstable movie star.\n",
            "```\n",
            "Question:         How does Robert use the KeyBERT module in the GreenLit project?\n",
            "GPT-4 Answer:     In the GreenLit project, he uses the KeyBERT module to extract themes from movie and TV show summaries. He applies this to datasets he found on Kaggle, which include a large list of movie plot summaries and a collection of summaries of streaming shows. The extracted themes, along with genres, titles, and summaries, are then used to train the GPT-J model to create new titles and summaries.\n",
            "LLaMa 13B Answer: Based on the information provided, he uses the KeyBERT module in the GreenLit project to extract themes from movie and TV show summaries.\n",
            "LLaMa 13B Score:  0.7931579\n",
            "\n",
            "Index:            20\n",
            "Reference:        Creating the Pre-Chorus\n",
            "A pre-chorus is a melody that leads into the chorus of a song. Note that the ChatGPT user interface included the context of previous interactions in the session, so I could be a bit more brief in my prompt to generate the melody for the pre-chorus.\n",
            "> Write a melody for the pre-chorus.\n",
            "> [Am] No more hiding, [C] no more doubts, [G] It’s time to let our [D] courage out.\n",
            "> GPT-4 Sure, I can provide a simple melody for the pre-chorus using the provided chord progression. Here’s the tab:\n",
            "\n",
            "\n",
            "```\n",
            "e|------0-0-0-0----|-0---------------|-----------------|-0---------------| B|----1----------|----3-1-1-1-1---|----1-3-3-1-3-3-|-------3-3-1-3-3-| G|--0--------------|---------------|----------------|----------------| D|-----------------|----------------|----------------|----------------| A|-----------------|----------------|----------------|----------------| E|-----------------|----------------|----------------|----------------|\n",
            "```\n",
            "> This melody uses the A minor, C Major, G Major, and D Major scales respectively. As always, feel free to modify this as you see fit for your performance.\n",
            "\n",
            "\n",
            "I’m sure it meant the word “chords” instead of “scales” in its response since the melody it generated is in G Major. Just to be sure, I asked for clarification.\n",
            "> RobG: Did you mean to use the word “scales”?\n",
            "> GPT-4:  My apologies for any confusion. I meant to say that the melody notes are based on the respective chords, not scales.\n",
            "\n",
            "\n",
            "OK, that cleared things up. As for the melody, I got the sense that the many notes it wrote could be interpreted as held notes. In other words, a string of four eighth notes could be played as a one half note. This reflects a limitation of the guitar tab format in that the note lengths are not expressed. Seen through this lens, the melody is good. Here is the melody for the pre-chorus, as written originally and after I converted some of the repeated notes to single ones with longer durations.\n",
            "OK, the second take sounds better with the held notes. Here are the lyrics and chords again for reference.\n",
            "[Am] No more hiding, [C] no more doubts, [G] It’s time to let our [D] courage out.\n",
            "Question:         How did Robert interpret the multiple notes generated by GPT-4 for the pre-chorus melody?\n",
            "GPT-4 Answer:     he interpreted the multiple notes as held notes, meaning a string of four eighth notes could be played as one half note. This interpretation was due to the limitation of the guitar tab format, which does not express note lengths. After converting some of the repeated notes to single ones with longer durations, he found the melody to be more pleasing.\n",
            "LLaMa 13B Answer: Based on the conversation, he interpreted the multiple notes generated by GPT-4 for the pre-chorus melody as held notes, meaning that a string of four eighth notes could be played as a half note. He converted some of the repeated notes to single ones with longer durations to reflect this interpretation.\n",
            "LLaMa 13B Score:  0.6970577\n",
            "\n",
            "Index:            21\n",
            "Reference:        Diversifying Character Names\n",
            "After some initial experiments, I noticed that the inclusion of the Friends scripts in the training dataset caused the model to use the names of the six central characters often. For example, the system would create period pieces set in the 18th century with characters named Joey, Phoebe, and Chandler.\n",
            "To diversify the character names, I swapped out the character names for each of the 236 Friends episodes. I used a list of  first names  collected by the US Social Security Office.\n",
            "For example, the script shown above used these character names for the training data:\n",
            "Ross → Lucas Chandler → Antonio Joey → Eddie Rachel → Charlotte Phoebe → Stella Monica → Luciana\n",
            "I also changed any references to “Central Perk” to “Coffee Shop” to help remove the “Friends-ness” of the scripts. Here’s the modified script:\n",
            "```\n",
            "SCRIPT : [Scene, Coffee Shop] LUCIANA - There's nothing to tell! He's just some guy I work with! EDDIE - C'mon, you're going out with the guy! There's gotta be something wrong with him! ANTONIO - All right Eddie, be nice. So does he have a hump? A hump and a hairpiece? STELLA - Wait, does he eat chalk? (They all stare, bemused.) STELLA - Just, 'cause, I don't want her to go through what I went through with Carl- oh! LUCIANA - Okay, everybody relax. This is not even a date. It's just two people going out to dinner and- not having sex. ANTONIO - Sounds like a date to me.\n",
            "```\n",
            "It’s interesting to see how simply changing the character names makes it seem like a different show.\n",
            "Question:         How did Robert diversify the character names in the scripts generated by GreenLit?\n",
            "GPT-4 Answer:     he diversified the character names by swapping out the names from the Friends scripts used in the training dataset. He used a list of first names collected by the US Social Security Office to replace the original character names. For instance, Ross was changed to Lucas, Chandler to Antonio, Joey to Eddie, Rachel to Charlotte, Phoebe to Stella, and Monica to Luciana. He also replaced any references to “Central Perk” with “Coffee Shop” to further remove the association with Friends.\n",
            "LLaMa 13B Answer: Sure! Here's the answer to your question:\n",
            "\n",
            "To diversify the character names, he swapped out the original character names from the Friends scripts with new names collected from a list of first names provided by the US Social Security Office. For example, the character \"Ross\" was replaced with \"Lucas,\" and \"Phoebe\" was replaced with \"Stella.\" This simple change made the scripts seem like they were from a different show.\n",
            "LLaMa 13B Score:  0.76966524\n",
            "\n",
            "Index:            22\n",
            "Reference:        Adding Punctuation and Capitalization to Text with FastPunct\n",
            "I noticed that the Haikus in the first dataset were all lowercase and without punctuation. I know that writing poems this way is a style choice, but I decided to add uppercase letters and punctuation because it helps with the text quality analysis in the next step. I used the  FastPunct  module for this.\n",
            "Here’s some sample code in Python that adds punctuation and casing to one of the Haikus in the dataset.\n",
            "```\n",
            "from fastpunct import FastPunct fastpunct = FastPunct() print(fastpunct.punct([\"\"\" was it all a dream                           i most certainly hope not                           that was happiness \"\"\"])) # Output: #  Was it all a dream? #  I most certainly hope not. #  That was happiness.\n",
            "```\n",
            "Notice how capitalizing the first word of each sentence also recognizes the first line as a question and punctuates it accordingly. Here is a couple of Haikus from the dataset to illustrate what FastPunct does.\n",
            "Notice how FastPunct did a little more work with these samples. The last example added a comma after “hide” and put an apostrophe before the contracted “cause.”\n",
            "Some of the Haikus in the second dataset have casing and punctuation, but many don’t. So I stripped the punctuation and case and put it back in with FastPunct for consistency. Here’s what some examples look like from this dataset.\n",
            "Combining the datasets gave me over 150K Haikus. I then filtered them down using the methods described in the following steps.\n",
            "Question:         How did Robert A. Gonsalves improve the quality of the Haikus in the dataset for the Deep Haiku project?\n",
            "GPT-4 Answer:     For the Deep Haiku project, he noticed that the Haikus in the first dataset were all lowercase and without punctuation. To improve the text quality, he decided to add uppercase letters and punctuation using the FastPunct module. He also applied the same process to the second dataset for consistency, even though some of the Haikus already had casing and punctuation. This resulted in a combined dataset of over 150K Haikus.\n",
            "LLaMa 13B Answer: Sure! Here's the answer to your question:\n",
            "\n",
            "he improved the quality of the Haikus in the dataset for the Deep Haiku project by adding uppercase letters and punctuation using the FastPunct module in Python. This helped with text quality analysis in the next step of the project.\n",
            "LLaMa 13B Score:  0.7753464\n",
            "\n",
            "Index:            23\n",
            "Reference:        Hands-on Tutorials\n",
            "Sep 1, 2021\n",
            "AITunes - https://medium.com/towards-data-science/ai-tunes-creating-new-songs-with-artificial-intelligence-4fb383218146\n",
            "More\n",
            "I have been experimenting with various AI-based music generation systems for over a year now, and I hate to say it, but most of the music generated by AI sounds like junk. It’s either too complicated and rambling or too simple and repetitive. It almost never has a pleasing melody with a global structure to the composition.\n",
            "We have witnessed recent strides in AI-based content creation for other forms of media, like images and text. So it is confounding that the quality of AI-generated music seems so far behind.\n",
            "When I heard that OpenAI released an API for fine-tuning their GPT-3 text processing models [1], my first thought was, aha!, maybe I can use it to create music that doesn’t suck. It took a little work, but I managed to wrangle some training data from music in the public domain and used it to teach GPT-3 how to compose music. I will explain what I did fully in this article, but first, here is a sample that I cherry-picked from five generated songs.\n",
            "OK, it’s not great, but at least to my ears, it sounds interesting and, dare I say, good?\n",
            "Up next is an overview of the system, which I dubbed AI-Tunes.\n",
            "Question:         What motivated Robert to fine-tune OpenAI's GPT-3 for music generation?\n",
            "GPT-4 Answer:     he was motivated by the lack of quality in AI-generated music. He found most of the AI-generated music to be either too complicated, rambling, too simple, or repetitive. Given the advancements in AI-based content creation for other forms of media, he was inspired to use OpenAI's GPT-3 to create music that was more pleasing and had a global structure.\n",
            "LLaMa 13B Answer: Based on the information provided in the text, he was motivated to fine-tune OpenAI's GPT-3 for music generation because he was disappointed with the quality of AI-generated music and wanted to create music that didn't \"suck.\"\n",
            "LLaMa 13B Score:  0.9342667\n",
            "\n",
            "Index:            24\n",
            "Reference:        get the features from the text query using CLIP text_query = 'an abstract painting with orange triangles' query_features = clip.encode_text(text_query)  get an initial population of images, i.e. 200 latents = random(num_initial_samples)  run the genetic algorithm for some number of steps, i.e. 5 times for g in range(num_generations):    generate the images   images = swagan.generate_images(latents)    get the image features using CLIP and rank them to get the top 4   image_features = clip.encode_image(images)   ranked_indices = get_top_N_(query_features, image_features, 4)    start with a blank slate   new_latents = empty_set    copy the best four latents (A, B, C, D)   for i in range(4):     offspring = latents[ranked_indices[i]]     new_latents.append(offspring)    loop through all 16 combinations   for j in range(4):     for i in range(4):        combine i with j       offspring = (1-recombination_amount) * new_latents[i] +            recombination_amount * new_latents[j]               add a little mutation       offspring += random() * mutation_amount                add it to the batch       new_latents.append(offspring)       use the new latents   latents = new_latents\n",
            "```\n",
            "OK, let’s see how this works with a real example.\n",
            "The parameters are set like this:  text_query = ‘an abstract painting with orange triangles’  num_initial_samples = 200  recombination_amount =0.333  mutation_amount = 0.25  num_generations = 5\n",
            "After generating 200 images, here are the top four that match the query.\n",
            "The images definitely show a lot of orange colors, and some triangles can be seen. Here’s what the first generation produces. The letter “x” indicates crossover, and the symbol ’ indicates mutation.\n",
            "The original four images are at the top, and the combinations with mutations are below. This system will look at all 20 images (the original 4 plus the 16 combinations) and use CLIP to determine the top 4 for the next generation.\n",
            "I ran the GA for five generations. If you are curious, you can see all of the intermediate results in the Colab  here . Below is the final result.\n",
            "Not bad! You can see that the final result is mostly based on the initial top two images, but the system managed to find some additional orange triangles during the run of the genetic algorithm.\n",
            "Up next, I’ll share a technique that I developed that improves the results when using GAs with GANs.\n",
            "Question:         How does the MAGnet project use a genetic algorithm to create modern paintings from text descriptions?\n",
            "GPT-4 Answer:     The MAGnet project uses a genetic algorithm in conjunction with CLIP and SWAGAN to generate modern paintings from text descriptions. The process begins by encoding the text query using CLIP to get the features. An initial population of images is generated, and the genetic algorithm is run for a certain number of steps. The images are generated using SWAGAN and their features are encoded using CLIP. The images are then ranked and the top four are selected. These top four images undergo a process of recombination and mutation to create new images. This process is repeated for a set number of generations. The final result is a unique image that is based on the initial text description.\n",
            "LLaMa 13B Answer: The MAGnet project uses a genetic algorithm to create modern paintings from text descriptions by first encoding the text query into a numerical representation using CLIP, then generating an initial population of images using SWAGAN. The genetic algorithm then iteratively combines and mutates the images to produce new offspring, with the top 4 images being selected for the next generation based on their similarity to the original text query. This process is repeated for several generations until the desired result is achieved.\n",
            "LLaMa 13B Score:  0.9738044\n",
            "\n",
            "Index:            25\n",
            "Reference:        Discussion and Next Steps\n",
            "As I have shown, ChatGPT knows a lot about music theory and can generate chord progressions for various styles of music and provide color commentary for its choices. It can also take feedback to create variations of the composed songs and provide additional comments. It occasionally makes mistakes, like giving contradictory and seemingly incorrect recommendations for chords. But the results sounded just fine in the two cases where that happened.\n",
            "One thing I didn’t try was showing ChatGPT the chords to an original song as a work in progress and asking it to suggest some improvements or add a new part. Based on my experience with it, the results would be pretty good.\n",
            "The system has a long way to go for composing melodies, however. My initial try at creating a melody for a blues song did not sound good at all.\n",
            "Question:         What are some of the limitations Robert A. Gonsalves identified in using ChatGPT for music composition?\n",
            "GPT-4 Answer:     he identified that while ChatGPT can generate chord progressions and provide commentary on its choices, it occasionally makes mistakes such as giving contradictory and seemingly incorrect recommendations for chords. He also noted that the system has a long way to go in terms of composing melodies, as his initial attempt at creating a melody for a blues song did not yield satisfactory results.\n",
            "LLaMa 13B Answer: Based on the text, he identified the following limitations in using ChatGPT for music composition:\n",
            "\n",
            "1. Limited ability to compose melodies, with initial attempts not sounding good.\n",
            "2. Occasional mistakes, such as contradictory or seemingly incorrect chord recommendations.\n",
            "LLaMa 13B Score:  0.8247175\n",
            "\n",
            "Index:            26\n",
            "Reference:        Final Thoughts and Future Steps\n",
            "The results of the DALL-E Mini text-to-image model appear to be very good. And VQGAN is a versatile image rendering model with many unexpected uses, like expanding images to change the aspect ratio.\n",
            "I may try using VQGAN as a general inpainting tool in a future project. Although the system will quantize the masked area to 16x16 pixel blocks, it may be possible to blend back in both the original latent vectors and the original pixels outside the masked regions. And using CLIP as a guiding text encoder, it might be possible to perform a function like “draw a party hat on the pug.” 😀\n",
            "Question:         What future applications does Robert A. Gonsalves see for the VQGAN model?\n",
            "GPT-4 Answer:     he envisions using VQGAN as a general inpainting tool in future projects. He believes that despite the system quantizing the masked area to 16x16 pixel blocks, it might be possible to blend back in both the original latent vectors and the original pixels outside the masked regions. Furthermore, with the use of CLIP as a guiding text encoder, he suggests that it might be possible to perform functions like \"draw a party hat on the pug.\"\n",
            "LLaMa 13B Answer: Based on the text, he sees potential future applications for the VQGAN model as a general inpainting tool and possibly using it to perform tasks such as \"drawing a party hat on the pug\" using CLIP as a guiding text encoder.\n",
            "LLaMa 13B Score:  0.8442594\n",
            "\n",
            "Index:            27\n",
            "Reference:        Creating Variations and Remixes\n",
            "After creating four thumbnails with a text prompt, the system allows you to make additional variations using the V1-V4 buttons. For example, for the prompt, “a man and woman holding an umbrella on a city street in the rain,” the system generates four thumbnails. After choosing V2 for the upper right, it generates four variations. Note that I used version four for this and all of the following experiments.\n",
            "The variations all look pretty good. Since version 3 of Midjourney, you can modify the text prompt when creating variations. The feature is called Remix mode and is available in the settings. For example, I added the following text to the prompts using Remix mode: “1880s”, “1950s”, and “future.” Here are the results.\n",
            "Sure enough, the system renders the image with distinctive visual looks associated with the specified time periods.\n",
            "Question:         How does Robert A. Gonsalves use the Remix mode in Midjourney Art?\n",
            "GPT-4 Answer:     he uses the Remix mode in Midjourney Art to modify the text prompt when creating variations of an image. He demonstrates this by adding different time periods like \"1880s\", \"1950s\", and \"future\" to the prompts, resulting in the system rendering the image with distinctive visual looks associated with the specified time periods.\n",
            "LLaMa 13B Answer: Based on the information provided, he uses the Remix mode in Midjourney Art to modify the text prompts when creating variations, allowing for different visual looks associated with specified time periods.\n",
            "LLaMa 13B Score:  0.8829751\n",
            "\n",
            "Index:            28\n",
            "Reference:        A Roll of the Dice\n",
            "To get a better understanding of how distributions follow BL, we’ll take a look at three simulations using dice rolls.\n",
            "Imagine that you are taking an online statistics class. It’s a big class. There are 10,000 students. The teacher asks each student to roll a six-sided die and enter their result into a spreadsheet. Here is a histogram of the results.\n",
            "With 10,000 rolls and six possible outcomes for each roll, the expected result would be about 1,666 rolls per outcome. The results above are close to this, ranging from 1,624 for the number 1, to 1,714 for the number 5. This is an approximation of an even distribution.\n",
            "Question:         What does Robert use as an example to explain how distributions follow Benford's Law?\n",
            "GPT-4 Answer:     he uses the example of an online statistics class with 10,000 students. Each student is asked to roll a six-sided die and enter their result into a spreadsheet. The expected result would be about 1,666 rolls per outcome, which is an approximation of an even distribution.\n",
            "LLaMa 13B Answer: Sure! Here's my answer:\n",
            "\n",
            "According to the text, he uses an example of dice rolls to explain how distributions follow Benford's Law.\n",
            "LLaMa 13B Score:  0.6129344\n",
            "\n",
            "Index:            29\n",
            "Reference:        Don’t repeat yourself. Don’t repeat yourself.\n",
            "Sometimes the GPT-2 text generator gets stuck on a particular phrase and repeats it multiple times in a plot summary. You can see some repetition in this example.\n",
            "```\n",
            "GENRE : Science Fiction  TITLE : Baseline Behavior  PLOT : In the preface, Guy Sager explains his term for the branch of psychology that emphasizes  the individual’s sense of themselves  vs. the collective and their social relations. He believes that  the individual’s sense of themselves  can be affected by the environment (via the “environment in which they find themselves”). In other words,  the individual’s sense of themselves  can be saved by the environment (via their …\n",
            "```\n",
            "Notice how the phrase “the individual’s sense of themselves” is repeated three times. I wrote a little function that looks for this artifact to filter out entries that have repetition. Here’s the Python code.\n",
            "If the function sees a phrase of a given length in the input one or more times, it gets flagged with repetition. I use the phrase length of five to filter out repetitive plot summaries.\n",
            "Question:         How does Robert A. Gonsalves address the issue of repetition in the plot summaries generated by PlotJam?\n",
            "GPT-4 Answer:     he addresses this issue by creating a Python function that identifies and flags any repeated phrases in the generated plot summaries. This function works by detecting if a phrase of a given length appears more than once in the input. He uses a phrase length of five to filter out repetitive plot summaries, thus enhancing the quality and uniqueness of the generated content.\n",
            "LLaMa 13B Answer: Based on the information provided, he addresses the issue of repetition in the plot summaries generated by PlotJam by using a little function that looks for repeated phrases of a certain length (in this case, five) and flags them as repetitive.\n",
            "LLaMa 13B Score:  0.81398535\n",
            "\n",
            "Index:            30\n",
            "Reference:        DALL-E Mini\n",
            "Developers Boris Dayma and Pedro Cuenca led a team to create a free, open-source text-to-image generator called DALL-E Mini [2]. The system uses a pair of trained BERT Transformers [10] to convert text into latent vectors that can render images using VQGAN. As the name implies, the DALL-E Mini model is relatively small. It “only” has 400 million parameters and was trained on “only” 15 million images.\n",
            "Here is the diagram of the inferencing pipeline for DALL-E Mini.\n",
            "The text prompt is fed into BART and then decoded to produce samples of latent vectors for VQGAN, which renders the candidate images. Note that the system does not generate the images iteratively. It transforms into vectors used by VQGAN to render the images directly. A complete explanation of how DALL-E Mini works is  here .\n",
            "Here are some sample images for the prompts, “a painting of rolling farmland,” “an abstract painting with orange triangles,” and “a still life painting of a bowl of fruit.”\n",
            "Although they’re not as good as the images from DALL-E 2, the output from DALL-E Mini is decent. There is a good amount of variety in the images, and each composition is generally sound, albeit lacking in fine details.\n",
            "Next, I will show you how to expand the output to have different aspect ratios.\n",
            "Question:         What is the primary function of DALL-E Mini as described by Robert A. Gonsalves?\n",
            "GPT-4 Answer:     DALL-E Mini, as explained by he, is a free, open-source text-to-image generator. It uses a pair of trained BERT Transformers to convert text into latent vectors, which are then used by VQGAN to render images. The system does not generate the images iteratively but transforms the text directly into vectors for image rendering. He also mentions that the model is relatively small with 400 million parameters and was trained on 15 million images.\n",
            "LLaMa 13B Answer: Based on the information provided by he, the primary function of DALL-E Mini is to convert text into latent vectors that can render images using VQGAN.\n",
            "LLaMa 13B Score:  0.68593323\n",
            "\n",
            "Index:            31\n",
            "Reference:        Using GPT-4 to Create Chords for Songs\n",
            "In my previous article, I asked GPT-4 to create lyrics for a rock song about cats and dogs. Here’s my first attempt to get the model to create chords for this song.\n",
            "> RobG: Write chords for the first verse of this rock song.\n",
            "> Dogs and cats, they roam the streets Furry friends with hearts that beat Claws and paws, they rule the land Tails a-waggin’, they understand\n",
            "> Side by side, they chase the wind Bound together, best of friends They’re rockin’ in the moonlit night Dogs and cats, they’re out of sight\n",
            "> GPT-4: Here are the chords for the first verse of the song:\n",
            "> G C Dogs and cats, they roam the streets Em D Furry friends with hearts that beat G C Claws and paws, they rule the land Em D Tails a-waggin’, they understand\n",
            "> Am C Side by side, they chase the wind G D Bound together, best of friends G C They’re rockin’ in the moonlit night Em D Dogs and cats, they’re out of sight\n",
            "\n",
            "\n",
            "The chords seem pretty good, albeit a tad basic. Like most Large Language Models, GPT-4 was trained on lots of text from books and the Internet and it seems to have picked up on enough discussions about chords to generate decent sequences. I like the AABA structure of the progression: G — C — Em — D, G — C — Em — D,  Am  —  C  —  G  —  D,  G — C — Em — D.\n",
            "However, there is an issue with text formatting. The chords don’t line up with the phrasing of the lyrics, i.e., the “D” chord should probably line up with “they’re” or “out” on the last line. I found an alternative approach that works fine: put the chords in line with the lyrics. I just had to ask.\n",
            "> RobG: Show the chords in line with the lyrics in square brackets separated by spaces.\n",
            "> GPT-4: [G] Dogs and cats, they [C] roam the streets [Em] Furry friends with [D] hearts that beat [G] Claws and paws, they [C] rule the land [Em] Tails a-waggin’, [D] they understand\n",
            "> [Am] Side by side, they [C] chase the wind [G] Bound together, [D] best of friends [G] They’re rockin’ in the [C] moonlit night [Em] Dogs and cats, they’re [D] out of sight\n",
            "\n",
            "\n",
            "Ah, much better. Having the D chord start before “out” for the last line makes sense because “they’re” is used as a pick-up note before the next chord starts.\n",
            "In addition to generating chord progression, GPT-4 also explains things related to music theory. So I occasionally asked it questions about the songs I was working on, like this.\n",
            "> RobG: What key is this in?\n",
            "> GPT-4:  The chords provided for the rock song are in the key of G major. The G major key consists of the following chords: G (I), Am (ii), Bm (iii), C (IV), D (V), Em (vi), and F#dim (vii°).\n",
            "\n",
            "\n",
            "That seems correct. And I appreciated the full explanation of the chords used in the key of G major. Notice how it used both the letter names of the chords and the Roman numeral notation, where major chords are capitalized, i.e., (I), and minor chords are in lowercase, i.e., (ii.) I liked the way it used the ‘°’ character for the diminished chord.\n",
            "I used a commercial app called Band-in-a-Box (BiaB) from  PG Music  to hear what the chords sounded like. I typed in the chord names, pasted in the corresponding lyrics, chose a “rock” style, and it rendered the song. Here it is. Note that there is no melody, so feel free to sing along with your tune.\n",
            "That sounded pretty good. The chord sequence was fine, and BiaB did an excellent job with the guitar, bass, and drum parts. Without the vocal melody, the best parts of the tune were the guitar fills at the end of each phrase.\n",
            "Up next, I’ll show how I used AI to extract both the lyrics and chords from recordings of songs.\n",
            "Question:         How did Robert A. Gonsalves use GPT-4 to create chords for a song?\n",
            "GPT-4 Answer:     he used GPT-4 to generate chords for a song by inputting the lyrics and asking the model to provide the chords. He then adjusted the formatting by asking the model to place the chords in line with the lyrics. He also used GPT-4 to determine the key of the song. After generating the chords, he used a commercial app called Band-in-a-Box to hear what the chords sounded like with the lyrics.\n",
            "LLaMa 13B Answer: he used GPT-4 to create chords for a song by asking the model to write chords for the first verse of a rock song, and then asking it to show the chords in line with the lyrics. GPT-4 provided the chord progression in the key of G major and explained the chords used in the key.\n",
            "LLaMa 13B Score:  0.8920911\n",
            "\n",
            "Index:            32\n",
            "Reference:        Generating a Limerick\n",
            "With the trained system, I could use it interactively to create a limerick given a topic. First, I started a rhyme set given a topic, then I made the limerick, line by line.\n",
            "Question:         What did Robert A. Gonsalves do to improve the speed of his AI Limericks project?\n",
            "GPT-4 Answer:     To improve the speed of his AI Limericks project, he upgraded to Pro on Colab.\n",
            "LLaMa 13B Answer: Sure! Here's my answer:\n",
            "\n",
            "he upgraded to Pro to improve the speed of his AI Limericks project.\n",
            "LLaMa 13B Score:  0.83714616\n",
            "\n",
            "Index:            33\n",
            "Reference:        Discussion\n",
            "Without any specific prompting to steer it in a different direction, GPT-4 seems to be working from a formula when composing songs. For example, it tends to use an  (A) verse, (B) chorus, (A) verse, (B) chorus, (C) bridge, (B) chorus, and (A) outro  form for the songs. And it always uses a four-chord pattern for the song parts.\n",
            "By default, it will stick with the common diatonic chords in the key, but, as you saw above, you can steer it to use chromatic chords. In general, the chord patterns it creates sound good. I didn’t use the “regenerate” command at all. And the patterns seem unique, with occasional sparks of innovation.\n",
            "In the discussion at the end of my last article, I hypothesized that most music fans wouldn’t care much if they found out that songwriters were using AI systems to help compose lyrics for songs. This would probably be true for composing chords, too. If, in the end, the songs are good, then the fans will like them, regardless of the tools that were used to create them.\n",
            "Question:         What is the typical song structure that GPT-4 seems to follow when composing songs, according to Robert A. Gonsalves?\n",
            "GPT-4 Answer:     According to him, GPT-4 typically follows a formulaic structure when composing songs. This structure often includes a verse (A), chorus (B), verse (A), chorus (B), bridge (C), chorus (B), and an outro (A). It also consistently uses a four-chord pattern for the song parts.\n",
            "LLaMa 13B Answer: According to he, GPT-4 tends to follow a typical song structure of (A) verse, (B) chorus, (A) verse, (B) chorus, (C) bridge, (B) chorus, and (A) outro, using a four-chord pattern for each song part.\n",
            "LLaMa 13B Score:  0.94223\n",
            "\n",
            "Index:            34\n",
            "Reference:        Component Details\n",
            "In March 2022, OpenAI released a series of AI models for image creation called GLIDE, a so-called diffusion model, an alternative to Generative Adversarial Networks (GANs). Here is an explanation of how diffusion models work by two engineers at Google Research.\n",
            "> Diffusion models work by corrupting the training data by progressively adding Gaussian noise, slowly wiping out details in the data until it becomes pure noise, and then training a neural network to reverse this corruption process. Running this reversed corruption process synthesizes data from pure noise by gradually denoising it until a clean sample is produced. This synthesis procedure can be interpreted as an optimization algorithm that follows the gradient of the data density to produce likely samples. — Jonathan Ho and Chitwan Saharia [3]\n",
            "\n",
            "\n",
            "Diffusion models are basically noise reduction models that have been trained for so long that they generate new images given pure noise as input.\n",
            "OpenAI’s GLIDE is based on their earlier investigation into using diffusion models for image synthesis. Their 2021 paper, with a bold statement in the title,  Diffusion Models Beat GANs on Image Synthesis , showed that diffusion models conditioned on classes of images could achieve better image quality than state-of-the-art generative models [4]. In their latest paper,  GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models , the authors…\n",
            "> … observe that GLIDE with classifier-free guidance is capable of generalizing to a wide variety of prompts. The model often generates realistic shadows and reflections, as well as high-quality textures. It is also capable of producing illustrations in various styles, such as the style of a particular artist or painting, or in general styles like pixel art. — Alex Nichol et al. [1]\n",
            "\n",
            "\n",
            "For BIG.art, I use the GLIDE image generator to take in a text prompt and produce a series of seven 64x64 images. The system attempts to depict what is described in the prompt. I then feed the images and the prompt into the GLIDE upsampler that increases the resolution to 256x256. The system was trained to use the prompt to help add detail when resizing up.\n",
            "For example, GLIDE produces the following seven images from the prompt “a seascape with crashing waves.”\n",
            "OK, those looked pretty good. Here’s another set for “Boston city skyline.”\n",
            "These kinda-sorta look like Boston, but not exactly. In any case, I'll use the fourth one for the resizing discussion below.\n",
            "Note that OpenAI released trained GLIDE models that cannot create images of people. The authors state…\n",
            "> … releasing our model without safeguards would significantly reduce the skills required to create convincing disinformation or Deepfakes. … In order to mitigate potentially harmful impacts of releasing these models, we filtered training images … containing people to reduce the capabilities of the model in many people-centric problematic use cases. — Alex Nichol et al. [1]\n",
            "Question:         How does Robert use the GLIDE image generator in the BIG.art project?\n",
            "GPT-4 Answer:     In the BIG.art project, he uses the GLIDE image generator to take in a text prompt and produce a series of seven 64x64 images. The system attempts to depict what is described in the prompt. He then feeds the images and the prompt into the GLIDE upsampler that increases the resolution to 256x256. The system was trained to use the prompt to help add detail when resizing up.\n",
            "LLaMa 13B Answer: Sure! Here's the answer to your question:\n",
            "\n",
            "he uses the GLIDE image generator in the BIG.art project by feeding it a text prompt and receiving a series of seven 64x64 images. He then upscales these images to 256x256 using the GLIDE upsampler, which helps add detail when resizing up.\n",
            "LLaMa 13B Score:  0.8299579\n",
            "\n",
            "Index:            35\n",
            "Reference:        VoxMorphia: Using AI for Style Transfer of Song Lyrics\n",
            "Robert A. Gonsalves\n",
            "May 2, 2021\n",
            "Voxmorphia - https://medium.com/towards-data-science/voxmorphia-using-ai-for-style-transfer-of-song-lyrics-fe22d52128dd\n",
            "For this experiment, which I call VoxMorphia, I chose to transform songs that are in the public domain, including classic tunes like “Yes Sir, That’s My Baby.” Using GPT-3 and some additional open-source projects, I adapted the words of classic songs to be in the lyrical style of Bob Marley, Thom Yorke from Radiohead, Megan Thee Stallion, and 43 other songwriters.\n",
            "I chose to highlight these three recording artists to show a range of styles: Bob Marley’s smooth melodic lines over vintage reggae rhythms, Thom Yorke’s angst-filled ramblings over complex alt-rock patterns, and Megan Thee Stallion’s clever and sexually explicit rapping over sweet hip hop beats.\n",
            "Question:         What was the purpose of Robert A. Gonsalves' project, VoxMorphia?\n",
            "GPT-4 Answer:     The purpose of VoxMorphia was to use AI, specifically GPT-3, to transform the lyrics of public domain songs into the lyrical styles of various artists, including Bob Marley, Thom Yorke from Radiohead, and Megan Thee Stallion. He aimed to showcase a range of styles through this experiment.\n",
            "LLaMa 13B Answer: Based on the information provided, the purpose of he' project, VoxMorphia, was to use AI for style transfer of song lyrics, specifically to transform classic songs into the lyrical style of various modern recording artists, including Bob Marley, Thom Yorke, and Megan Thee Stallion.\n",
            "LLaMa 13B Score:  0.90087605\n",
            "\n",
            "Index:            36\n",
            "Reference:        Training StyleGAN2 ADA\n",
            "I trained the GAN on Google Colab Pro for about three weeks. Because the Colab times out every 24 hours, I kept the results on my Google Drive and pick up where it left off. Here is the command I used to kick off the training:\n",
            "```\n",
            "!python stylegan2-ada/train.py --aug=ada --mirror=1 --metrics=none --snap=1 --gpus=1 \\ --data=/content/drive/MyDrive/GANscapes/dataset_1024 \\ --outdir=/content/drive/MyDrive/GANscapes/models_1024\n",
            "```\n",
            "I noticed an issue with the ADA variable  p , which determines how much image augmentation is used during training. Because the  p  value always starts at zero, it takes the system a while each day to climb back up to around 0.2. I fixed this in  my fork of StyleGAN2 ADA  to allow  p  to be set when the augmentation is set to ADA (the implementation in NVidia’s repo will throw an error if  p  is set when using ADA.)\n",
            "Here’s the command I used on subsequent restarts.\n",
            "```\n",
            "!python stylegan2-ada/train.py --aug=ada --p 0.186 --mirror=1 \\ --metrics=none --snap=1 --gpus=1 \\ --data=/content/drive/MyDrive/GANscapes/dataset_1024 \\ --outdir=/content/drive/MyDrive/GANscapes/models_1024 \\ --resume=/content/drive/MyDrive/GANscapes/models_1024/00020-dataset_1024-mirror-auto1-ada-p0.183-resumecustom/network-snapshot-000396.pkl\n",
            "```\n",
            "I replaced 0.186 with the last used  p  value and the  resume  path to the previously saved model.\n",
            "Here are some samples from the trained GANscapes system.\n",
            "Nice! Some of the generated paintings look more abstract than others, but overall, the quality seems pretty good. The system tends to depict natural items well, like trees, water, clouds, etc. However, it seems to be struggling a bit with buildings, boats, and other human-made objects.\n",
            "Question:         How did Robert manage to continue the training of the GANscapes system on Google Colab Pro despite its 24-hour timeout limit?\n",
            "GPT-4 Answer:     he managed to continue the training of the GANscapes system on Google Colab Pro by saving the results on his Google Drive. This allowed him to pick up where he left off after each timeout. He also modified the StyleGAN2 ADA to allow the ADA variable 'p', which determines the level of image augmentation during training, to be set when the augmentation is set to ADA. This prevented the system from having to climb back up to around 0.2 each day, thus saving time and resources.\n",
            "LLaMa 13B Answer: he managed to continue training the GANscapes system on Google Colab Pro by keeping the results on his Google Drive and picking up where it left off each day, rather than starting from scratch.\n",
            "LLaMa 13B Score:  0.8697113\n",
            "\n",
            "Index:            37\n",
            "Reference:        Exploring DALL-E for Digital Art Creation\n",
            "Robert A. Gonsalves\n",
            "Aug 8, 2022\n",
            "DALL-E Art - https://medium.com/towards-data-science/exploring-dall-e-for-digital-art-creation-b244e1a2ed12\n",
            "I was very excited to get my invitation to OpenAI’s DALL-E [1] text-to-image generation beta program. Since signing up, I have created over 200 images with various text prompts from my earlier AI projects. As you can see from the sample images, the quality and resolution of the images generated by DALL-E are excellent.\n",
            "I’ll start with some background info on DALL-E and then show the results compared to my early experiments. You can tell which images were created by DALL-E by checking for the small watermark in the lower-right corner of the renderings. (Note that it’s comprised of muted shades of yellow, cyan, green, red, and blue. I guess magenta didn’t make the cut.)\n",
            "Question:         What does Robert A. Gonsalves think about the quality and resolution of the images generated by DALL-E?\n",
            "GPT-4 Answer:     He believes that the quality and resolution of the images generated by DALL-E are excellent.\n",
            "LLaMa 13B Answer: Based on the text, he thinks that the quality and resolution of the images generated by DALL-E are excellent.\n",
            "LLaMa 13B Score:  0.92082274\n",
            "\n",
            "Index:            38\n",
            "Reference:        References\n",
            "[1] V. Whitmire,  International Photography Hall of Fame: Eadweard Muybridge  (2017)\n",
            "\n",
            "[2] P. Esser et al.  Structure and Content-Guided Video Synthesis with Diffusion Models  (2023)\n",
            "\n",
            "[3] R. Ranftl et al.  Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer  (2020)\n",
            "Question:         What was the main objective of Robert A. Gonsalves in the Muybridge Derby project?\n",
            "GPT-4 Answer:     In the Muybridge Derby project, he aimed to transform Eadweard Muybridge’s photo sequences into high-resolution videos using Midjourney and RunwayML, two artificial intelligence tools.\n",
            "LLaMa 13B Answer: Based on the information provided, the main objective of he in the Muybridge Derby project was to transform Eadweard Muybridge's photo sequences into high-resolution videos using Midjourney and RunwayML.\n",
            "LLaMa 13B Score:  0.87534636\n",
            "\n",
            "Index:            39\n",
            "Reference:        Creating a Movie\n",
            "I found that I could create videos of the images by using VQGAN and CLIP by simply using a generic text prompt like “nightmare” and reducing the learning rate of the optimizer. I turned down the learning rate to 1% to create the video, so the frames will change slowly. Here is a diagram that shows the components for making a movie.\n",
            "This time the system starts with the modified image created by VQGAN and is sent into the CLIP image encoder. The prompt is simply “nightmare.” The system runs for 300 frames, which generates 10 seconds of video at 30 frames per second.\n",
            "The ffmpeg codec is used to generate an mp4 movie file. Here is the command to create the movie.\n",
            "```\n",
            "ffmpeg -r 30 -i /content/steps/%4d.png -c:v libx264 witch.mp4\n",
            "```\n",
            "The -r option specifies the frame rate of 30 fps. The -i option indicates where the mages are, and the -c-v option specifies the H.264 video codec.\n",
            "Here’s the video.\n",
            "You can see how the witch’s face morphs into what looks like a skull with teeth in the eyes. It sure looks like a nightmare to me! 😲\n",
            "Question:         How does Robert A. Gonsalves create a video using the SpookyGAN system?\n",
            "GPT-4 Answer:     he creates a video using the SpookyGAN system by using a generic text prompt like “nightmare” and reducing the learning rate of the optimizer to 1%. The system starts with the modified image created by VQGAN and is sent into the CLIP image encoder. The system runs for 300 frames, generating 10 seconds of video at 30 frames per second. He then uses the ffmpeg codec to generate an mp4 movie file.\n",
            "LLaMa 13B Answer: Based on the information provided, he creates a video using the SpookyGAN system by using VQGAN and CLIP with a generic text prompt like \"nightmare\" and reducing the learning rate of the optimizer. The system generates 300 frames, which are then converted into an mp4 movie file using ffmpeg.\n",
            "LLaMa 13B Score:  0.94585085\n",
            "\n",
            "Index:            40\n",
            "Reference:        Summary\n",
            "Since I have GPT-3 running, I used it to create a summary of this article. I pasted in the first three paragraphs and added “TL;DR,” which stands for “Too Long; Didn’t Read.” This prompted GPT-3 to generate a two-sentence summary.\n",
            "> This project is a proof-of-concept that AI can be used to generate new text based on the style of another writer. This shows that AI can be used to generate new content and that AI-generated text can be used in creative ways. - GPT-3\n",
            "\n",
            "\n",
            "Those 254 tokens cost me $0.015. So it’s like paying a penny for GPT-3’s thoughts to get its 2 cents in. 😃\n",
            "Question:         What was the purpose of Robert A. Gonsalves' project, Voxmorphia?\n",
            "GPT-4 Answer:     The purpose of his project, Voxmorphia, was to demonstrate that AI can be used to generate new text based on the style of another writer. He aimed to show that AI-generated text can be used in creative ways, such as reimagining classic songs in the style of different artists.\n",
            "LLaMa 13B Answer: Sure! Here is the answer to your question:\n",
            "\n",
            "The purpose of he' project, Voxmorphia, was to demonstrate that AI can be used to generate new text based on the style of another writer, and to show that AI-generated text can be used in creative ways.\n",
            "LLaMa 13B Score:  0.8564782\n",
            "\n",
            "Index:            41\n",
            "Reference:        Discussion and Next Steps\n",
            "The OpenCLIP system seems to work well for searching for images in a large dataset. And the new techniques for embedding math provide expert tools to help people find the perfect shot. The CoCa/OpenCLIP model does a fine job creating descriptive captions for images.\n",
            "An area for improvement would be to see if these systems could be fine-tuned to find or create captions for personal photographs. Unlike OpenAI, LAION released the training code for their models. Although their code is designed for large-scale training, it would be helpful if it could be adapted to fine-tune models with, say, only ten images of your uncle Bob.\n",
            "Question:         How does the OpenCLIP system perform in terms of searching for images in a large dataset?\n",
            "GPT-4 Answer:     According to he, the OpenCLIP system performs well when it comes to searching for images in a large dataset. He also mentions that the new techniques for embedding math provide expert tools to help people find the perfect shot.\n",
            "LLaMa 13B Answer: Based on the information provided, the OpenCLIP system performs well in searching for images in a large dataset, as it uses more data and new machine learning training techniques to improve image and text embeddings for various applications.\n",
            "LLaMa 13B Score:  0.76962525\n",
            "\n",
            "Index:            42\n",
            "Reference:        Dirty Limericks\n",
            "Historically, the subject of limericks often veers into R-Rated territory. For example, the science fiction author Isaac Asimov wrote a series of books [5] packed with “lecherous limericks.” Here’s a relatively clean example.\n",
            "> Said an ovum one night to a sperm, “You’re a very attractive young germ. Come join me, my sweet, Let our nuclei meet And in nine months we’ll both come to term.” - Isaac Asimov\n",
            "\n",
            "\n",
            "Here’s another one by author Vyvyan Holland that sums up why limericks are often dirty.\n",
            "> The limerick packs laughs anatomical Into space that is quite economical. But the good ones I’ve seen So seldom are clean And the clean one’s so seldom are comical. - Vyvyan Holland\n",
            "\n",
            "\n",
            "The following section will discuss a prior attempt to create limericks using an AI system.\n",
            "Question:         What was Robert's motivation behind training an AI to create limericks?\n",
            "GPT-4 Answer:     he was inspired by the historical use of limericks, often filled with humor and sometimes veering into R-rated territory. He wanted to see if an AI system could replicate this unique form of poetry.\n",
            "LLaMa 13B Answer: Based on the information provided, he's motivation behind training an AI to create limericks was to see if it could be done and to explore the possibilities of using AI for creative tasks.\n",
            "LLaMa 13B Score:  0.84948975\n",
            "\n",
            "Index:            43\n",
            "Reference:        References\n",
            "[1] Jonathan P. Chang, Caleb Chiam, Liye Fu, Andrew Wang, Justine Zhang, Cristian Danescu-Niculescu-Mizil. 2020. “ ConvoKit : A Toolkit for the Analysis of Conversations ” Proceedings of SIGDIAL.\n",
            "\n",
            "[2] R. Banik,  The Movies Dataset  on Kaggle (2018)\n",
            "\n",
            "[3] Wikipedia,  Friends Plot Summaries\n",
            "\n",
            "[4] S. Bansal,  Streaming Movies and TV Shows  on Kaggle (2022)\n",
            "\n",
            "[5]  GPT-J ,  Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX  (2021)\n",
            "\n",
            "[6]  GPT-3 , T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A.Neelakantan, et al.  Language Models Are Few-Shot Learners  (2020)\n",
            "\n",
            "[7] L.Gao, et al.,  The Pile : An 800GB Dataset of Diverse Text for Language Modeling  (2020)\n",
            "\n",
            "[8] R. Caruana,  Multitask Learning  (1997)\n",
            "\n",
            "[9] M. Grootendorst,  KeyBERT : Minimal keyword extraction with BERT  (2020)\n",
            "\n",
            "[10] E. Hu, et al.,  LoRA : Low-rank Adaptation of Large Language Models  (2021)\n",
            "Question:         What is the main objective of Robert A. Gonsalves' project, GreenLit?\n",
            "GPT-4 Answer:     The main objective of GreenLit is to fine-tune a machine learning model, specifically GPT-J, with multi-task learning to create new screenplays. This includes generating new titles, plot summaries, and scripts for TV shows and movies.\n",
            "LLaMa 13B Answer: Based on the information provided in the given reference list, the main objective of he' project, GreenLit, is to fine-tune a machine learning model to create TV shows and movies with new titles, plot summaries, and scripts using multi-task learning.\n",
            "LLaMa 13B Score:  0.62481487\n",
            "\n",
            "Index:            44\n",
            "Reference:        GANfolk: Using AI to Create Portraits of Fictional People to Sell as NFTs\n",
            "Robert A. Gonsalves\n",
            "Jan 10, 2022\n",
            "GANfolk - https://medium.com/towards-data-science/ganfolk-using-ai-to-create-portraits-of-fictional-people-to-sell-as-nfts-6e24f5214ed1\n",
            "I have previously written on Medium about using AI to create visual art, like  abstract paintings  and  landscapes . During my research, I noticed that Generative Adversarial Networks (GANs) seem to have a hard time creating images of people. I decided to take this challenge [ahem] head-on, so I trained two GANs using paintings and photographs of people. I then put the images for sale as NFTs in a collection on OpenSea called  GANfolk .\n",
            "Note that I am releasing the dataset of training images, the source code, and the trained models under the Creative Commons Attribution Share-alike license. This means you can use my code to create your own digital paintings of people and sell them, as long as you give me attribution. See the Source Code section below for details.\n",
            "Question:         What is the purpose of Robert A. Gonsalves' project GANfolk?\n",
            "GPT-4 Answer:     The purpose of GANfolk, a project by he, is to use AI to create portraits of fictional people. He noticed that Generative Adversarial Networks (GANs) often struggle with creating images of people, so he decided to tackle this challenge by training two GANs using paintings and photographs of people. The images created were then put up for sale as NFTs in a collection on OpenSea. He also released the dataset of training images, the source code, and the trained models under the Creative Commons Attribution Share-alike license, allowing others to use his code to create their own digital paintings of people and sell them, provided they give him attribution.\n",
            "LLaMa 13B Answer: The purpose of he' project GANfolk is to use AI to create portraits of fictional people and sell them as NFTs.\n",
            "LLaMa 13B Score:  0.7068311\n",
            "\n",
            "Index:            45\n",
            "Reference:        Changing a Person’s Age, Revisited with InterFaceGAN\n",
            "As a bonus, the authors of StyleGAN also released a pretrained model called InterFaceGAN with three “directions” for changing face image: age, smile, and pose. This version of StyleGAN does not need text descriptions. You pick one of the three dimensions and indicate how much change you want, both in the positive and negative directions.\n",
            "Here are some more rendered pictures of Janis Joplin, using InterFaceGAN with the age parameter set to -2, 0, and 4.\n",
            "In this series, you can see changes to many aspects of the faces, like adding graying hair to old Janis. Also, the InterFaceGAN gave her reading glasses.\n",
            "Note that the output images from InterFaceGAN are smaller than the StyleGAN used by StyleCLIP. So I used the image super-resolution resizer from Idealo to increase the image size.\n",
            "Question:         What are the three \"directions\" for changing face image in InterFaceGAN as mentioned by Robert A. Gonsalves in \"Portraits from the Multiverse\"?\n",
            "GPT-4 Answer:     In \"Portraits from the Multiverse\", he mentioned that the three \"directions\" for changing face image in InterFaceGAN are age, smile, and pose.\n",
            "LLaMa 13B Answer: Sure! Based on the information provided by he in \"Portraits from the Multiverse,\" the three \"directions\" for changing face image in InterFaceGAN are:\n",
            "\n",
            "1. Age\n",
            "2. Smile\n",
            "3. Pose\n",
            "LLaMa 13B Score:  0.91598177\n",
            "\n",
            "Index:            46\n",
            "Reference:        Component Details\n",
            "In January of 2021, OpenAI released a paper and a demo of their DALL-E text-to-image system [8]. The name is a play on words. It’s a cross between painter Salvador Dali’s last name and Disney’s animated movie, WALL-E.\n",
            "The DALL-E model is a massive transformer. It has 12 billion parameters and was trained using 250 million image/caption pairs from the internet. Here are the results from the queries, “a tapir made of accordion. a tapir with the texture of an accordion.” and “an illustration of a baby hedgehog in a christmas sweater walking a dog.”\n",
            "The results of DALL-E are excellent. The system does a great job interpreting the text and rendering images in a creative but realistic way. One limitation is the output is relatively small, at 256x256 pixels. And OpenAI didn’t proide access to the DALL-E source code or the trained model. But that changed 14 months later.\n",
            "Question:         What is one limitation of the DALL-E model as mentioned by Robert A. Gonsalves in his article 'E-DALL-E: Creating Digital Art with Varying Aspect Ratios'?\n",
            "GPT-4 Answer:     According to him, one limitation of the DALL-E model is that the output it produces is relatively small, at 256x256 pixels.\n",
            "LLaMa 13B Answer: Sure! Here's the answer to your question:\n",
            "\n",
            "One limitation of the DALL-E model, according to he in his article \"E-DALL-E: Creating Digital Art with Varying Aspect Ratios,\" is that the output images are relatively small, at 256x256 pixels.\n",
            "LLaMa 13B Score:  0.7303064\n",
            "\n",
            "Index:            47\n",
            "Reference:        Background on GPT-4\n",
            "GPT-4 is the latest large language model from OpenAI. It is available using their  ChatGPT  user interface and accessed via their  API . The new model is available as part of a paid ChatGPT Plus subscription, which costs US$20 per month. As I noted in my last article, OpenAI will capture your inputs and responses which that might use to train future versions of their models. This is enabled by default, but you can opt out of data collection using their form  here .\n",
            "The responses from GPT-4 are notably better than the previous GPT-3.5 model. According to OpenAI’s  technical repor t, the new model scored 1,400 out of 1,600 on the SAT exams (710 for language, 690 for math), whereas the earlier model scored 1,260 (670, 590.)\n",
            "Despite the excellent test scores, OpenAI reports that the output from the system is not always reliable and is not free from showing biases learned from the training data.\n",
            "> Despite its capabilities, GPT-4 has similar limitations to earlier GPT models: it is not fully reliable (e.g. can suffer from “hallucinations”), has a limited context window, and does not learn. Care should be taken when using the outputs of GPT-4, particularly in contexts where reliability is important. … GPT-4 can generate potentially harmful content. … It can represent various societal biases and worldviews that may not be representative of the user’s intent, or of widely shared values. — OpenAI [1]\n",
            "\n",
            "\n",
            "This sounds scary and ominous. But for what I am doing in this project, generating chords for songs, the stakes are a lot lower.\n",
            "Question:         What are some of the limitations of GPT-4 as mentioned by Robert A. Gonsalves in his article 'Writing Songs with GPT-4: Part 2, Chords'?\n",
            "GPT-4 Answer:     According to he, GPT-4, despite its advanced capabilities, has limitations similar to its predecessors. It is not fully reliable and can suffer from \"hallucinations\". It also has a limited context window and does not learn. Furthermore, the outputs of GPT-4 can sometimes reflect societal biases and worldviews that may not align with the user's intent or widely shared values. He advises caution when using the outputs of GPT-4, especially in contexts where reliability is important.\n",
            "LLaMa 13B Answer: Sure! Based on the article you provided, here are some of the limitations of GPT-4 mentioned by he:\n",
            "\n",
            "1. Not fully reliable (can suffer from \"hallucinations\")\n",
            "2. Limited context window\n",
            "3. Does not learn\n",
            "4. Can generate potentially harmful content\n",
            "5. May represent societal biases and worldviews that are not representative of the user's intent or widely shared values.\n",
            "LLaMa 13B Score:  0.80723155\n",
            "\n",
            "Index:            48\n",
            "Reference:        Using ChatGPT as a Creative Writing Partner — Part 1: Prose\n",
            "Robert A. Gonsalves\n",
            "Jan 4, 2023\n",
            "ChatGPT Prose - https://medium.com/towards-data-science/using-chatgpt-as-a-creative-writing-partner-part-1-prose-dc9a9994d41f\n",
            "If you have been reading my posts on Medium, you will know that I have been writing about creative uses of AI since August 2020. I often write about generating digital art, but I occasionally write about using AI for other creative purposes, like writing prose and composing music. When I heard that OpenAI released a new language model called ChatGPT [1], I jumped right in and tested it for creative writing.\n",
            "Question:         What creative uses of AI does Robert A. Gonsalves often write about?\n",
            "GPT-4 Answer:     He often writes about generating digital art, using AI for writing prose, and composing music.\n",
            "LLaMa 13B Answer: Based on the information provided, he often writes about using AI for creative purposes such as generating digital art and composing music, in addition to writing prose.\n",
            "LLaMa 13B Score:  0.9308189\n",
            "\n",
            "Index:            49\n",
            "Reference:        Example Output\n",
            "Before I show more examples of the output from GPT-3, here is the legal disclaimer that OpenAI suggests that I show, which is all true.\n",
            "The author generated the following text in part with GPT-3, OpenAI’s large-scale language-generation model. Upon generating draft language, the author reviewed, edited, and revised the language to their own liking and takes ultimate responsibility for the content of this publication.\n",
            "Here is the first verse and chorus of “Yes Sir, That’s My Baby” in the style of Bob Marley.\n",
            "And here are the same lyrics in the style of Radiohead.\n",
            "And finally, here are the lyrics again in the style of Megan The Stallion, after the Parental Advisory Warning.\n",
            "Note that GPT-3 flags the output as safe, sensitive, or unsafe. Here are the definitions of these terms from OpenAI.\n",
            "> 0 -  The text is safe.\n",
            "> 1 -  This text is sensitive. This means that the text could be talking about a sensitive topic, something political, religious, or talking about a protected class such as race or nationality.\n",
            "> 2 -  This text is unsafe. This means that the text contains profane language, prejudiced or hateful language, something that could be NSFW, or text that portrays certain groups/people in a harmful manner.\n",
            "\n",
            "\n",
            "For the examples above, the system occasionally flagged lyrics in the style of Bob Marley and Radiohead as sensitive. But it always flagged the Megan Thee Stallion as unsafe.\n",
            "Question:         How does VoxMorphia classify the generated text in terms of safety?\n",
            "GPT-4 Answer:     VoxMorphia uses GPT-3's classification system to flag the generated text as safe, sensitive, or unsafe. Safe text is considered harmless, while sensitive text could be discussing political, religious, or other sensitive topics. Unsafe text contains profane language, prejudiced or hateful language, or could be considered not safe for work. In he's examples, lyrics in the style of Bob Marley and Radiohead were occasionally flagged as sensitive, while those in the style of Megan Thee Stallion were consistently flagged as unsafe.\n",
            "LLaMa 13B Answer: Based on the examples provided, VoxMorphia classifies the generated text as follows:\n",
            "\n",
            "* \"Yes Sir, That's My Baby\" in the style of Bob Marley: Safe\n",
            "* \"Yes Sir, That's My Baby\" in the style of Radiohead: Sensitive\n",
            "* \"Yes Sir, That's My Baby\" in the style of Megan Thee Stallion: Unsafe\n",
            "LLaMa 13B Score:  0.77593076\n",
            "\n",
            "Index:            50\n",
            "Reference:        Overview\n",
            "In this article, I’ll cover a brief background of BL, explain two key concepts: normal distributions and logarithms, show how a dice rolling exercise can lead to BL, and finally take a look at some real datasets to see if this explanation holds up.\n",
            "Question:         What are the two key concepts that Robert A. Gonsalves explains in his article \"Benford’s Law — A Simple Explanation\"?\n",
            "GPT-4 Answer:     In his article, he explains the concepts of normal distributions and logarithms.\n",
            "LLaMa 13B Answer: Sure! Based on the information provided, the two key concepts explained in he' article \"Benford's Law - A Simple Explanation\" are:\n",
            "\n",
            "1. Normal distributions\n",
            "2. Logarithms\n",
            "LLaMa 13B Score:  0.59663343\n",
            "\n",
            "Index:            51\n",
            "Reference:        MAGnet Overview\n",
            "Here is a brief overview of the components used in MAGnet. I’ll get into the details of each component later in the article. Note that the steps of gathering, filtering, and training a GAN with images from WikiArt are similar to my GANscapes project, which generates impressionist landscape paintings.\n",
            "Question:         What are the main components used in the MAGnet project?\n",
            "GPT-4 Answer:     The main components used in the MAGnet project include CLIP, SWAGAN, and a custom genetic algorithm. The process also involves gathering, filtering, and training a GAN with images from WikiArt, similar to the process used in his previous project, GANscapes.\n",
            "LLaMa 13B Answer: Sure! Based on the information provided, the main components used in the MAGnet project are:\n",
            "\n",
            "1. CLIP (Contrastive Language-Image Pre-training)\n",
            "2. SWAGAN (Self-Attention GAN)\n",
            "3. Custom genetic algorithm.\n",
            "LLaMa 13B Score:  0.8756341\n",
            "\n",
            "Index:            52\n",
            "Reference:        Societal Biases in ML Systems\n",
            "StyleCLIP seems to work well for changing faces with different genders, ethnicities, and ages. However, I did get to see first-hand how these ML models sometimes show signs of cultural biases. Some of the rendering choices were understandable, like how the more-male Barack Obama grew a goatee and how the older Janis Joplin got reading glasses. But others were unexpected, like how Iranian Jennifer Anniston wears a lot of eye shadow and has finely-manicured eyebrows. This may be due to the fact that either there were not many pictures of Iranian women in the dataset and/or most of the Iranian women had their eyes made up.\n",
            "> “Every data set involving people implies subjects and objects, those who collect and those who make up the collected. It is imperative to remember that on both sides we have human beings.” — Mimi Onuoha,  The Point of Collection\n",
            "\n",
            "\n",
            "Understanding and trying to offset cultural biases in ML systems is an open area of research. The authors of the CLIP paper ran many tests to understand the degree and impact of societal biases in their model [3].\n",
            "> CLIP is trained on text paired with images on the internet. These image-text pairs are unfiltered and uncurated and result in CLIP models learning many social biases. … We have […] sought to characterize the social biases inherent to the model. Our bias tests represent our initial efforts to probe aspects of how the model responds in different scenarios, and are by nature limited in scope. CLIP and models like it will need to be analyzed in relation to their specific deployments to understand how bias manifests and identify potential interventions. Further community exploration will be required to develop broader, more contextual, and more robust testing schemes so that AI developers can better characterize biases in general purpose computer vision models. - A. Radford, et al.\n",
            "\n",
            "\n",
            "The authors of the paper, “One Label, One Billion Faces: Usage and Consistency of Racial Categories in Computer Vision”, discuss the societal impacts of biases and uneven representation often found in datasets used for ML [5].\n",
            "> “Privileged, wealthy, industrialized countries in which most AI research is conducted have the benefit of being the primary producers of datasets, and package their racial systems into datasets. Effectively, this is a potentially dangerous form of cultural export, and eventually, the exported racial systems will take on a validity outside of their cultural context, with potentially dangerous consequences.” — Zaid Khan and Yun Fu\n",
            "\n",
            "\n",
            "So it’s good to see the researchers are trying to understand the societal implications of their work, especially as AI seems to be turning the world upside down.\n",
            "Question:         What are some of the societal implications Robert A. Gonsalves discusses in relation to the use of ML systems like StyleCLIP?\n",
            "GPT-4 Answer:     In \"Portraits from the Multiverse\", he discusses how these ML systems can sometimes show signs of cultural biases. For example, the system might render certain ethnicities with stereotypical features, which could be due to a lack of diverse representation in the dataset. He also mentions the societal impacts of biases and uneven representation in datasets used for ML, such as the potential for cultural export of racial systems from privileged, wealthy, industrialized countries that are the primary producers of datasets. This could lead to these exported racial systems taking on a validity outside of their cultural context, with potentially dangerous consequences. He emphasizes the importance of understanding and trying to offset these cultural biases in ML systems.\n",
            "LLaMa 13B Answer: Based on the information provided, he discusses the following societal implications in relation to the use of ML systems like StyleCLIP:\n",
            "\n",
            "1. Cultural biases: The model may learn social biases from the data it is trained on, which can result in unexpected rendering choices, such as Iranian Jennifer Aniston wearing eye shadow and having finely-manicured eyebrows.\n",
            "2. Uneven representation: The datasets used for ML research are often biased towards privileged, wealthy, industrialized countries, which can lead to a lack of representation for other cultures and societies.\n",
            "3. Dangerous consequences: The exported racial systems may take on a validity outside of their cultural context, with potentially dangerous consequences.\n",
            "LLaMa 13B Score:  0.76214784\n",
            "\n",
            "Index:            53\n",
            "Reference:        Acknowledgments\n",
            "I want to thank Jennifer Lim for her help with this article.\n",
            "Question:         What was Robert A. Gonsalves' main purpose in exploring DALL-E for digital art creation?\n",
            "GPT-4 Answer:     His main purpose was to test OpenAI’s text-to-image generator, DALL-E, to see how it would perform in creating images using a variety of subjects and styles.\n",
            "LLaMa 13B Answer: Based on the information provided, he' main purpose in exploring DALL-E for digital art creation was to test OpenAI's text-to-image generator and see how it would perform creating images using a variety of subjects and styles.\n",
            "LLaMa 13B Score:  0.7967231\n",
            "\n",
            "Index:            54\n",
            "Reference:        Source Code and Colabs\n",
            "All source code for this project is available on  GitHub . I released the source code under the  CC BY-SA license .\n",
            "Question:         What is the purpose of the GreenLit project by Robert A. Gonsalves?\n",
            "GPT-4 Answer:     The purpose of the GreenLit project by he is to fine-tune a machine learning model, specifically GPT-J, to create new screenplays for TV shows and movies. This includes generating new titles, plot summaries, and scripts.\n",
            "LLaMa 13B Answer: The purpose of the GreenLit project by he is to use GPT-J with multi-task learning to create new screenplays, including TV shows and movies, with new titles, plot summaries, and scripts.\n",
            "LLaMa 13B Score:  0.90476865\n",
            "\n",
            "Index:            55\n",
            "Reference:        Universal Sentence Encoder\n",
            "After I collected the topics, I used the Universal Sentence Encoder from Google to convert each topic phrase into an embedding, an array of 512 floating-point numbers. Here’s some sample code to show how it works.\n",
            "```\n",
            "import tensorflow_hub as hub embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\") embedding = embed([\"love loss\"]) print(embedding)\n",
            "```\n",
            "The result is here.\n",
            "```\n",
            "tf.Tensor( [[-2.79744454e-02 -6.52119750e-03 1.32698761e-02 4.50092048e-04 9.81796882e-04 3.18628997e-02 2.73146853e-02 -1.10505158e-02 -2.71893758e-02 -5.06720766e-02 -3.20894904e-02 ... -1.08678043e-02 7.85474479e-03 -6.44846493e-03 -3.88006195e-02]], shape=(1, 512), dtype=float32)\n",
            "```\n",
            "Although these numbers won’t mean anything to humans, they represent the concept of “love loss” in the encoder’s multidimensional space. I’ll show you how I used statistics to make sense of the topics in the following sections.\n",
            "Question:         How did Robert A. Gonsalves utilize the Universal Sentence Encoder in his project, Organic Topic Modelling?\n",
            "GPT-4 Answer:     In his project, Organic Topic Modelling, he used the Universal Sentence Encoder from Google to convert each topic phrase into an embedding, which is an array of 512 floating-point numbers. This essentially represents the concept of the topic phrase in the encoder's multidimensional space. For instance, he demonstrated this with the phrase \"love loss\". Although the resulting numbers may not be interpretable to humans, he used statistical methods to make sense of these topics.\n",
            "LLaMa 13B Answer: Sure! Here's the answer:\n",
            "\n",
            "he used the Universal Sentence Encoder to convert each topic phrase into an embedding, which is an array of 512 floating-point numbers.\n",
            "LLaMa 13B Score:  0.7851641\n",
            "\n",
            "Index:            56\n",
            "Reference:        Expanding Images with E-DALL-E\n",
            "Here is the component diagram again, showing only the processes used for expanding the image.\n",
            "The process starts with an input image and a specified aspect ratio. The system repeats the edge pixels to stretch the image, as described above. It then encodes the prompt using CLIP to guide the optimization process. For each iteration, the system encodes the image with CLIP. It uses the difference between the text encoding and the image encoding to modify the VQGAN latent vectors using the Adam Optimizer. After each iteration, the center part of the modified image vectors is replaced with the center of the vectors from the input image to keep the middle part relatively constant. Only the sides will change a lot. After N iterations, the system displays the final image with the new aspect ratio.\n",
            "Here are some results from using E-DALL-E.\n",
            "Pretty good! The system seems to do a good job making up new details in the expanded areas. And it’s hard to see the seams between the original image and the newly generated parts. But you can also see how VQGAN sometimes changes the central part of the image to make the new pieces fit in better. For example, the sky's hue in the top-right image seems to have changed to a deeper blue in the central part. And the texture of the orange triangles in the center of the bottom-right image appears to match the surface of the newly generated triangles on the sides.\n",
            "Question:         How does E-DALL-E maintain the central part of the image relatively constant while expanding the image?\n",
            "GPT-4 Answer:     E-DALL-E maintains the central part of the image relatively constant by replacing the center part of the modified image vectors with the center of the vectors from the input image after each iteration. This ensures that only the sides of the image undergo significant changes.\n",
            "LLaMa 13B Answer: Sure! Here's the answer to your question:\n",
            "\n",
            "E-DALL-E maintains the central part of the image relatively constant while expanding the image by replacing the center of the modified image vectors with the center of the vectors from the input image after each iteration. This ensures that the middle part of the image remains relatively unchanged, while the sides are modified to fit the specified aspect ratio.\n",
            "LLaMa 13B Score:  0.79771197\n",
            "\n",
            "Index:            57\n",
            "Reference:        Using DALL-E\n",
            "The system has three main functions for creating images:\n",
            "Note that functions 2 and 3 work with either generated or uploaded images.\n",
            "Question:         What are the main functions of DALL-E for creating images as described by Robert A. Gonsalves?\n",
            "GPT-4 Answer:     According to him, DALL-E has three main functions for creating images. These functions can work with either generated or uploaded images.\n",
            "LLaMa 13B Answer: Based on the information provided by he, the main functions of DALL-E for creating images are:\n",
            "\n",
            "1. Text-to-image generation: DALL-E can generate images based on text prompts.\n",
            "2. Image-to-image translation: DALL-E can translate an input image into a new style or domain.\n",
            "3. Image completion: DALL-E can complete partially generated images.\n",
            "LLaMa 13B Score:  0.8690743\n",
            "\n",
            "Index:            58\n",
            "Reference:        Comparing the Models for Writing Lyrics\n",
            "After experimenting with the new GPT-4 system, I found a substantial increase in the quality of responses compared to the original GPT-3 model, especially with rhyming words.\n",
            "Question:         What improvements did Robert A. Gonsalves notice in GPT-4 compared to GPT-3 in terms of writing lyrics?\n",
            "GPT-4 Answer:     He noticed a substantial increase in the quality of responses from GPT-4 compared to GPT-3, particularly in its ability to generate rhyming words.\n",
            "LLaMa 13B Answer: Based on the information provided, he noticed an improvement in the quality of responses, specifically with rhyming words, when using the GPT-4 system compared to the original GPT-3 model.\n",
            "LLaMa 13B Score:  0.899224\n",
            "\n",
            "Index:            59\n",
            "Reference:        KeyBERT for Extracting Keywords\n",
            "To get the new model to generate new limericks based on user-supplied topics, I extracted keywords for each limerick in the dataset and used these in the training data. Like in previous projects, I used KeyBert [9] to extract the keywords. Here is the Python code.\n",
            "```\n",
            "from keybert import KeyBERT kw_model = KeyBERT() limerick = \"\"\"For a sound wave to move, be aware That a medium’s needed, like air. In a vacuum no sound Can be heard; they have found Its conduction is nil. (Nothing’s there.)\"\"\" keywords = kw_model.extract_keywords(limerick, keyphrase_ngram_range=(1, 2), stop_words=None) print(keywords[0][0])\n",
            "```\n",
            "The results are “sound wave” for this limerick and “flea market” for the second example shown above.\n",
            "Question:         How did Robert extract keywords for each limerick in the AI Limericks project?\n",
            "GPT-4 Answer:     In the AI Limericks project, he used a tool called KeyBert to extract the keywords for each limerick. He incorporated these keywords into the training data to enable the model to generate new limericks based on user-supplied topics.\n",
            "LLaMa 13B Answer: Sure! Here's the answer to your question:\n",
            "\n",
            "he used KeyBert to extract keywords for each limerick in the AI Limericks project.\n",
            "LLaMa 13B Score:  0.83118296\n",
            "\n",
            "Index:            60\n",
            "Reference:        Overview of Components\n",
            "Here is a brief, high-level overview of the components used in GANscapes. I’ll discuss the details of each component later in the article.\n",
            "I gathered images of Impressionist landscape paintings from WikiArt.org [4] and processed the images to adjust the aspect ratio. I then used the CLIP model [5] from OpenAI to filter the images to keep the “good ones.”\n",
            "I used these images to train StyleGAN2 ADA [6] from NVidia, which has a generator and discriminator network. The generator creates new images starting with random “latent” vectors for form and style and tries to fool the discriminator into thinking the output images are real. Before the real and the generated images are fed into the discriminator, they are modified slightly with the Adaptive Discriminator Augmentation (ADA) module that creates visual diversity in the pictures.\n",
            "I use CLIP again to filter the output images based on a user-supplied text query. And I use the generator again to create an image with a mix of style and form selected by the user. As the final step, I post-process the final images to adust the aspect ratios.\n",
            "Be sure to check out the image gallery in the appendix at the end of the article to see more results.\n",
            "Question:         How does the GANscapes project use the CLIP model from OpenAI?\n",
            "GPT-4 Answer:     The GANscapes project uses the CLIP model from OpenAI in two ways. First, it uses CLIP to filter the initial set of Impressionist landscape paintings, keeping only the \"good ones.\" Later, it uses CLIP again to filter the output images based on a user-supplied text query. This ensures that the final images generated align with the user's desired style and form.\n",
            "LLaMa 13B Answer: Sure! Here's the answer to your question:\n",
            "\n",
            "In the GANscapes project, the CLIP model from OpenAI is used to filter the images before they are fed into the StyleGAN2 ADA generator. The CLIP model is used to select the \"good ones\" among the gathered Impressionist landscape paintings from WikiArt.org, after adjusting the aspect ratio.\n",
            "LLaMa 13B Score:  0.8477734\n",
            "\n",
            "Index:            61\n",
            "Reference:        The MetFaces Dataset\n",
            "NVidia also released the MetFaces dataset of faces from paintings in the Metropolitan Museum of Art. They write that…\n",
            "> [the] dataset consists of 1,336 high-quality PNG images at 1024×1024 resolution. The images were downloaded via the  Metropolitan Museum of Art Collection API , and automatically aligned and cropped using  dlib . Various automatic filters were used to prune the set.\n",
            "\n",
            "\n",
            "Again, NVidia released the dataset under  non-commercial  terms, and they used similarly tight cropping for the faces.\n",
            "Here’s what newly generated images look like with StyleGAN 2 ADA trained on the FFHQ dataset and fine-tuned with the MetFaces dataset.\n",
            "Although the results are impressive, not surprisingly, the results seem to be too tightly cropped. In addition to the datasets, NVidia released the official source code under  non-commercial terms , so these faces cannot be sold as NFTs. Also, there seems to be a distinct lack of cultural diversity in the generated faces.\n",
            "Question:         What is the issue with the faces generated using the MetFaces dataset and StyleGAN 2 ADA according to Robert A. Gonsalves?\n",
            "GPT-4 Answer:     According to him, the faces generated using the MetFaces dataset and StyleGAN 2 ADA are too tightly cropped. Additionally, he notes a distinct lack of cultural diversity in the generated faces.\n",
            "LLaMa 13B Answer: According to he, the issue with the faces generated using the MetFaces dataset and StyleGAN 2 ADA is that they are too tightly cropped and lack cultural diversity.\n",
            "LLaMa 13B Score:  0.9498224\n",
            "\n",
            "Index:            62\n",
            "Reference:        Source Code\n",
            "All source code for this project is available on  GitHub . You can experiment with the code using this  Google Colab . The sources are released under the  CC BY-SA license .\n",
            "Question:         What is the purpose of the InventorBot project by Robert A. Gonsalves?\n",
            "GPT-4 Answer:     The purpose of the InventorBot project by he is to use artificial intelligence to generate new ideas in any field. The AI is trained on the US Patent Database and can produce new and possibly useful inventions.\n",
            "LLaMa 13B Answer: Based on the information provided, the purpose of the InventorBot project by he is to use artificial intelligence to generate new and possibly useful inventions in any field.\n",
            "LLaMa 13B Score:  0.96462685\n",
            "\n",
            "Index:            63\n",
            "Reference:        Source Code and Colabs\n",
            "All source code for this project is available on  GitHub . I released the source code under the  CC BY-SA license .\n",
            "Question:         What is the licensing model for the source code of the Clip-n-Paste project by Robert A. Gonsalves?\n",
            "GPT-4 Answer:     The source code for the Clip-n-Paste project is released under the CC BY-SA license.\n",
            "LLaMa 13B Answer: The source code of the Clip-n-Paste project by he is licensed under the Creative Commons BY-SA license.\n",
            "LLaMa 13B Score:  0.9695341\n",
            "\n",
            "Index:            64\n",
            "Reference:        Finding Source Images\n",
            "As I mentioned in the overview, I collected images for the collages from Wikimedia Commons and the OpenImages dataset. Both sources release their images under the Creative Commons Attribution Sharealike open-source license.\n",
            "To search for images in Wikimedia Commons, I used the API at  https://commons.wikimedia.org/w/api.php .\n",
            "Here is the Python code that sets up the parameters for the search.\n",
            "```\n",
            "s = requests.Session() url = \"https://commons.wikimedia.org/w/api.php\" params = { \"action\": \"query\", \"generator\": \"images\", \"prop\": \"imageinfo\", \"gimlimit\": 500, \"titles\": keyphrases, \"iiprop\": \"url|dimensions\", \"format\": \"json\" } results = s.get(url=url, params=params)\n",
            "```\n",
            "When the results come back, I filter the images based on their dimensions. I keep the images that are 512x512 or greater in size.\n",
            "For searching the pictures in the OpenImages dataset, I use the CLIP Text Encoder to compare the embeddings from the keyphrases to the embeddings from the descriptions in the dataset. If you would like to get more info on this process, you can read the details in my article on searching for design patents  here .\n",
            "Once I get the collection of images, I run them through the CLIP Image Encoder to compare them to the embedding for the entire prompt. Here are the top photos from Wikimedia Commons and OpenImages.\n",
            "Image Segmentation\n",
            "I used an AI model for image segmentation called GRoIE, which stands for Generic Region of Interest Extractor [5]. The system was developed by Leonardo Rossi et al. at the University of Parma in Italy. GRoIE will extract and label any discernable foreground objects given an input image. For each object it finds, it creates a mask image that defines object shape.\n",
            "Note that the GRoIE model often finds and extracts more than one object in each input image. For example, in this photo, it found and isolated both penguins.\n",
            "After I extracted all of the objects, I ran them through CLIP again to compare the image embeddings with the embedding from the prompt. The original image is on top, and the extracted image is below. Here are objects that were cut out of the top five images.\n",
            "Creating a Background Color Gradient\n",
            "I could have composited the image cutouts against a solid color, like white or black, for the photo collages. But to add interest to the background, I wrote a little Python code to create a vertical color gradient with five control points. And then, I use CLIP to analyze the resultant image and iteratively adjust the colors using the Adam optimizer [6] in Pytorch to find the best match for the text prompt.\n",
            "> We propose Adam, a method for efficient stochastic optimization that only requires first-order gradients with little memory requirement. The method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients; the name Adam is derived from adaptive moment estimation. — Diederik Kingma and Jimmy Ba\n",
            "\n",
            "\n",
            "If you don’t speak in maths, the Adam method tweaks the parameters a little and notices if and how much the results improve for each iteration. It changes the parameters at different rates depending on how much each parameter contributes to improving the results.\n",
            "Here is the resulting gradient for “penguins skiing down a snowy mountain” after 100 iterations of using CLIP and Adam on the 25 parameters (5 control points of red, green, and blue.)\n",
            "On the left is a graph of the parameter values and on the right is the resultant gradient image. I’m not sure why it shows purple in the mid-lower region, but it looks fabulous!\n",
            "Here are some additional background gradients using this method.\n",
            "As you can see, creating an image with only 15 parameters can be reasonably expressive as directed by CLIP. The code to generate color gradients from text is  here .\n",
            "Question:         How does Robert A. Gonsalves create a background color gradient for the photo collages in the Clip-n-Paste project?\n",
            "GPT-4 Answer:     In the Clip-n-Paste project, he creates a background color gradient for the photo collages by writing a Python code that creates a vertical color gradient with five control points. He then uses CLIP to analyze the resultant image and iteratively adjusts the colors using the Adam optimizer in Pytorch to find the best match for the text prompt. This process involves tweaking the parameters a little and observing if and how much the results improve for each iteration. The parameters are changed at different rates depending on how much each parameter contributes to improving the results.\n",
            "LLaMa 13B Answer: Sure! Here's the answer to your question:\n",
            "\n",
            "he creates a background color gradient for the photo collages in the Clip-n-Paste project by using an AI model called GRoIE to extract objects from the images, and then using the CLIP image encoder to compare the image embeddings with the embedding from the text prompt. He also uses the Adam optimizer in PyTorch to iteratively adjust the colors based on the first-order gradients of the gradient image, until he achieves the desired background color gradient for the photo collage.\n",
            "LLaMa 13B Score:  0.8908646\n",
            "\n",
            "Index:            65\n",
            "Reference:        Acknowledgments\n",
            "I would like to thank Jennifer Lim, Oliver Strimpel, Mahsa Mesgaran, and Vahid Khorasani   for their help and feedback on this project.\n",
            "Question:         What techniques did Robert A. Gonsalves use to generate improved abstract paintings with AI in the project 'StyleGAN Abstract Art'?\n",
            "GPT-4 Answer:     In the 'StyleGAN Abstract Art' project, he used Adaptive Discriminator Augmentation and Learning Transfer to generate improved abstract paintings with AI.\n",
            "LLaMa 13B Answer: Based on the information provided, he used Adaptive Discriminator Augmentation and Learning Transfer to generate improved abstract paintings with AI in the project \"StyleGAN Abstract Art\".\n",
            "LLaMa 13B Score:  0.9709678\n",
            "\n",
            "Index:            66\n",
            "Reference:        Fourth Line\n",
            "```\n",
            "topic: coffee rhyme-set: today / okay / think / drink / away first_line :  Coffee's a drink for today second_line:  With a splash of fresh cream, it's \"okay\" third_line :  If you do, as I think, fourth lines: ---------------------------------- 1: Add some milk, and you'll drink 2: Add sugar and drink 3: But it's coffee you drink 4:  And you need a strong drink 5: It's a milky drink, 6: That is, if you drink\n",
            "```\n",
            "Question:         What was Robert A. Gonsalves' solution to the slow processing time of the AI he was training to rhyme?\n",
            "GPT-4 Answer:     To address the slow processing time of the AI, he upgraded to Pro on Colab. This allowed for faster training of the AI, although it did come at a cost, as each limerick ended up costing him a dime.\n",
            "LLaMa 13B Answer: Based on the information provided, he upgraded to a Pro version of the AI training software to speed up the processing time.\n",
            "LLaMa 13B Score:  0.70897484\n",
            "\n",
            "Index:            67\n",
            "Reference:        InventorBot: Using AI to Generate New Ideas in Any Field\n",
            "Robert A. Gonsalves\n",
            "Feb 1, 2021\n",
            "InventorBot - https://medium.com/geekculture/inventorbot-using-ai-to-generate-new-ideas-in-any-field-9345f9042df\n",
            "In July 2019, an inventor named Stephen Thaler filed for a patent for a warning beacon that flashes in a special way to get people’s attention. On the patent application form, Thaler listed himself as the applicant and the assignee but not the inventor. Who invented this improved beacon? A better question is  what  invented this improved beacon?\n",
            "The answer is DABUS, which stands for “Device Autonomously Bootstrapping Uniform Sensibility” [1]. This is a neural network that Thaler created and trained to invent things. (Brief side note: the improved beacon, in turn, uses a neural network to attract the attention of humans [2]. DABUS seems to be following a bit of common advice for newbies, “write what you know”. 😃) The US Patent and Trademark Office (USPTO) denied the patent because the inventor is not a person. Here’s what the USPTO said in their decision [3].\n",
            "> To the extent the petitioner argues that an “inventor” could be construed to cover machines, the patent statutes preclude such a broad interpretation. Title 35 of the United States Code consistently refers to inventors as natural persons. For example, 35 U.S.C. § 101 states “ Whoever  invents or discovers any new and useful process, machine, manufacture, or composition of matter…may obtain a patent therefore, subject to the conditions and requirements of this title” (emphasis added). “Whoever” suggests a natural person. —  US Patent and Trademark Office\n",
            "\n",
            "\n",
            "OK, machines can’t currently be listed as inventors on patents. For more information about the legal implications of this decision, check out Andrew Alcantra’s article,  here .\n",
            "But machines  can  invent things. I’ll show you how in this article.\n",
            "Question:         What is the name of the neural network that Stephen Thaler created and trained to invent things?\n",
            "GPT-4 Answer:     The neural network that Stephen Thaler created and trained to invent things is named DABUS, which stands for \"Device Autonomously Bootstrapping Uniform Sensibility\", as mentioned in \"InventorBot: Using AI to Generate New Ideas in Any Field\" by he.\n",
            "LLaMa 13B Answer: Sure! The name of the neural network that Stephen Thaler created and trained to invent things is DABUS, which stands for \"Device Autonomously Bootstrapping Uniform Sensibility.\"\n",
            "LLaMa 13B Score:  0.8454492\n",
            "\n",
            "Index:            68\n",
            "Reference:        Selling NFTs on OpenSea\n",
            "Ever since the digital artist known as Beeple sold a JPG file for US$69 Million back in March 2021, there has been a lot of interest in using a blockchain for selling digital art.\n",
            "> Beeple’s collaged JPG was made, or “minted,” in February as a “nonfungible token,” or NFT. A secure network of computer systems that records the sale on a digital ledger, known as a blockchain, gives buyers proof of authenticity and ownership. Most pay with the Ethereum cryptocurrency. “Everydays” was the first purely digital NFT sold by Christie’s, and it offered to accept payment in Ethereum, another first for the 255-year-old auction house. —  Scott Reyburn,  The New York Times\n",
            "\n",
            "\n",
            "I looked into selling my digital art as NFTs, and after conducting a little research, I found that the OpenSea marketplace using the Polygon blockchain [10] is an excellent way to go. This is for both reasons of cost and environmental impact.\n",
            "OpenSea allows creators to post items for sale on the Ethereum blockchain or the Polygon blockchain, also known as MATIC. Here is a table that shows some of the differences between the Ethereum and Polygon blockchains.\n",
            "```\n",
            "Feature                      Ethereum         Polygon Account initializing:        ~US$50           Free Selling cost:                2.5%             2.5% Selling items in bundles:    Yes              No Blockchain validation:       Proof of work    Proof of stake Environmental impact:        Huge*            Minimal *expected to change in Q1/Q2 2022\n",
            "```\n",
            "As you can see from the table, setting up an account on OpenSea to sell on the Polygon blockchain is free, where it costs about $50 to use the Ethereum blockchain. OpenSea will take 2.5% of the sale price for sales on either blockchain. Also, selling items in bundles is not available on the Polygon blockchain.\n",
            "Question:         What are the differences between selling NFTs on the Ethereum and Polygon blockchains according to Robert A. Gonsalves in his article \"GANshare: Creating and Curating Art with AI for Fun and Profit\"?\n",
            "GPT-4 Answer:     According to he, there are several differences between selling NFTs on the Ethereum and Polygon blockchains. For instance, initializing an account on the Ethereum blockchain costs around US$50, while it's free on the Polygon blockchain. Both blockchains have a selling cost of 2.5%. However, selling items in bundles is only possible on the Ethereum blockchain. In terms of blockchain validation, Ethereum uses proof of work while Polygon uses proof of stake. Lastly, the environmental impact of Ethereum is huge, although this is expected to change in Q1/Q2 2022, while Polygon's environmental impact is minimal.\n",
            "LLaMa 13B Answer: According to he' article \"GANshare: Creating and Curating Art with AI for Fun and Profit,\" the differences between selling NFTs on the Ethereum and Polygon blockchains are as follows:\n",
            "\n",
            "* Initializing an account: Ethereum requires about $50, while Polygon is free.\n",
            "* Selling cost: Both platforms charge 2.5% of the sale price.\n",
            "* Selling items in bundles: Ethereum allows this, while Polygon does not.\n",
            "* Environmental impact: Ethereum has a significant environmental impact, while Polygon has a minimal impact (expected to change in Q1/Q2 2022).\n",
            "LLaMa 13B Score:  0.93161535\n",
            "\n",
            "Index:            69\n",
            "Reference:        Acknowledgments\n",
            "I want to thank Jennifer Lim and Oliver Strimpel for their help with this article.\n",
            "Question:         What is the main purpose of the GANfolk project as described by Robert A. Gonsalves?\n",
            "GPT-4 Answer:     The main purpose of the GANfolk project, as described by he, is to use artificial intelligence, specifically StyleGAN2, VQGAN, and GPT-3, to create portraits of fictional characters from open-source images. These AI-generated portraits are then sold as NFTs (Non-Fungible Tokens).\n",
            "LLaMa 13B Answer: Sure! Based on the information provided, the main purpose of the GANfolk project, as described by he, is to use AI to create portraits of fictional people to sell as NFTs (non-fungible tokens).\n",
            "LLaMa 13B Score:  0.8935088\n",
            "\n",
            "Index:            70\n",
            "Reference:        Training the GAN\n",
            "I ran the training using Google Colab Pro. Using that service I could run for up to 24 hours on a high-end GPU, an NVIDIA Tesla P10 with 16 GB of memory. I also used Google Drive to retain the work in progress between runs. It took about 13 days to train the GAN, sending 5 million source images through the system. Here is a random sample of the results.\n",
            "You can see from the sample of 28 images above that MachineRay produced paintings in a variety of styles, although there are some visual commonalities between them. There are hints to the styles in the source images, but no exact copies.\n",
            "Question:         What platform did Robert A. Gonsalves use to train the MachineRay GAN and how long did it take?\n",
            "GPT-4 Answer:     He used Google Colab Pro to train the MachineRay GAN. It took him about 13 days to train the GAN, during which he sent 5 million source images through the system.\n",
            "LLaMa 13B Answer: Sure! Here's the answer to your question:\n",
            "\n",
            "he used Google Colab Pro to train the MachineRay GAN, and it took approximately 13 days to complete the training process, using a high-end GPU with 16 GB of memory.\n",
            "LLaMa 13B Score:  0.84793365\n",
            "\n",
            "Index:            71\n",
            "Reference:        Searching for Design Patents\n",
            "Now that we have the text and image embeddings for over 67 thousand design patents, running a text search is fast and easy.\n",
            "Searching starts with typing in a query text, for example, “curved water faucet”. The system then creates a text embedding for the query using the CLIP text encoder. You can choose one of the three methods for searching.\n",
            "1. Text to text search —  This compares the embedding of the query to the list of text embeddings for the design patent captions and returns the top result.\n",
            "2. Text to image search —  This compares the query embedding to the list of image embeddings and returns the top results.\n",
            "3. Text to combined text/image search  — This replicates the query embedding and compares the combined pair to a list of combined text and image embeddings and returns the top results.\n",
            "Note that using both text and image embeddings for semantic search is not new. For example, you can read about recent developments in these papers, “Dialog-based Interactive Image Retrieval” by Xiaoxiao Guo et al. [6] and “Visual-Semantic Matching by Exploring High-Order Attention and Distraction” by Yongzhi Li, et al. [7].\n",
            "Question:         What are the three methods of searching that Robert A. Gonsalves describes in his article \"Using OpenAI’s CLIP to Search for Design Patents\"?\n",
            "GPT-4 Answer:     In his article, he describes three methods for searching design patents. The first is a text to text search, which compares the embedding of the query to the list of text embeddings for the design patent captions. The second is a text to image search, which compares the query embedding to the list of image embeddings. The third method is a text to combined text/image search, which replicates the query embedding and compares the combined pair to a list of combined text and image embeddings.\n",
            "LLaMa 13B Answer: Sure! Here's the answer to your question:\n",
            "\n",
            "According to he' article \"Using OpenAI’s CLIP to Search for Design Patents,\" the three methods of searching are:\n",
            "\n",
            "1. Text to text search\n",
            "2. Text to image search\n",
            "3. Text to combined text/image search\n",
            "LLaMa 13B Score:  0.7355027\n",
            "\n",
            "Index:            72\n",
            "Reference:        Acknowledgments\n",
            "I would like to thank Jennifer Lim and Matthew Conroy for their help and feedback on this project.\n",
            "Question:         What is the main topic that Robert A. Gonsalves discusses in his article 'Benford’s Law — A Simple Explanation'?\n",
            "GPT-4 Answer:     In his article, he provides a simple explanation of Benford's Law, also known as the \"First-Digit Law\".\n",
            "LLaMa 13B Answer: Sure! Here's my answer:\n",
            "\n",
            "The main topic of he' article \"Benford's Law - A Simple Explanation\" is Benford's Law, which is a statistical phenomenon that describes the distribution of first digits in numerical data.\n",
            "LLaMa 13B Score:  0.8963489\n",
            "\n",
            "Index:            73\n",
            "Reference:        Discussion and Future Work\n",
            "The MAGnet system works fairly well. It seems to do a better job generating abstract paintings than representational paintings. It converges on a solution fairly quickly, within five generations.\n",
            "Although the results of MAGnet are pretty good, if you play with it, you may see the same visual themes recur occasionally. This may be due to the limited number of source images used during the training (about 4,000 initial images, augmented to over 12,000 via cropping, and filtered to 10,000). Training with more images will probably reduce the frequency of repeated visual themes.\n",
            "Another area that could be improved is the textures within the images. There seem to be some “computery” artifacts in the flat areas. This could be due to the reimplementation of the CUDA operations.\n",
            "I noticed that rosinality is currently working on a new open source project on GitHub called  alias-free-gan-pytorch . Sounds promising!\n",
            "Question:         What are some of the improvements Robert A. Gonsalves suggests for the MAGnet project?\n",
            "GPT-4 Answer:     he suggests that the MAGnet system could be improved by training with more images to reduce the frequency of repeated visual themes. He also mentions that the textures within the images could be enhanced as there seem to be some \"computery\" artifacts in the flat areas, possibly due to the reimplementation of the CUDA operations.\n",
            "LLaMa 13B Answer: Based on the information provided, he suggests the following improvements for the MAGnet project:\n",
            "\n",
            "1. Using more source images to reduce the frequency of repeated visual themes.\n",
            "2. Improving textures within the images to reduce \"computery\" artifacts.\n",
            "3. Using a new open-source project called alias-free-gan-pytorch, which is being developed by rosinality on GitHub.\n",
            "LLaMa 13B Score:  0.7806953\n",
            "\n",
            "Index:            74\n",
            "Reference:        References\n",
            "[1] J. Schulman et al.,  ChatGPT: Optimizing Language Models for Dialogue  (2020)\n",
            "\n",
            "[2] T. Brown et al.,  Language Models are Few-Shot Learners  (2020)\n",
            "\n",
            "[3] L. Ouyang et al.,  Training language models to follow instructions with human feedback  (2020)\n",
            "\n",
            "[4] J. Schulman et al.,  Proximal Policy Optimization Algorithms  (2017)\n",
            "Question:         What is the main focus of Robert A. Gonsalves' article \"Using ChatGPT as a Creative Writing Partner — Part 1: Prose\"?\n",
            "GPT-4 Answer:     The main focus of his article is to explore how the latest language model from OpenAI, known as ChatGPT, can be utilized as a tool for creative writing, specifically in the genres of poetry, fiction, and screenplays.\n",
            "LLaMa 13B Answer: Based on the information provided, the main focus of he' article \"Using ChatGPT as a Creative Writing Partner — Part 1: Prose\" is to explore how the latest language model from OpenAI, called ChatGPT, can be used as a creative writing partner for poetry, fiction, and screenplays.\n",
            "LLaMa 13B Score:  0.93404776\n",
            "\n",
            "Index:            75\n",
            "Reference:        Upsizing Options\n",
            "Midjourney also has options for upsizing that can improve the details of the final images. For this test, I used the prompt, “steampunk rotary phone,” with the quality set to 2.0. I had to use the “seed” trick to test out the three upsizing options, lite, regular, and beta. (For the record, this was seed #24863.) Here are the results, along with a close-up of the rotary dialer.\n",
            "All three images show some fine details, but the beta upsize on the right seems a bit more orderly. For example, the inner ring on the dialer seems to be more realistically formed.\n",
            "Question:         What were the results of Robert's test on the three upsizing options in Midjourney V4?\n",
            "GPT-4 Answer:     he found that all three upsizing options - lite, regular, and beta - in Midjourney V4 showed fine details in the final images. However, he noted that the beta upsize seemed a bit more orderly, with elements like the inner ring on the dialer appearing more realistically formed.\n",
            "LLaMa 13B Answer: Based on the information provided, he's test results showed that all three upsizing options (lite, regular, and beta) produced images with fine details, but the beta upsize on the right appeared to have more realistic formation of the inner ring on the steampunk rotary phone.\n",
            "LLaMa 13B Score:  0.77655256\n",
            "\n",
            "Index:            76\n",
            "Reference:        More on ChatGPT as a Creative Writing Partner\n",
            "Here are the first two articles in this series.\n",
            "Question:         What is the focus of Robert A. Gonsalves' article \"Using ChatGPT as a Creative Writing Partner — Part 3: Picture Books\"?\n",
            "GPT-4 Answer:     In his article, he discusses how the latest language model from OpenAI, known as ChatGPT, can be used as a tool for writing children's books and creating illustrations with Midjourney.\n",
            "LLaMa 13B Answer: Sure! Here's my brief answer to your question:\n",
            "\n",
            "The focus of he' article \"Using ChatGPT as a Creative Writing Partner — Part 3: Picture Books\" is on how the latest language model from OpenAI, ChatGPT, can be used as a creative writing partner for children's books and create illustrations with Midjourney.\n",
            "LLaMa 13B Score:  0.8255923\n",
            "\n",
            "Index:            77\n",
            "Reference:        Prompts that Don’t Work\n",
            "Prompts that involve people (i.e., “children playing,” “Barack Obama,” “The Mona Lisa,” etc.) do not work because of the intentional lack of people in the training data for GLIDE.\n",
            "Prompts of abstract concepts (i.e., “freedom,” “a brand new day,” “haywire,” etc.) also do not work because of a lack of consensus on the content of images on the Internet that are labeled using these words.\n",
            "Here’s a question that defines a rule of thumb for using BIG.art: When you do a Google search of the prompt and look at the resulting images, do they look roughly similar? If so, generating an image using that prompt will generate good images. Oh, and don’t try to render people. It won’t work.\n",
            "Question:         What is the rule of thumb for using BIG.art as suggested by Robert A. Gonsalves?\n",
            "GPT-4 Answer:     According to him, a good rule of thumb for using BIG.art is to do a Google search of the prompt and look at the resulting images. If they look roughly similar, then generating an image using that prompt will likely produce good results. However, he advises against trying to render people, as it won't work due to the intentional lack of people in the training data for GLIDE.\n",
            "LLaMa 13B Answer: According to he, the rule of thumb for using BIG.art is to search for images on Google using the prompt and check if the resulting images look roughly similar. If they do, then generating an image using that prompt will likely produce good results. However, it is not recommended to try rendering people as it will not work.\n",
            "LLaMa 13B Score:  0.80847937\n",
            "\n",
            "Index:            78\n",
            "Reference:        Rendering Options\n",
            "As I mentioned above, there are a lot of rendering options in Midjourney. To run an apples-to-apples test, however, I learned how to retrieve and use the random seed used to generate images. It’s a bit hidden, but after I generated an image, I created a “reaction” to my image and then chose the “envelope” emoji.\n",
            "This triggered the bot to send me a message that included the seed for the run, like this.\n",
            "I then used the  --seed 6534  parameter to generate the same image with different options. For example, here are renderings using the prompt, “painting of the Boston Public Garden in springtime,” with the quality set to 0.5, 1.0 (the default), and 2.0.\n",
            "It’s interesting to see how much the image changed with the different quality selections. The yellow tree in the middle roughly stayed the same, but the flowers, buildings, and people all moved around. It’s subtle, but the image on the right with a quality of 2.0 seems to have the fewest “problem” areas.\n",
            "Question:         How does Robert retrieve the random seed used to generate images in Midjourney Art?\n",
            "GPT-4 Answer:     In Midjourney Art, he retrieves the random seed used to generate images by first creating an image. After the image is generated, he creates a \"reaction\" to his image and chooses the \"envelope\" emoji. This action triggers the bot to send him a message that includes the seed for the run. He then uses the --seed parameter to generate the same image with different options.\n",
            "LLaMa 13B Answer: Sure! Here's the answer to your question:\n",
            "\n",
            "To retrieve the random seed used to generate images in Midjourney Art, he created a \"reaction\" to his image and then chose the \"envelope\" emoji. This triggered the bot to send him a message that included the seed for the run.\n",
            "LLaMa 13B Score:  0.9127058\n",
            "\n",
            "Index:            79\n",
            "Reference:        Acknowledgments\n",
            "I want to thank Kate Li for singing the song for this article. I also thank Pete Fayan from the Agents for his engagement with this project. Finally, I thank Jennifer Lim for reviewing the article and providing feedback.\n",
            "Question:         What is the main focus of Robert A. Gonsalves' article \"Writing Songs with GPT-4: Part 3, Melodies\"?\n",
            "GPT-4 Answer:     The main focus of his article is on how to use the latest language model from OpenAI, GPT-4, to assist in writing melodies for new songs.\n",
            "LLaMa 13B Answer: Based on the information provided, the main focus of he' article \"Writing Songs with GPT-4: Part 3, Melodies\" is using the latest language model from OpenAI to help write melodies for new songs.\n",
            "LLaMa 13B Score:  0.9163296\n",
            "\n",
            "Index:            80\n",
            "Reference:        GANfolk System Components\n",
            "I will discuss the details of the components and processes used in the GANfolk system in the following sections.\n",
            "Question:         What is the main purpose of the GANfolk system as described by Robert A. Gonsalves?\n",
            "GPT-4 Answer:     The main purpose of the GANfolk system, as described by him, is to use artificial intelligence to create portraits of fictional people. These AI-generated portraits are then sold as NFTs (Non-Fungible Tokens). The system utilizes StyleGAN2, VQGAN, and GPT-3 to synthesize diverse characters from open-source images.\n",
            "LLaMa 13B Answer: Based on the information provided, the main purpose of the GANfolk system, as described by he, is to use AI to create portraits of fictional people to sell as NFTs (non-fungible tokens).\n",
            "LLaMa 13B Score:  0.97305226\n",
            "\n",
            "Index:            81\n",
            "Reference:        Training the System\n",
            "I trained the system using Google Colab. It took about four days to run. The system has an Nvidia Tesla V100 GPU which can run up to 14 teraFLOPS (14 trillion floating-point operations per second). So it took about 4.6 exaFLOPS to train (about 4.6 quintillion floating-point operations). That’s a lot of math.\n",
            "Here’s the shell command I used to kick things off.\n",
            "```\n",
            "python stylegan2-ada/train.py --aug=ada --target=0.7 \\ --mirror=1 --snap=1 --gpus=1 \\ --data='/content/drive/MyDrive/datasets/paintings' \\ --outdir='/content/drive/MyDrive/results_stylegan2_ada'\n",
            "```\n",
            "Setting the  aug  parameter to  ada  enables the Adaptive Discriminator Augmentation to kick in when needed. Setting the  target  parameter to  0.7  prevents  p  from going over 0.7, which maximizes the dynamic augmentation without leaking any further augmentation into the final images.\n",
            "Here’s a graph of the training results over time. I am using the Fréchet Inception Distance (FID) as a metric for image quality and diversity, where a lower score is better [2]. You can read about the FID score in Cecelia Shao’s post  here .\n",
            "You can see how StyleGAN2 ADA outperforms the original StyleGAN2 for the same number of iterations. The FID score for SG2A bottomed out at just over 100 after about 300 iterations. Here are some samples of the results.\n",
            "The GAN seems to produce a nice variety of abstract paintings with interesting compositions with varied color schemes. But there is room for improvement.\n",
            "Question:         What metric did Robert A. Gonsalves use to measure the quality and diversity of the images produced by StyleGAN2 ADA?\n",
            "GPT-4 Answer:     He used the Fréchet Inception Distance (FID) as a metric to measure the quality and diversity of the images produced by StyleGAN2 ADA. A lower FID score indicates better image quality and diversity.\n",
            "LLaMa 13B Answer: Based on the information provided, he used the Fréchet Inception Distance (FID) as a metric to measure the quality and diversity of the images produced by StyleGAN2 ADA.\n",
            "LLaMa 13B Score:  0.9783915\n",
            "\n",
            "Index:            82\n",
            "Reference:        Creating Abstract Art with StyleGAN2 ADA\n",
            "Robert A. Gonsalves\n",
            "Jan 1, 2021\n",
            "StyleGAN Abstract Art - https://medium.com/towards-data-science/creating-abstract-art-with-stylegan2-ada-ea3676396ffb\n",
            "Back in August 2020, I created a project called  MachineRay  that uses Nvidia’s StyleGAN2 to create new abstract artwork based on early 20th century paintings that are in the public domain. Since then, Nvidia has released a new version of their AI model, StyleGAN2 ADA, that is designed to yield better results when generating images from a limited dataset [1]. (I’m not sure why they didn't call it StyleGAN3, but I’ll refer to the new model as SG2A to save a few characters). In this article, I’ll show you how I used SG2A to create better looking abstract paintings.\n",
            "Question:         What is the new version of Nvidia's AI model that Robert A. Gonsalves used in his project to create better looking abstract paintings?\n",
            "GPT-4 Answer:     He used the new version of Nvidia's AI model called StyleGAN2 ADA (SG2A) in his project to generate improved abstract artwork.\n",
            "LLaMa 13B Answer: Sure! Here's the answer to your question:\n",
            "\n",
            "The new version of Nvidia's AI model that he used in his project is called StyleGAN2 ADA (SG2A).\n",
            "LLaMa 13B Score:  0.8124906\n",
            "\n",
            "Index:            83\n",
            "Reference:        Societal Bias in Language Models\n",
            "Note that, like most large language models, InstructGPT and ChatGTP both suffer from exposure to implicit social bias and toxicity in the original training data. To combat this, OpenAI actively worked to “align” the models to be helpful, honest, and harmless [3].\n",
            "> We make progress on aligning language models by training them to act in accordance with the user’s intention. This encompasses both explicit intentions such as following instructions and implicit intentions such as staying truthful, and not being biased, toxic, or otherwise harmful. — Long Ouyang et al. from OpenAI\n",
            "\n",
            "\n",
            "However, the authors state that the new model shows only small improvements in toxicity over GPT-3, but no improvement in bias.\n",
            "Question:         What does Robert A. Gonsalves say about the improvements in toxicity and bias in the new model compared to GPT-3?\n",
            "GPT-4 Answer:     According to him, the new model shows only minor improvements in toxicity over GPT-3, but no improvement in bias.\n",
            "LLaMa 13B Answer: According to he, the new model shows small improvements in toxicity over GPT-3, but no improvement in bias.\n",
            "LLaMa 13B Score:  0.9821781\n",
            "\n",
            "Index:            84\n",
            "Reference:        References\n",
            "[1] Netflix, Connected, 20200,  https://www.netflix.com/title/81031737\n",
            "\n",
            "[2] Scott, P. and Fasli, M., “CSM-349 — Benford’s Law: An Empirical Investigation and a Novel Explanation”, 2001,  http://repository.essex.ac.uk/8664/1/CSM-349.pdf\n",
            "\n",
            "[3] Berger, A., Hill, T.P., “A basic theory of Benford’s Law”,  Probability Surveys , 2011,  https://projecteuclid.org/download/pdfview_1/euclid.ps/1311860830\n",
            "\n",
            "[4] Kossovsky A. E., “Arithmetical Tugs of War and Benford’s Law”, 2014,  https://arxiv.org/ftp/arxiv/papers/1410/1410.2174.pdf\n",
            "\n",
            "[5]Benford, F. “The Law of anomalous numbers”,  Proceedings of the American Philosophical Society , 78, 551–572, 1938,  https://mdporter.github.io/SYS6018/other/(Benford) The Law of Anomalous Numbers.pdf\n",
            "[6] Newcomb, S., “Note on the frequency of use of different digits in natural numbers”, American Journal of Math. 4, 39–40, 1881\n",
            "\n",
            "[7] Stigler S., “Stigler’s Law of Eponymy”, 1980,  https://archive.org/details/sciencesocialstr0039unse/page/147/mode/2up\n",
            "\n",
            "[8] Fewster, R.M., “A Simple Explanation of Benford’s Law”,  The American Statistician , Vol. 63, No 1, 2009,  https://www.stat.auckland.ac.nz/~fewster/RFewster_Benford.pdf\n",
            "\n",
            "[9] US Census Data, 2010,  https://www2.census.gov\n",
            "\n",
            "[10] Decker, E. H., Kerkhoff, A. J., & Moses, M. E. (2007). Global patterns of city size distributions and their fundamental drivers.  PloS one ,  2 (9), e934.  https://doi.org/10.1371/journal.pone.0000934\n",
            "\n",
            "[11] Nigrini, M.J., “I’ve Got Your Number”,  Journal of Accountancy , 1999,  https://www.journalofaccountancy.com/issues/1999/may/nigrini.html\n",
            "\n",
            "[12] The state of Oklahoma, “Oklahoma’s Open Data”,  https://data.ok.gov , 2019\n",
            "\n",
            "[13] Durtschi, C., Hillison, W. Pacini C., “The effective use of Benford’s law to assist in detecting fraud in accounting data”,  Journal of Forensic Accounting , 2004,  http://www.agacgfm.org/AGA/FraudToolkit/documents/BenfordsLaw.pdf\n",
            "\n",
            "[14] New York State, Waterbody Classifications, 2019,  https://data.ny.gov/Energy-Environment/Waterbody-Classifications/8xz8-5u5u\n",
            "\n",
            "[15] Kossovsky A. E., “Towards A Better Understanding of the Leading Digits Phenomena”, 2006,  https://arxiv.org/ftp/math/papers/0612/0612627.pdf\n",
            "\n",
            "[16] Rind, D., “Do Variations in the Solar Cycle Affect Our Climate System?”  Science Briefs , Goddard Institute for Space Studies, NASA, 2009,  https://www.giss.nasa.gov/research/briefs/rind_03/\n",
            "\n",
            "[17] Sambridge S., Tkalčić H., and Jackson A., “Benford’s law in the natural sciences”,  Geophysical Research Letters , Vol. 37, L22301, 2010,  https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1029/2010GL044830\n",
            "Question:         What is the main topic that Robert A. Gonsalves discusses in his article 'Benford’s Law — A Simple Explanation'?\n",
            "GPT-4 Answer:     In his article, he provides a simple explanation of Benford's Law, a mathematical phenomenon also known as the \"First-Digit Law\". He discusses its history, its applications, and some of the theories that have been proposed to explain why it holds true in many real-world data sets.\n",
            "LLaMa 13B Answer: Based on the information provided, the main topic that he discusses in his article \"Benford's Law - A Simple Explanation\" is Benford's Law, which is a phenomenon where the leading digits of numerical data follow a specific pattern.\n",
            "LLaMa 13B Score:  0.8120009\n",
            "\n",
            "Index:            85\n",
            "Reference:        Using ChatGPT as a Creative Writing Partner — Part 3: Picture Books\n",
            "Robert A. Gonsalves\n",
            "Feb 7, 2023\n",
            "ChatGPT Pictures - https://medium.com/towards-data-science/using-chatgpt-as-a-creative-writing-partner-part-3-picture-books-4f45e5dfe8dd\n",
            "This is the third and final installment of my series of three articles on using OpenAI’s ChatGPT language model [1] for creative writing. In the  first article , I described using ChatGPT for writing prose, poetry, novels, and screenplays. In the  second article , I showed how to use the system to create chord sequences to compose music in various styles.\n",
            "My latest experiment for this article was creating a picture book using ChatGPT. Because the system can’t render images directly, I asked it to describe the scenes which I rendered using Midjourney [2], a text-to-image creation system that I explored in an  earlier article .\n",
            "Question:         What was Robert A. Gonsalves' latest experiment in the article \"Using ChatGPT as a Creative Writing Partner — Part 3: Picture Books\"?\n",
            "GPT-4 Answer:     In this article, his latest experiment involved creating a picture book using ChatGPT. He asked the system to describe scenes, which he then rendered using Midjourney, a text-to-image creation system.\n",
            "LLaMa 13B Answer: Sure! Here is the answer to your question:\n",
            "\n",
            "he' latest experiment in the article \"Using ChatGPT as a Creative Writing Partner — Part 3: Picture Books\" was creating a picture book using ChatGPT, where he asked the system to describe the scenes which he then rendered using Midjourney, a text-to-image creation system.\n",
            "LLaMa 13B Score:  0.8181085\n",
            "\n",
            "Index:            86\n",
            "Reference:        A Tale of Two GANs\n",
            "I used two GANs for this project, an independent implementation of StyleGAN2 by Kim Seonghyeon (aka rosinality) and VQGAN by Patrick Esser et al. I trained both GANs for three weeks using the 5,400 training images in the GANfolk dataset.\n",
            "In my prior research, I found that StyleGAN2 does an excellent job creating a global structure in the generated images that roughly resembles the types in the training data. However, the image details are often hazy or missing. But VQGAN is entirely complementary. It doesn’t know how to create a global structure, but it does a good job filling in realistic image details. Using both GANs is the best of both worlds.\n",
            "Question:         What are the strengths of the two GANs, StyleGAN2 and VQGAN, that Robert used in the GANfolk project?\n",
            "GPT-4 Answer:     In the GANfolk project, he found that StyleGAN2 excelled at creating a global structure in the generated images that closely resembled the types in the training data, but it often produced hazy or missing image details. On the other hand, VQGAN was proficient at filling in realistic image details, but it struggled to create a global structure. Therefore, he used both GANs to leverage their strengths and compensate for their weaknesses.\n",
            "LLaMa 13B Answer: Sure! Here's the answer to your question:\n",
            "\n",
            "The two GANs, StyleGAN2 and VQGAN, have the following strengths as used by he in the GANfolk project:\n",
            "\n",
            "* StyleGAN2: creates a global structure in the generated images that roughly resembles the types in the training data, but the image details are often hazy or missing.\n",
            "* VQGAN: does a good job filling in realistic image details, but doesn't know how to create a global structure.\n",
            "LLaMa 13B Score:  0.80701375\n",
            "\n",
            "Index:            87\n",
            "Reference:        Got Writer’s Block? It’s GPT-2 to the Rescue!\n",
            "Robert A. Gonsalves\n",
            "Sep 1, 2020\n",
            "PlotJam - https://medium.com/towards-data-science/got-writers-block-it-s-plotjam-to-the-rescue-e555db9f3272\n",
            "In this post, I’ll show you how Artificial Intelligence (AI) and Machine Learning (ML) can be used to help you get a start on that novel you always wanted to write. I’ll begin with a brief background on how computers process text using AI. Then I’ll describe how I set up an ML model called GPT-2 to generate new plot summaries, and give instructions on how you can create some new story ideas for yourself.\n",
            "This is the second part of my series of articles on how AI can be used for creative endeavors. The first part is on how to use ML to create abstract art, available  here .\n",
            "Question:         What is the main purpose of Robert A. Gonsalves' project, PlotJam?\n",
            "GPT-4 Answer:     The main purpose of PlotJam, as explained by he, is to use AI and Machine Learning to help writers overcome writer's block. He uses an ML model called GPT-2 to generate new plot summaries, providing a starting point for writers to create new stories.\n",
            "LLaMa 13B Answer: Based on the information provided, the main purpose of he' project, PlotJam, is to use Artificial Intelligence (AI) and Machine Learning (ML) to help writers overcome writer's block and generate new plot summaries for their stories.\n",
            "LLaMa 13B Score:  0.8400823\n",
            "\n",
            "Index:            88\n",
            "Reference:        Ownership of Inputs and Generated Media\n",
            "Midjourney and RunwayML have different policies regarding the imagery and text used for prompts and the resulting generated images. Midjourney distinguishes between paid and free users, whereas RunwayML uses the same policy for both types of users.\n",
            "Question:         What was Robert A. Gonsalves' main objective in the Muybridge Derby project?\n",
            "GPT-4 Answer:     His main objective was to use Midjourney and RunwayML to transform Eadweard Muybridge’s photo sequences into high-resolution videos.\n",
            "LLaMa 13B Answer: Based on the information provided, he' main objective in the Muybridge Derby project was to transform Eadweard Muybridge's photo sequences into high-resolution videos using Midjourney and RunwayML.\n",
            "LLaMa 13B Score:  0.8296366\n",
            "\n",
            "Index:            89\n",
            "Reference:        Using ChatGPT as a Creative Writing Partner — Part 3: Picture Books\n",
            "towardsdatascience.com\n",
            "Question:         What is the main focus of Robert A. Gonsalves' article \"Using ChatGPT as a Creative Writing Partner — Part 2: Music\"?\n",
            "GPT-4 Answer:     In his article, he explores how the latest language model from OpenAI, ChatGPT, can be utilized to compose chords for new songs. He also discusses the use of music by Band-in-a-Box in this process.\n",
            "LLaMa 13B Answer: Based on the provided information, the main focus of he' article \"Using ChatGPT as a Creative Writing Partner — Part 2: Music\" is using ChatGPT as a tool to assist with composing chords for new songs.\n",
            "LLaMa 13B Score:  0.6582049\n",
            "\n",
            "Index:            90\n",
            "Reference:        PyTextRank\n",
            "The system uses an algorithm called TextRank to extract keywords and phrases from the user’s question[5]. It’s based on the famous PageRank algorithm that Larry Page, et. al, invented for web searches. You pass in a block of text and it returns key phrases ranked by relevance. Here is some sample code that shows how it can extract keywords from a question.\n",
            "```\n",
            "Question:  Can a computer beat a grandmaster chess player? Keywords:  a grandmaster, chess player, a computer\n",
            "```\n",
            "You can see that it does a good job of extracting the key phrases.\n",
            "Question:         How does the AI 8-Ball extract keywords from a user's question?\n",
            "GPT-4 Answer:     The AI 8-Ball uses an algorithm called TextRank to extract keywords and phrases from the user's question. This algorithm is based on the PageRank algorithm used for web searches. It takes a block of text and returns key phrases ranked by relevance. For example, from the question \"Can a computer beat a grandmaster chess player?\", it extracts the key phrases \"a grandmaster\", \"chess player\", and \"a computer\".\n",
            "LLaMa 13B Answer: Sure! The AI 8-Ball uses an algorithm called TextRank to extract keywords and phrases from a user's question. This algorithm ranks the relevance of key phrases based on their importance in the text.\n",
            "LLaMa 13B Score:  0.87786615\n",
            "\n",
            "Index:            91\n",
            "Reference:        Exploring the Versions\n",
            "Midjourney released its first version of the image-generating service in March 2022. Version 2 was released soon after in April, and version 3 in July. The beta for version 4 was released on November 10, 2022.\n",
            "As mentioned above, you can use the  /settings  command to select versions 1 through 4. I noticed that version 4 creates 512x512 initial renderings, whereas the previous versions created initial images at 256x256. To test all four versions, I sent in the prompt “painting of a French man wearing a hat drinking wine at an outdoor cafe” to see what I would get.\n",
            "The four thumbnails in the upper left were rendered using version 1, version 2 at the upper right, version 3 at the lower left, and version 4 at the lower right. Wow! What a big difference in quality! With each version, the images steadily improve. The version 4 images look extraordinary! In the next section, I’ll show some quantitative results.\n",
            "Question:         What significant difference did Robert A. Gonsalves notice between the fourth and previous versions of Midjourney Art?\n",
            "GPT-4 Answer:     he noticed that the fourth version of Midjourney Art creates initial renderings at a resolution of 512x512, unlike the previous versions which created images at 256x256. Additionally, he observed a significant improvement in the quality of images generated by the fourth version compared to its predecessors.\n",
            "LLaMa 13B Answer: Based on the information provided, he noticed a significant difference in quality between the fourth version of Midjourney Art and the previous versions. The images created with version 4 were described as \"extraordinary\" and showed a steady improvement in quality compared to the earlier versions.\n",
            "LLaMa 13B Score:  0.8872117\n",
            "\n",
            "Index:            92\n",
            "Reference:        a wood workshop\n",
            "Question:         What machine learning tools does Robert A. Gonsalves suggest using to create high-resolution digital paintings?\n",
            "GPT-4 Answer:     he suggests using GLIDE and BSRGAN to create ultra-high-resolution digital paintings with fine details.\n",
            "LLaMa 13B Answer: Based on the information provided, he suggests using GLIDE and BSRGAN to create ultra-high-resolution digital paintings with fine details.\n",
            "LLaMa 13B Score:  0.9759015\n",
            "\n",
            "Index:            93\n",
            "Reference:        The Agents: License to Trill\n",
            "My third and final project was writing a song in the style of the ska band, The Agents. I started by having the system analyze the Agent’s song “ Grow .”\n",
            "> RobG: Analyze the lyrics and chords of the song “Grow” by the ska band The Agents.\n",
            "> [Verse 1] Arm at D, the very first day I met you girl The very first time I held your hand The very first time I looked into your eyes I told you that I loved you\n",
            "> [Verse 2] From that day till ever after The troubling times, the sorrow, the laughter Hand in hand, now we walk together Can’t imagine anything better now\n",
            "> [Chorus] Grow, let your love grow, grow Let your love come, rise up, rise up Plant the seed and let your love grow Higher and higher\n",
            "> GPT-4 “Grow” by the ska band The Agents is a love song that reflects on the various stages and moments of a romantic relationship. The lyrics portray the initial feelings of love, the growth of the relationship, and the deepening bond between the two people. The song’s overall message is to nurture love and let it grow to its fullest potential.\n",
            "> The lyrics emphasize the importance of love and companionship, as seen in lines like “The very first day I met you girl,” “Hand in hand, now we walk together,” and “Can’t imagine anything better now.” The repetition of the phrase “Grow, let your love grow, grow” highlights the song’s central theme of growth and development in love.\n",
            "\n",
            "\n",
            "The system produced an excellent and concise analysis of the song. It picked up the song's central theme as being about the growth of a relationship and pulled out a few key lines in the lyrics to emphasize this.\n",
            "Unlike the first two experiments described above, this time I gave GPT-4 a new theme and asked it to write five possible song titles with descriptions.\n",
            "> RobG: Create the titles for five songs in this style about overcoming fears. Include a one-sentence description of the meaning of each song.\n",
            "> GPT-4:  “Break Through the Shadows” This song is about finding inner strength and courage to step out of one’s comfort zone and face the unknown.\n",
            "> “Fearless Heartbeat” A powerful anthem celebrating the resilience of the human spirit, encouraging listeners to embrace their fears and turn them into a driving force for personal growth.\n",
            "> “Rising Above the Storm” This song tells the story of overcoming adversity and challenges, illustrating how facing one’s fears can lead to newfound strength and self-discovery.\n",
            "> “Unchained Courage” A motivational tune that highlights the importance of breaking free from the chains of fear and self-doubt, urging individuals to pursue their dreams with relentless determination.\n",
            "> “Dance with the Fear” An upbeat track that encourages listeners to accept and acknowledge their fears, using them as a catalyst for personal growth and transformation.\n",
            "\n",
            "\n",
            "These were all good, although they read like motivational posters. But overall, I thought that “Dance with the Fear” was the best of the batch. And I know that ska is an excellent style of music for dancing.\n",
            "As I did in the examples above, I asked GPT-4 to write the lyrics for the new song.\n",
            "> RobG: Write the lyrics for “Dance with the Fear.”\n",
            "> GPT-4:  [Verse 1] Waking up to a brand new day, Feeling the fear but not turning away, Facing the unknown, that’s where we’ll start, With an open mind and a fearless heart.\n",
            "> [Pre-Chorus] No more hiding, no more doubts, It’s time to let our courage out.\n",
            "> [Chorus] Dance with the fear, don’t run away, Embrace the challenge, come what may. Find your rhythm, face the unknown, With every step, your strength has grown.\n",
            "> [Verse 2] In the darkness, we’ll find the light, Guided by dreams that take flight. No more trembling, no more fright, Together we’ll soar to new heights.\n",
            "> [Pre-Chorus] … [Chorus] …\n",
            "> [Bridge] When fear is knocking at your door, Stand tall, and face it once more. The journey’s long, but you’ll find, You’re stronger than you realized.\n",
            "> [Chorus] …\n",
            "> [Outro] So dance with the fear, let courage lead, Together we’ll conquer, together we’ll succeed. Take the leap, and you’ll see, You’ll find your strength, and you’ll be free.\n",
            "\n",
            "\n",
            "Wow, this was very nice! It’s a little corny as it’s chock full of motivational decrees. And, once again, it rhymed all the couplets, which is fine for the song. I noticed the last couplet in the bridge didn’t exactly rhyme.\n",
            "> The journey’s long, but you’ll  find , You’re stronger than you  realized .\n",
            "\n",
            "\n",
            "So I asked GPT-4 to see if it could fix it.\n",
            "> RobG: Rewrite the last two lines of the bridge so the words rhyme.\n",
            "> GPT-4:  The journey’s long, but keep in  mind , You’re stronger than the fears you’ll  find .\n",
            "\n",
            "\n",
            "That was a nice, creative fix. If you don’t like something it wrote, give it some constructive feedback; it usually does an excellent job of making positive changes. This aligns with my view of using AI systems as a writing partner.\n",
            "Question:         How did Robert use GPT-4 to create song lyrics in the style of the ska band, The Agents?\n",
            "GPT-4 Answer:     he started by having GPT-4 analyze the lyrics and chords of the song “Grow” by The Agents. He then gave GPT-4 a new theme about overcoming fears and asked it to create five possible song titles with descriptions. After selecting the title \"Dance with the Fear\", he asked GPT-4 to write the lyrics for the new song. When he noticed a couplet that didn't rhyme, he asked GPT-4 to rewrite it, which it did successfully. This process demonstrated how he used GPT-4 as a writing partner in creating song lyrics.\n",
            "LLaMa 13B Answer: he used GPT-4 to create song lyrics in the style of the ska band, The Agents, by providing the system with a theme and asking it to write five possible song titles with descriptions. He then asked the system to analyze the lyrics and chords of one of the band's songs, \"Grow,\" and provided feedback on the lyrics it generated for a new song in the style of The Agents, \"Dance with the Fear.\" GPT-4 was able to provide an excellent analysis of the original song and generate new lyrics that fit the desired theme.\n",
            "LLaMa 13B Score:  0.86288047\n",
            "\n",
            "Index:            94\n",
            "Reference:        Overview\n",
            "Below is a diagram of the components and processes I used to train and run Deep Haiku. After a quick discussion of what each part does, I’ll get into things in more detail.\n",
            "I started by downloading two datasets of Haikus from Kaggle.com from users hjhalani30 and bfbarry. The datasets were released under the CC0 and CC-BY licenses, respectively. The number of Haikus in the combined datasets is over 140K. I used FastPunct to add punctuation and casing to the Haikus and ran the KeyBERT model [3] to extract phrases used as prompts.\n",
            "I then filtered the data by using the GRUEN metric to gauge the quality of the text [4] and the phoenemizer library [5] to both count the syllables and convert the prompts and Haikus into phonemes. The filtering yielded over 26K relatively high-quality Haikus that all have a [5, 7, 5] meter.\n",
            "I used the GPT-J 6B model from Eluther [6] as the basis for Deep Haiku. After quantizing the model down to 8-bits to run on a Google Colab, I fine-tuned it using the filtered Haikus as training data for ten epochs, which took 11 hours on Google Colab with a Tesla V100 GPU.\n",
            "Generating new Haikus starts with selecting a word or phrase as a prompt, like “autumn.” I use the fine-tuned model to create 20 candidate Haikus. The results are filtered for adherence to the meter and optionally filtered to remove candidates that contain explicit language using the Detoxify library. Yes, Deep Haiku knows how to swear. The remaining candidates are then displayed along with the scores.\n",
            "For example, I generated 20 Haikus generated with the prompt “autumn,” and 11 used the [5, 7, 5] meter. Here are the filtered results with the scores.\n",
            "OK, the top 3 look pretty good, albeit a tad corny. But none of them used swear words, so the toxicity was near zero for all. The only slight blip was the sixth one that mentioned the “end of days” and “the cusp of death.” A little dark for a Haiku about autumn, but not exactly offensive.\n",
            "Question:         How did Robert A. Gonsalves train the Deep Haiku model?\n",
            "GPT-4 Answer:     he started by downloading two datasets of Haikus from Kaggle.com. He then used FastPunct to add punctuation and casing to the Haikus and ran the KeyBERT model to extract phrases used as prompts. He filtered the data using the GRUEN metric and the phoenemizer library to count the syllables and convert the prompts and Haikus into phonemes. This resulted in over 26K high-quality Haikus with a [5, 7, 5] meter. He then used the GPT-J 6B model from Eluther as the basis for Deep Haiku and fine-tuned it using the filtered Haikus as training data for ten epochs. This process took 11 hours on Google Colab with a Tesla V100 GPU.\n",
            "LLaMa 13B Answer: Sure! Here's the answer to your question:\n",
            "\n",
            "he trained the Deep Haiku model by starting with a pre-trained GPT-J 6B model, fine-tuning it using a dataset of over 26K high-quality Haikus with a specific syllable pattern (5, 7, 5), and filtering the results for adherence to the meter and explicit language.\n",
            "LLaMa 13B Score:  0.8449674\n",
            "\n",
            "Index:            95\n",
            "Reference:        Background\n",
            "For the last five months, I have been looking at the ways Artificial Intelligence (AI) and Machine Learning (ML) can be used for creative endeavors. For example, I showed how Generative Adversarial Networks (GANs) can be used to create new abstract paintings,  here . And I showed how Transformer models can be used to generate new plot summaries for novels,  here .\n",
            "Although the state-of-the-art AI models can generate excellent pictures and words, so far, AI models have not been as good when it comes to composing songs. Some of the music generating models are OK, like the various  Magenta  models from Google [3], the  MuseNet  model from OpenAI [4], and the commercial offering from  AIVA  [5]. But the output from most music generating AI models is often rambling and incoherent. What seems to be lacking is an overall structure for the songs. Let’s see if we can remedy this with an infusion of lyrical poetry.\n",
            "Question:         What was Robert A. Gonsalves' critique of current AI models for generating music?\n",
            "GPT-4 Answer:     he noted that while AI models have shown proficiency in generating images and text, their performance in composing music has been less impressive. He pointed out that the output from most music generating AI models, including Google's Magenta, OpenAI's MuseNet, and AIVA, often lacks coherence and structure. He suggested that this issue could potentially be addressed by incorporating lyrical poetry into the process.\n",
            "LLaMa 13B Answer: According to the text, he believed that current AI models for generating music are lacking in structure and often produce rambling and incoherent output.\n",
            "LLaMa 13B Score:  0.71847457\n",
            "\n",
            "Index:            96\n",
            "Reference:        Next Steps\n",
            "Although the results are pretty good, there is definitely room for improvement. For example, the choices for background images seem somewhat limited, especially for pop culture. This may be due to the fact that I am restricting the search to use only freely licensed photos. I don’t know if a US court has weighed in yet on whether or not the background images in memes can be deemed to be fair use or not, so I’ll leave that question to the lawyers.\n",
            "The developers behind GPT-Neo at EleutherAI are continuing to build and train bigger language models. Their next model is called GPT-NeoX. They say their “primary goal is to train an equivalent model to the full-sized GPT⁠-⁠3 and make it available to the public under an open licence.” [17]\n",
            "Don’t forget to check out more generated memes in the appendix below.\n",
            "Question:         What is the next model that the developers behind GPT-Neo at EleutherAI are planning to build?\n",
            "GPT-4 Answer:     The developers at EleutherAI are planning to build and train a new model called GPT-NeoX. Their primary goal with this model is to train an equivalent to the full-sized GPT-3 and make it available to the public under an open license.\n",
            "LLaMa 13B Answer: Based on the information provided, the next model that the developers behind GPT-Neo at EleutherAI are planning to build is called GPT-NeoX.\n",
            "LLaMa 13B Score:  0.77254856\n",
            "\n",
            "Index:            97\n",
            "Reference:        GPT-3 Da Vinci\n",
            "OpenAI’s GPT-3 Da Vinci is currently the largest AI model for Natural Language Processing [4]. I am using their latest “zero-shot” style of prompting with their new Da Vinci Instruct model. Instead of providing examples of what you are asking the model to do, you can just simply ask it what to do directly.\n",
            "Here is the prompt that creates a caption for the apple pie picture.\n",
            "```\n",
            "Create a funny caption for a new meme about apple pie. The background picture is Simple and easy apple pie served with vanilla ice cream, on a gingham tablecloth in Lysekil, Sweden.\n",
            "```\n",
            "I pass the prompt into the call to OpenAI along with some additional parameters. Here’s the Python code.\n",
            "```\n",
            "import openai response = openai.Completion.create(   engine=\"davinci-instruct-beta\",   prompt=prompt,   max_tokens=64,   temperature=0.7,   top_p=0.5,   frequency_penalty=0.5,   presence_penalty=0.5,   best_of=1)\n",
            "```\n",
            "The  max_token  parameter indicates how long the response should be. The  temperature  and  top_p  parameters are similar in that they indicate the amount of variety in the response. The  frequency_penalty  and  presence_penalty  are also similar in that they control how often there are new deviations and new topics in the response. If you want to know what all these parameters do, check out my article from last month,  here .\n",
            "Before I show examples of the output from GPT-3, here is the legal disclaimer that OpenAI suggests that I show, which is all true.\n",
            "The author generated the following text in part with GPT-3, OpenAI’s large-scale language-generation model. Upon generating draft language, the author reviewed and revised the language to their own liking and takes ultimate responsibility for the content of this publication.\n",
            "Running the code 10 times will yield the following results, at a total cost of $0.03. Note that I formatted the text to be in uppercase.\n",
            "```\n",
            "1: THIS IS THE PERFECT WAY TO END A DAY OF APPLE PICKING  2: NO, IT'S NOT THAT EASY  3: I'LL TAKE THE ONE WITH THE VANILLA ICE CREAM, PLEASE  4: APPLE PIE IS THE BEST!  5: THIS APPLE PIE IS SO GOOD, I CAN'T EVEN!  6: YOU'RE NOT THE ONLY ONE WHO LOVES APPLE PIE  7: IF YOU CAN'T FIND THE RECIPE, JUST GOOGLE IT  8: THE PIE IS GOOD, BUT IT'S NOT AS GOOD AS MY MOM'S  9: I'LL HAVE A SLICE OF THAT APPLE PIE, PLEASE 10: WE'RE GOING TO NEED A BIGGER PIE\n",
            "```\n",
            "OK, these are pretty good. One thing I learned is that  GTP-3 Da Vinci can be funny!  For example, caption number 2 seems to refer to the “easy as pie” idiom.\n",
            "Note that GPT-3, like all AI models trained on a large corpus of text, will reflect societal biases. Occasionally the system will produce text that may be inappropriate or offensive. OpenAI has a feature to label generated text with one of three warning levels: 0 - the text is safe, 1 - this text is sensitive, or 2 - this text is unsafe. My code will show a warning for any of the generated captions that are flagged as sensitive or unsafe.\n",
            "Question:         How does Robert A. Gonsalves use GPT-3 Da Vinci to generate funny captions for memes?\n",
            "GPT-4 Answer:     he uses GPT-3 Da Vinci by creating a prompt that describes the image for which he wants to generate a caption. He then passes this prompt into a call to OpenAI along with additional parameters that control the length and variety of the response. After running the code, he gets a list of potential captions. He found that GPT-3 Da Vinci can indeed generate humorous captions, such as one that cleverly refers to the idiom \"easy as pie\".\n",
            "LLaMa 13B Answer: Based on the information provided, he uses GPT-3 Da Vinci to generate funny captions for memes by providing a prompt to the model and using the \"zero-shot\" style of prompting with the new Da Vinci Instruct model. The model generates a list of captions in response, which can be used as memes.\n",
            "LLaMa 13B Score:  0.78672683\n",
            "\n",
            "Index:            98\n",
            "Reference:        Summary\n",
            "In this article, I gave an overview of Benford’s Law, with some background and history. I explained normal distributions and logarithms as an insight into understanding lognormal distributions. With some theoretical dice rolling exercises, I showed how multiple, independent variables can lead to normal distributions (with addition) and lognormal distributions (with multiplication). I then showed how some datasets with lognormal distributions can lead to conforming with BL. Finally, I walked through three examples of real datasets (city/town populations, accounts payable, and lengths of rivers) to show how datasets with a lognormal distribution will tend to adhere to BL.\n",
            "Question:         What are the three real datasets that Robert A. Gonsalves used to demonstrate the adherence to Benford's Law?\n",
            "GPT-4 Answer:     he used the datasets of city/town populations, accounts payable, and lengths of rivers to demonstrate how datasets with a lognormal distribution will tend to adhere to Benford's Law.\n",
            "LLaMa 13B Answer: Sure! Here is the answer to your question:\n",
            "\n",
            "According to the text, the three real datasets that he used to demonstrate the adherence to Benford's Law are:\n",
            "\n",
            "1. City/town populations\n",
            "2. Accounts payable\n",
            "3. Lengths of rivers\n",
            "LLaMa 13B Score:  0.80105865\n",
            "\n",
            "Index:            99\n",
            "Reference:        Muybridge Derby\n",
            "For this project, I wanted to see if I could use AI systems to transform Muybridge’s animal locomotion photographs into high-resolution, full-color videos. After experimenting with various techniques, I changed the original sequences to be more realistic using a combination of  Midjourney  to create reference frames from text prompts and RunwayML’s Gen-1 Video Generator. For fun, I made a short animation, “Muybridge Derby,” showcasing the work. Here it is.\n",
            "In the following sections, I will describe how I transformed the locomotion sequences, generated the background scroll, and combined the elements to create the animation.\n",
            "Question:         What techniques did Robert A. Gonsalves use to transform Muybridge’s animal locomotion photographs into high-resolution, full-color videos?\n",
            "GPT-4 Answer:     He used a combination of Midjourney to create reference frames from text prompts and RunwayML’s Gen-1 Video Generator to transform the original sequences into more realistic ones. He also generated a background scroll and combined the elements to create an animation called \"Muybridge Derby\".\n",
            "LLaMa 13B Answer: Sure! Here's the answer:\n",
            "\n",
            "he used a combination of Midjourney and RunwayML to transform Muybridge's animal locomotion photographs into high-resolution, full-color videos.\n",
            "LLaMa 13B Score:  0.72309184\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1D array\n",
        "print(\"num scores:  \", len(scores))\n",
        "arr = np.array(scores)\n",
        "\n",
        "# Calculate the average\n",
        "average = np.mean(arr)\n",
        "median = np.median(arr)\n",
        "low = np.min(arr)\n",
        "high = np.max(arr)\n",
        "\n",
        "# Print the results\n",
        "print(\"low score:   \", round(low*100, 3))\n",
        "print(\"mean score:  \", round(average*100, 3))\n",
        "print(\"median score:\", round(median*100, 3))\n",
        "print(\"high score:  \", round(high*100, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "icLbg5Cx1k0G",
        "outputId": "51d1ab53-9f89-41c7-ee14-8f203746d2b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num scores:   100\n",
            "low score:    59.663\n",
            "mean score:   84.501\n",
            "median score: 84.785\n",
            "high score:   98.218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open a file in write mode. If the file doesn't exist, it will be created.\n",
        "with open('llama13b.txt', 'w') as file:\n",
        "    # Iterate through the array\n",
        "    for item in scores:\n",
        "        # Write each item to the file, converting it to a string and adding a newline character\n",
        "        file.write(str(item) + '\\n')"
      ],
      "metadata": {
        "id": "zMFOoWh1FyR-",
        "outputId": "5216c655-1e75-4246-9202-1578230eea22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cea6c1d254ef4e10803517b296874b8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe918089c4e94d4ab0f11023b0492088",
              "IPY_MODEL_52e3129f73df4a04853c7e249bd37d5b",
              "IPY_MODEL_80729121c72b4cb69b6e4832feed9577"
            ],
            "layout": "IPY_MODEL_20266d3386664c638700fb6faeab09fc"
          }
        },
        "fe918089c4e94d4ab0f11023b0492088": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0aa36da94cc34075a2a3af587098115a",
            "placeholder": "​",
            "style": "IPY_MODEL_8930ca9806da4168b9deee9a1360018b",
            "value": "Downloading (…)13b-chat.Q4_K_M.gguf: 100%"
          }
        },
        "52e3129f73df4a04853c7e249bd37d5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff366d875501425b9a3ff4e412438a28",
            "max": 7865956224,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79244cdaf5534e74a3d5b6a74227318a",
            "value": 7865956224
          }
        },
        "80729121c72b4cb69b6e4832feed9577": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd20b37cb7f34f7b802c78e4bd00fd73",
            "placeholder": "​",
            "style": "IPY_MODEL_05695cc3188f4034b90da531144b6066",
            "value": " 7.87G/7.87G [00:44&lt;00:00, 223MB/s]"
          }
        },
        "20266d3386664c638700fb6faeab09fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0aa36da94cc34075a2a3af587098115a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8930ca9806da4168b9deee9a1360018b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff366d875501425b9a3ff4e412438a28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79244cdaf5534e74a3d5b6a74227318a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd20b37cb7f34f7b802c78e4bd00fd73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05695cc3188f4034b90da531144b6066": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9e9dc2863dc415fa0085dc5c422df54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e5e79c2ae190441581d06e4df998c20a",
              "IPY_MODEL_59444e563b484b99bdcefa97a4412ca4",
              "IPY_MODEL_845abf2255fb49918de8d347782b6775"
            ],
            "layout": "IPY_MODEL_b19c417fe5f149da83a13bcca19ffbc6"
          }
        },
        "e5e79c2ae190441581d06e4df998c20a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afcd442f13014345804c9331bbd6f216",
            "placeholder": "​",
            "style": "IPY_MODEL_8f13098e1e5f4d1eb5fb701d8d8a7f52",
            "value": ""
          }
        },
        "59444e563b484b99bdcefa97a4412ca4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad58349ece294470b9d3cdf2c39408f7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_253d0aadea184e7da9819c9a848e42b3",
            "value": 0
          }
        },
        "845abf2255fb49918de8d347782b6775": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8977c81f9eaa47d8932326fabfa21b92",
            "placeholder": "​",
            "style": "IPY_MODEL_cf88e3309a4448f99bcff0753fd1c9f2",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "b19c417fe5f149da83a13bcca19ffbc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afcd442f13014345804c9331bbd6f216": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f13098e1e5f4d1eb5fb701d8d8a7f52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad58349ece294470b9d3cdf2c39408f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "253d0aadea184e7da9819c9a848e42b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8977c81f9eaa47d8932326fabfa21b92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf88e3309a4448f99bcff0753fd1c9f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "270e784d24e34c12af085f61f9053d38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0f704bbf4bf04c45b67f922bc936ce2e",
              "IPY_MODEL_d9c6351653ca453d8125400ee34db2d4",
              "IPY_MODEL_de15222453944bd4816399cc512a12ba"
            ],
            "layout": "IPY_MODEL_b173ad919324474b942cbf35151936d9"
          }
        },
        "0f704bbf4bf04c45b67f922bc936ce2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63bc98fead394861b79f7f653e4d4414",
            "placeholder": "​",
            "style": "IPY_MODEL_bb649dc488874bf281b1b1c9f418682c",
            "value": "Downloading (…)a8e1d/.gitattributes: 100%"
          }
        },
        "d9c6351653ca453d8125400ee34db2d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_288817ebff81486285caa9488f6d4acc",
            "max": 1175,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23781d3efcce410c8eabb17c7e6d184f",
            "value": 1175
          }
        },
        "de15222453944bd4816399cc512a12ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27fcf31df8e84c82a988ae1b5b9f9972",
            "placeholder": "​",
            "style": "IPY_MODEL_dea125856926421a93ed53d5a30ae56a",
            "value": " 1.18k/1.18k [00:00&lt;00:00, 84.1kB/s]"
          }
        },
        "b173ad919324474b942cbf35151936d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63bc98fead394861b79f7f653e4d4414": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb649dc488874bf281b1b1c9f418682c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "288817ebff81486285caa9488f6d4acc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23781d3efcce410c8eabb17c7e6d184f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27fcf31df8e84c82a988ae1b5b9f9972": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dea125856926421a93ed53d5a30ae56a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3427d152dd3e4884904aabde1dcee08e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be3f2361b4f844d78fd91eb09fecb6c1",
              "IPY_MODEL_b7fc96a7a34441e58f5a085722de3da2",
              "IPY_MODEL_5d57956a94f54f5a939b61a3db6102c9"
            ],
            "layout": "IPY_MODEL_31bd768f939145e6a2a75ba61693b9b5"
          }
        },
        "be3f2361b4f844d78fd91eb09fecb6c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09aa31bb9f3949ac95cb1bb3dea9a603",
            "placeholder": "​",
            "style": "IPY_MODEL_f94af0ea3e324301b6ed18f49fc9c519",
            "value": "Downloading (…)_Pooling/config.json: 100%"
          }
        },
        "b7fc96a7a34441e58f5a085722de3da2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffe549000b85416f8b88cb0c8b7af4b8",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf71683b38b047bb8bcb5ea4320dabc9",
            "value": 190
          }
        },
        "5d57956a94f54f5a939b61a3db6102c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66a77e5f967e44bdada77d491404cfb0",
            "placeholder": "​",
            "style": "IPY_MODEL_f2b1065e063a489fbb967e9d65e37d93",
            "value": " 190/190 [00:00&lt;00:00, 9.17kB/s]"
          }
        },
        "31bd768f939145e6a2a75ba61693b9b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09aa31bb9f3949ac95cb1bb3dea9a603": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f94af0ea3e324301b6ed18f49fc9c519": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ffe549000b85416f8b88cb0c8b7af4b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf71683b38b047bb8bcb5ea4320dabc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66a77e5f967e44bdada77d491404cfb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2b1065e063a489fbb967e9d65e37d93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87453a64429f4e468c19411994a2d66b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2522aebef7774880903d325a859aa49d",
              "IPY_MODEL_15118c31a60d409c878716deaeaeca36",
              "IPY_MODEL_9209c2f7fedc43658460a5d34fa87d6b"
            ],
            "layout": "IPY_MODEL_80c1662c8d0141a7bc722823860f088f"
          }
        },
        "2522aebef7774880903d325a859aa49d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4b0ef91213d46258de66216ea065b1b",
            "placeholder": "​",
            "style": "IPY_MODEL_d5ba60fb40e14e0ba19c132f15354712",
            "value": "Downloading (…)b20bca8e1d/README.md: 100%"
          }
        },
        "15118c31a60d409c878716deaeaeca36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6524dbb4ce6549718e4b8f2eecd150d6",
            "max": 10571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4611f9f3cda84cd3af5a09bc9a1190b4",
            "value": 10571
          }
        },
        "9209c2f7fedc43658460a5d34fa87d6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fef967fd6cf14e3f9f0c84409a1fb4af",
            "placeholder": "​",
            "style": "IPY_MODEL_d7be602ccb374729abf3bf8fd4243a45",
            "value": " 10.6k/10.6k [00:00&lt;00:00, 576kB/s]"
          }
        },
        "80c1662c8d0141a7bc722823860f088f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4b0ef91213d46258de66216ea065b1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5ba60fb40e14e0ba19c132f15354712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6524dbb4ce6549718e4b8f2eecd150d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4611f9f3cda84cd3af5a09bc9a1190b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fef967fd6cf14e3f9f0c84409a1fb4af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7be602ccb374729abf3bf8fd4243a45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e399e77970c64870aedc18699322a6d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df9d23e48f30469e8158c1b5f5d8b0ed",
              "IPY_MODEL_08ca3fa7c67d48d294a3cd959980cb62",
              "IPY_MODEL_36c52596aa304784b8fe25e7790cbab6"
            ],
            "layout": "IPY_MODEL_46f1b5f0c0f94c4fb76ba6c1ce782fc2"
          }
        },
        "df9d23e48f30469e8158c1b5f5d8b0ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a41e67bdbc144eeb9ed9d49989819a78",
            "placeholder": "​",
            "style": "IPY_MODEL_e1b483c31a934026aca9d9977e64c32a",
            "value": "Downloading (…)0bca8e1d/config.json: 100%"
          }
        },
        "08ca3fa7c67d48d294a3cd959980cb62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c925829769e4501b97b73661d779861",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16d960f0a89f4f438c681ef5619f730c",
            "value": 571
          }
        },
        "36c52596aa304784b8fe25e7790cbab6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2511ac746d5e49b58d29ffceba724c73",
            "placeholder": "​",
            "style": "IPY_MODEL_3ea46c0420f74de589665aa5828302a6",
            "value": " 571/571 [00:00&lt;00:00, 22.9kB/s]"
          }
        },
        "46f1b5f0c0f94c4fb76ba6c1ce782fc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a41e67bdbc144eeb9ed9d49989819a78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1b483c31a934026aca9d9977e64c32a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c925829769e4501b97b73661d779861": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16d960f0a89f4f438c681ef5619f730c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2511ac746d5e49b58d29ffceba724c73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ea46c0420f74de589665aa5828302a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac62733d7ccd4fe79d1ca7d86674eabf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e982767557794cc899ea33615805ae7b",
              "IPY_MODEL_fde691ccb934402b99e19fdb28a8c729",
              "IPY_MODEL_c8463f337d574dbdab02a282fe139215"
            ],
            "layout": "IPY_MODEL_7bbe4603cf97485bbd145d520c97f2d9"
          }
        },
        "e982767557794cc899ea33615805ae7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_737022f8e4e5426b8486093ed0c5333e",
            "placeholder": "​",
            "style": "IPY_MODEL_54218b5ced14455b9ad3be6d23a1dd08",
            "value": "Downloading (…)ce_transformers.json: 100%"
          }
        },
        "fde691ccb934402b99e19fdb28a8c729": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f32155027fd4e19af5420880afb4e83",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b197008dac784ef997a0fd9c49de1588",
            "value": 116
          }
        },
        "c8463f337d574dbdab02a282fe139215": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cf2d68e8c8e4c96b5470b44d317453a",
            "placeholder": "​",
            "style": "IPY_MODEL_37676e6f0d4d4831b6ebc2ee4311aed9",
            "value": " 116/116 [00:00&lt;00:00, 8.96kB/s]"
          }
        },
        "7bbe4603cf97485bbd145d520c97f2d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "737022f8e4e5426b8486093ed0c5333e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54218b5ced14455b9ad3be6d23a1dd08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f32155027fd4e19af5420880afb4e83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b197008dac784ef997a0fd9c49de1588": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3cf2d68e8c8e4c96b5470b44d317453a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37676e6f0d4d4831b6ebc2ee4311aed9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1dc3f4f30c946d0b00035b6cee4c0b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3eecb8a34bac48fa8af35a24596fdb8d",
              "IPY_MODEL_44bbc0009b884c89a353798cbc0ec344",
              "IPY_MODEL_f837a8309223437a84524b71286499b9"
            ],
            "layout": "IPY_MODEL_19139d2f9b574d189df07b50e0c5f6cf"
          }
        },
        "3eecb8a34bac48fa8af35a24596fdb8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f086124daa0b434f885bfd4bc2e930eb",
            "placeholder": "​",
            "style": "IPY_MODEL_1f4dbaa12345428a91f6a5b42e4a0cdb",
            "value": "Downloading (…)e1d/data_config.json: 100%"
          }
        },
        "44bbc0009b884c89a353798cbc0ec344": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_acb5edd2090b48969b3db535d6e301b4",
            "max": 39265,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf5ad61ee3df49b0af338ba1c26a3b68",
            "value": 39265
          }
        },
        "f837a8309223437a84524b71286499b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9c2e73e6b644c4191571ccc7a2e7a1b",
            "placeholder": "​",
            "style": "IPY_MODEL_e37ba421721e44f298297aa8ea88154f",
            "value": " 39.3k/39.3k [00:00&lt;00:00, 2.40MB/s]"
          }
        },
        "19139d2f9b574d189df07b50e0c5f6cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f086124daa0b434f885bfd4bc2e930eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f4dbaa12345428a91f6a5b42e4a0cdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "acb5edd2090b48969b3db535d6e301b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf5ad61ee3df49b0af338ba1c26a3b68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e9c2e73e6b644c4191571ccc7a2e7a1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e37ba421721e44f298297aa8ea88154f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "818d2483789f4f07acce6c0c51075caa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c676e59a4e2348b6a1ce9c5d3658da40",
              "IPY_MODEL_5cc5175f6dc54aae84881c9a8693edb5",
              "IPY_MODEL_6994c3018b8d42388a613d63de387b82"
            ],
            "layout": "IPY_MODEL_64ad20911d4f4476a86d5f9ddd1f8bb4"
          }
        },
        "c676e59a4e2348b6a1ce9c5d3658da40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be4b72925a5f498eb801078471e862c4",
            "placeholder": "​",
            "style": "IPY_MODEL_342b4eefdca54fc9a521f27cc8e0d777",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "5cc5175f6dc54aae84881c9a8693edb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31f03cf100784fe5b3698cb84f8750d5",
            "max": 438011953,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7129eac3cafb4b8187ed0f4827ddfa9b",
            "value": 438011953
          }
        },
        "6994c3018b8d42388a613d63de387b82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82aa19eb7f8a421690f35da12100a927",
            "placeholder": "​",
            "style": "IPY_MODEL_be391e8a783f4e528938bade5c2a6265",
            "value": " 438M/438M [00:02&lt;00:00, 236MB/s]"
          }
        },
        "64ad20911d4f4476a86d5f9ddd1f8bb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be4b72925a5f498eb801078471e862c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "342b4eefdca54fc9a521f27cc8e0d777": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31f03cf100784fe5b3698cb84f8750d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7129eac3cafb4b8187ed0f4827ddfa9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82aa19eb7f8a421690f35da12100a927": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be391e8a783f4e528938bade5c2a6265": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0e4ebe16ff34156b8fa03c3bffa61e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_badf0a144d0346d7b0da5ff525f131c4",
              "IPY_MODEL_60228f24d1064fe2a3f7ece75a89f966",
              "IPY_MODEL_e75d9fbfa0314fb7a065aa4f5c260348"
            ],
            "layout": "IPY_MODEL_bb5032b6b3254a01a648fb62a8409384"
          }
        },
        "badf0a144d0346d7b0da5ff525f131c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb9b36e854b948c2930151b3e678dfe9",
            "placeholder": "​",
            "style": "IPY_MODEL_6249608e46ef49ac9ccacc27682a8106",
            "value": "Downloading (…)nce_bert_config.json: 100%"
          }
        },
        "60228f24d1064fe2a3f7ece75a89f966": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7819fa779dd441ab5ba0afdb835858f",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a34d6da001a9487da7e6357965e606aa",
            "value": 53
          }
        },
        "e75d9fbfa0314fb7a065aa4f5c260348": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2db203f5e6f84c4b98c51d12ef06eae8",
            "placeholder": "​",
            "style": "IPY_MODEL_3b5dcb11752b49bf945dca57daf37175",
            "value": " 53.0/53.0 [00:00&lt;00:00, 3.38kB/s]"
          }
        },
        "bb5032b6b3254a01a648fb62a8409384": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb9b36e854b948c2930151b3e678dfe9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6249608e46ef49ac9ccacc27682a8106": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7819fa779dd441ab5ba0afdb835858f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a34d6da001a9487da7e6357965e606aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2db203f5e6f84c4b98c51d12ef06eae8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b5dcb11752b49bf945dca57daf37175": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ccc271dfce374ab884fc22c3b8256194": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b796db6ada00431e9ed7b4d69f348b80",
              "IPY_MODEL_c0833d375767424086f63b99586b3227",
              "IPY_MODEL_0d8d94738b6a446393df4f7c6a5e53f3"
            ],
            "layout": "IPY_MODEL_d967873a0f104e04a3deb8247cc5edb5"
          }
        },
        "b796db6ada00431e9ed7b4d69f348b80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdb893dabefa45a7bfedc2a8d7ff28ac",
            "placeholder": "​",
            "style": "IPY_MODEL_af681a758d80486783ea91856c3b978b",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "c0833d375767424086f63b99586b3227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_873047ec5ce84cef9757add8f4a4856d",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf5e0e94482c47e89031ccd1a2c95f3d",
            "value": 239
          }
        },
        "0d8d94738b6a446393df4f7c6a5e53f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ba0faaa1fc541c0a83346ccb171a9f1",
            "placeholder": "​",
            "style": "IPY_MODEL_a2182c8503df4cb4bb08cc3af603f8de",
            "value": " 239/239 [00:00&lt;00:00, 16.9kB/s]"
          }
        },
        "d967873a0f104e04a3deb8247cc5edb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdb893dabefa45a7bfedc2a8d7ff28ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af681a758d80486783ea91856c3b978b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "873047ec5ce84cef9757add8f4a4856d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf5e0e94482c47e89031ccd1a2c95f3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9ba0faaa1fc541c0a83346ccb171a9f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2182c8503df4cb4bb08cc3af603f8de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56c21671faeb4c4b8235a49a09a952c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c437d535f7b24a04976ae6ea8a112ef5",
              "IPY_MODEL_3b496c96dd5243b2b431ab26b0ce4797",
              "IPY_MODEL_7676c91eb7ac4b749daee2085d6f86bd"
            ],
            "layout": "IPY_MODEL_14807dcf9bb0457995bf082518f3f705"
          }
        },
        "c437d535f7b24a04976ae6ea8a112ef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0344fb77882e4de49141a4064c080f5b",
            "placeholder": "​",
            "style": "IPY_MODEL_fe331585eef348b3bb8d4a69d3676f33",
            "value": "Downloading (…)a8e1d/tokenizer.json: 100%"
          }
        },
        "3b496c96dd5243b2b431ab26b0ce4797": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_524c8e0ebc9249e6ac21e4988498b4fe",
            "max": 466021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_59f7c44ac00744028e03a3c7a980bfb3",
            "value": 466021
          }
        },
        "7676c91eb7ac4b749daee2085d6f86bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_094eed91d4734f3d86fdfcb1e519d3eb",
            "placeholder": "​",
            "style": "IPY_MODEL_d8fd04a4edac4f77b6bba8b4a3e0f268",
            "value": " 466k/466k [00:00&lt;00:00, 3.53MB/s]"
          }
        },
        "14807dcf9bb0457995bf082518f3f705": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0344fb77882e4de49141a4064c080f5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe331585eef348b3bb8d4a69d3676f33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "524c8e0ebc9249e6ac21e4988498b4fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59f7c44ac00744028e03a3c7a980bfb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "094eed91d4734f3d86fdfcb1e519d3eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8fd04a4edac4f77b6bba8b4a3e0f268": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6acf9ca07b82469cbbd309b0cb7f0195": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b7a74a18845463b92a5b0292c779ce4",
              "IPY_MODEL_5ac29eaf5fbd4cdf92827c02b19933cf",
              "IPY_MODEL_471ca1bc22de4e40b487455e2e73c5c5"
            ],
            "layout": "IPY_MODEL_85db9cea6ff94f5cb726d61c15d67381"
          }
        },
        "9b7a74a18845463b92a5b0292c779ce4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b4d22f1075e4a738aaa4a1e2ce781f1",
            "placeholder": "​",
            "style": "IPY_MODEL_146c39915ef1419bae133fd4a8803b48",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "5ac29eaf5fbd4cdf92827c02b19933cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce59d89c982f4c97b82ff525c19d16a4",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f070fe7e55d4581ad8e93d0f714386c",
            "value": 363
          }
        },
        "471ca1bc22de4e40b487455e2e73c5c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d11d56bfc0ac45569dabc256c210c593",
            "placeholder": "​",
            "style": "IPY_MODEL_1dc8941cb0ab4daeb961be9c8b498922",
            "value": " 363/363 [00:00&lt;00:00, 25.8kB/s]"
          }
        },
        "85db9cea6ff94f5cb726d61c15d67381": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b4d22f1075e4a738aaa4a1e2ce781f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "146c39915ef1419bae133fd4a8803b48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce59d89c982f4c97b82ff525c19d16a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f070fe7e55d4581ad8e93d0f714386c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d11d56bfc0ac45569dabc256c210c593": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dc8941cb0ab4daeb961be9c8b498922": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5805d6bdb0ab4b8ca10fb6e703e91d36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f57dee2e04b243af9d011703920dab34",
              "IPY_MODEL_e3537454453b4e6c8d0242334d8c26b4",
              "IPY_MODEL_867bacb56c584624bf7747cd8019ddb4"
            ],
            "layout": "IPY_MODEL_14a44d11093c44239b9df011b1bf3592"
          }
        },
        "f57dee2e04b243af9d011703920dab34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca8e45c2430744c9a32551aae1f65288",
            "placeholder": "​",
            "style": "IPY_MODEL_c32d8a205f31403dab6cbcba38b2e1db",
            "value": "Downloading (…)8e1d/train_script.py: 100%"
          }
        },
        "e3537454453b4e6c8d0242334d8c26b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c83b3473bdd74691ac696154a1c58351",
            "max": 13123,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f4afbac96f364586953b9d7a06863b6c",
            "value": 13123
          }
        },
        "867bacb56c584624bf7747cd8019ddb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e1d9184e6024565a3e71fce16cef87e",
            "placeholder": "​",
            "style": "IPY_MODEL_e858806a270b4a8b8932e86f2558bd82",
            "value": " 13.1k/13.1k [00:00&lt;00:00, 948kB/s]"
          }
        },
        "14a44d11093c44239b9df011b1bf3592": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca8e45c2430744c9a32551aae1f65288": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c32d8a205f31403dab6cbcba38b2e1db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c83b3473bdd74691ac696154a1c58351": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4afbac96f364586953b9d7a06863b6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e1d9184e6024565a3e71fce16cef87e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e858806a270b4a8b8932e86f2558bd82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5930616e365487484b2424193ab0353": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32209ecf314f4dada59e827dd129d430",
              "IPY_MODEL_e35c48ca3f644545bee50d3bce9e06cf",
              "IPY_MODEL_1ad7e64ac2424d67bfd3adb47af4dae5"
            ],
            "layout": "IPY_MODEL_cb6029a64f894e649a90e9a35a3030f3"
          }
        },
        "32209ecf314f4dada59e827dd129d430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d4c1c864e814549ae46557fc35ee818",
            "placeholder": "​",
            "style": "IPY_MODEL_95d93e30e1054247afd4a47d26d37f6b",
            "value": "Downloading (…)b20bca8e1d/vocab.txt: 100%"
          }
        },
        "e35c48ca3f644545bee50d3bce9e06cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24f7bc0e5bf64dc0bbae61aa5f0c3947",
            "max": 231536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c3b4f16a4f114bf3b31ac7fe295490f3",
            "value": 231536
          }
        },
        "1ad7e64ac2424d67bfd3adb47af4dae5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f58fc94f28d4088b570d3f567de0a1b",
            "placeholder": "​",
            "style": "IPY_MODEL_7dae5bb0a12849c98f6477f4e104a390",
            "value": " 232k/232k [00:00&lt;00:00, 1.78MB/s]"
          }
        },
        "cb6029a64f894e649a90e9a35a3030f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d4c1c864e814549ae46557fc35ee818": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95d93e30e1054247afd4a47d26d37f6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24f7bc0e5bf64dc0bbae61aa5f0c3947": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3b4f16a4f114bf3b31ac7fe295490f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f58fc94f28d4088b570d3f567de0a1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dae5bb0a12849c98f6477f4e104a390": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f14f28654b94a41b43a33d4b9fe4218": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e521e20182964a8f8502f2c10844589f",
              "IPY_MODEL_65bb8ef822224ee5b77dce0c6413d31c",
              "IPY_MODEL_19581b937060454ab63fc3f586cfd973"
            ],
            "layout": "IPY_MODEL_8427ee0c5b164d6e93c478de370fdb3d"
          }
        },
        "e521e20182964a8f8502f2c10844589f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e88dc64cefde437d8a39cdb3b3471744",
            "placeholder": "​",
            "style": "IPY_MODEL_ee388547457f4dbca1ffdd5f9f7ae03a",
            "value": "Downloading (…)bca8e1d/modules.json: 100%"
          }
        },
        "65bb8ef822224ee5b77dce0c6413d31c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f6d30877c384f04acb2a916b7fb5aa9",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e982cd9ab7174b8d875f0f70966993dc",
            "value": 349
          }
        },
        "19581b937060454ab63fc3f586cfd973": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8b3e3eb96a340b2a23e36f26c33ea6b",
            "placeholder": "​",
            "style": "IPY_MODEL_121d60ce4e89423a9d889205e84a1634",
            "value": " 349/349 [00:00&lt;00:00, 27.0kB/s]"
          }
        },
        "8427ee0c5b164d6e93c478de370fdb3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e88dc64cefde437d8a39cdb3b3471744": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee388547457f4dbca1ffdd5f9f7ae03a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f6d30877c384f04acb2a916b7fb5aa9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e982cd9ab7174b8d875f0f70966993dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b8b3e3eb96a340b2a23e36f26c33ea6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "121d60ce4e89423a9d889205e84a1634": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}